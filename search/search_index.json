{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-thehive-5-documentation-website","title":"Welcome to TheHive 5 documentation website","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"cortex/","title":"Home","text":"Cortex : Installation, operation and user guides    <p>Source Code: https://github.com/thehive-project/Cortex/</p> <p>Website: https://www.strangebee.com</p>"},{"location":"cortex/#cortex","title":"Cortex","text":"<p>Cortex solves two common problems frequently encountered by SOCs, CSIRTs and security researchers in the course of threat intelligence, digital forensics and incident response:</p> <ul> <li>How to analyze observables they have collected, at scale, by querying a single tool instead of several?</li> <li>How to actively respond to threats and interact with the constituency and other teams?</li> </ul> <p>Thanks to its many analyzers and to its RESTful API, Cortex makes observable analysis a breeze, particularly if called from TheHive, the highly popular, Security Incident Response Platform (SIRP). </p> <p>TheHive can also leverage Cortex responders to perform specific actions on alerts, cases, tasks and observables collected in the course of the investigation: send an email to the constituents, block an IP address at the proxy level, notify team members that an alert needs to be taken care of urgently and much more.</p> <p>Many features are included with Cortex: </p> <ul> <li>Manage multiple organizations (i.e multi-tenancy)</li> <li>Manage users per organizations and roles</li> <li>Specify per-org analyzer &amp; responder configuration</li> <li>Define rate limits: avoid consuming all your quotas at once</li> <li>Cache: an analysis is not re-executed for the same observable if a given analyzer is called on that observable several times within a specific timespan (10 minutes by default, can be adjusted for each analyzer).</li> </ul>"},{"location":"cortex/#installation-and-configuration-guides","title":"Installation and configuration guides","text":"<p>This documentation contains step-by-step installation instructions for Cortex for different operating systems as well as corresponding binary archives. </p> <p>All aspects of the configuration are aslo detailled in a dedicated section. s</p>"},{"location":"cortex/#user-guides","title":"User guides","text":"<p>The first connection to the application requires several actions. </p> <p>Cortex supports differents roles for users. Refer to User roles for more details.</p>"},{"location":"cortex/#license","title":"License","text":"<p>Cortex is an open source and free software released under the AGPL (Affero General Public License). We, StrangeBee, are committed to ensure that Cortex will remain a free and open source project on the long-run.</p>"},{"location":"cortex/#updates-and-community-discussions","title":"Updates and community discussions","text":"<p>Information, news and updates are regularly posted on several communication channels:</p> <p> StrangeBee Twitter account / TheHive Project Twitter account</p> <p> TheHive Project Mastodon account / StrangeBee Mastodon account</p> <p> blog at StrangeBee</p> <p> Join users community on Discord</p>"},{"location":"cortex/#professional-support","title":"Professional support","text":"<p> Since 2018, Cortex is fully developped and maintained by StrangeBee. Should you need specific assistance, be aware that StrangeBee also provides professional services and support.</p>"},{"location":"cortex/code-of-conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"cortex/code-of-conduct/#contributor-covenant-code-of-conduct","title":"Contributor Covenant Code of Conduct","text":""},{"location":"cortex/code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"cortex/code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"cortex/code-of-conduct/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior, in compliance with the licensing terms applying to the Project developments.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. However, these actions shall respect the licensing terms of the Project Developments that will always supersede such Code of Conduct.</p>"},{"location":"cortex/code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"cortex/code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at support@thehive-project.org. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"cortex/code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4</p> <p>This version includes a clarification to ensure that the code of conduct is in compliance with the free software licensing terms of the project.</p>"},{"location":"cortex/api/api-guide/","title":"API Guide","text":""},{"location":"cortex/api/api-guide/#api-guide","title":"API Guide","text":"<p>This guide applies only to Cortex 2 and newer. It is not applicable to Cortex 1.</p>"},{"location":"cortex/api/api-guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction<ul> <li>Request &amp; Response Formats</li> <li>Authentication</li> </ul> </li> <li>Organization APIs<ul> <li>Organization Model</li> <li>List</li> <li>Create</li> <li>Update</li> <li>Delete</li> <li>Obtain Details</li> <li>List Users</li> <li>List Enabled Analyzers</li> </ul> </li> <li>User APIs<ul> <li>User Model</li> <li>List All</li> <li>List Users within an Organization</li> <li>Search</li> <li>Create</li> <li>Update</li> <li>Get Details</li> <li>Set a Password</li> <li>Change a password</li> <li>Set and Renew an API Key</li> <li>Get an API Key</li> <li>Revoke an API Key</li> </ul> </li> <li>Job APIs<ul> <li>Job Model</li> <li>List and Search</li> <li>Get Details</li> <li>Get Details and Report</li> <li>Wait and Get Job Report</li> <li>Get Artifacts</li> <li>Delete</li> </ul> </li> <li>Analyzer APIs<ul> <li>Analyzer Model</li> <li>Enable</li> <li>List and Search</li> <li>Get Details</li> <li>Get By Type</li> <li>Update</li> <li>Run</li> <li>Disable</li> </ul> </li> <li>Miscellaneous APIs<ul> <li>Paging and Sorting</li> </ul> </li> </ul>"},{"location":"cortex/api/api-guide/#introduction","title":"Introduction","text":"<p>Cortex 2 offers a REST API that can be leveraged by various applications and programs to interact with it. The following guide describe the Cortex 2 API to allow developers to interface the powerful observable analysis engine with other SIRPs (Security Incident Response Platforms) besides TheHive, TIPs (Threat Intelligence Platforms), SIEMs or scripts. Please note that the Web UI of Cortex 2 exclusively leverage the REST API to interact with the back-end.</p> <p>Note: You can use Cortex4py, the Python library we provide, to facilitate interaction with the REST API of Cortex. You need Cortex4py 2.0.0 or later as earlier versions are not compatible with Cortex 2.</p> <p>All the exposed APIs share the same request &amp; response formats and authentication strategies as described below.</p> <p>There are also some transverse parameters supported by several calls, in addition to utility APIs.</p> <p>If you want to create an analyzer, please read the How to Write and Submit an Analyzer  guide.</p>"},{"location":"cortex/api/api-guide/#request-response-formats","title":"Request &amp; Response Formats","text":"<p>Cortex accepts several parameter formats within a HTTP request. They can be used indifferently. Input data can be:</p> <ul> <li>A query string</li> <li>A URL-encoded form</li> <li>A multi-part</li> <li>JSON</li> </ul> <p>Hence, the requests shown below are equivalent.</p>"},{"location":"cortex/api/api-guide/#query-string","title":"Query String","text":"<pre><code>curl -XPOST 'https://CORTEX_APP_URL:9001/api/login?user=me&amp;password=secret'\n</code></pre>"},{"location":"cortex/api/api-guide/#url-encoded-form","title":"URL-encoded Form","text":"<pre><code>curl -XPOST 'https://CORTEX_APP_URL:9001/api/login' -d user=me -d password=secret\n</code></pre>"},{"location":"cortex/api/api-guide/#json","title":"JSON","text":"<pre><code>curl -XPOST https://CORTEX_APP_URL:9001/api/login -H 'Content-Type: application/json' -d '{\n  \"user\": \"me\",\n  \"password\": \"secret\"\n}'\n</code></pre>"},{"location":"cortex/api/api-guide/#multi-part","title":"Multi-part","text":"<pre><code>curl -XPOST https://CORTEX_APP_URL:9001/api/login -F '_json=&lt;-;type=application/json' &lt;&lt; _EOF_\n{\n  \"user\": \"me\",\n  \"password\": \"secret\"\n}\n_EOF_\n</code></pre>"},{"location":"cortex/api/api-guide/#response-format","title":"Response Format","text":"<p>For each request submitted, Cortex will respond back with JSON data. For example, if the authentication request is successful, Cortex should return the following output:</p> <pre><code>{\"id\":\"me\",\"name\":\"me\",\"roles\":[\"read\",\"analyze\",\"orgadmin\"]}\n</code></pre> <p>If not, Cortex should return an authentication error:</p> <pre><code>{\"type\":\"AuthenticationError\",\"message\":\"Authentication failure\"}\n</code></pre>"},{"location":"cortex/api/api-guide/#authentication","title":"Authentication","text":"<p>Most API calls require authentication. Credentials can be provided using a session cookie, an API key or directly using HTTP basic authentication (if this method is specifically enabled).</p> <p>Session cookies are better suited for browser authentication. Hence, we recommend authenticating with API keys when calling the Cortex APIs.</p>"},{"location":"cortex/api/api-guide/#generating-api-keys-with-an-orgadmin-account","title":"Generating API Keys with an orgAdmin Account","text":"<p>API keys can be generated using the Web UI. To do so, connect using an <code>orgAdmin</code> account then click on Organization and then on the <code>Create API Key</code> button in the row corresponding to the user you intend to use for API authentication. Once the API key has been created, click on <code>Reveal</code> to display the API key then click on the copy to clipboard button if you wish to copy the key to your system's clipboard.</p> <p>If the user is not yet created, start by clicking on <code>Add user</code> to create it then follow the steps mentioned above.</p>"},{"location":"cortex/api/api-guide/#generating-api-keys-with-a-superadmin-account","title":"Generating API Keys with a superAdmin Account","text":"<p>You can use a <code>superAdmin</code> account to achieve the same result as described above. Once authenticated, click on Users then on the <code>Create API Key</code> button in the row corresponding to the user you intend to use for API authentication. Please make sure the user is in the right organization by thoroughly reading its name, which is shown below the user name. Once the API key has been created, click on <code>Reveal</code> to display the API key then click on the copy to clipboard button if you wish to copy the key to your system's clipboard.</p>"},{"location":"cortex/api/api-guide/#authenticating-with-an-api-key","title":"Authenticating with an API Key","text":"<p>Once you have generated an API key you can use it, for example, to list the Cortex jobs thanks to the following <code>curl</code> command:</p> <p></p><pre><code># Using API key\ncurl -H 'Authorization: Bearer **API_KEY**' https://CORTEX_APP_URL:9001/api/job\n</code></pre> As you can see in the example above, we instructed <code>curl</code> to add the Authorization header to the request. The value of the header is <code>Bearer: **API_KEY**</code>. So if your API key is <code>GPX20GUAQWwpqnhA6JpOwNGPMfWuxsX3</code>, the <code>curl</code> command above would look like the following: <pre><code># Using API key\ncurl -H 'Authorization: Bearer GPX20GUAQWwpqnhA6JpOwNGPMfWuxsX3' https://CORTEX_APP_URL:9001/api/job\n</code></pre>"},{"location":"cortex/api/api-guide/#using-basic-authentication","title":"Using Basic Authentication","text":"<p>Cortex also supports basic authentication but it is disabled by default for security reasons. If you absolutely need to use it, you can enable it by adding <code>auth.method.basic=true</code> to the configuration file (<code>/etc/cortex/application.conf</code> by default). Once you do, restart the Cortex service. You can then, for example, list the Cortex jobs using the following <code>curl</code> command:</p> <pre><code># Using basic authentication\ncurl -u mylogin:mypassword https://CORTEX_APP_URL:9001/api/job\n</code></pre>"},{"location":"cortex/api/api-guide/#organization-apis","title":"Organization APIs","text":"<p>Cortex offers a set of APIs to create, update and list organizations.</p>"},{"location":"cortex/api/api-guide/#organization-model","title":"Organization Model","text":"<p>An organization (org) is defined by the following attributes:</p> Attribute Description Type <code>id</code> Copy of the org's name (see next row) readonly <code>name</code> Name readonly <code>status</code> Status (<code>Active</code> or <code>Locked</code>) writable <code>description</code> Description writable <code>createdAt</code> Creation date computed <code>createdBy</code> User who created the org computed <code>updatedAt</code> Last update computed <code>updatedBy</code> User who last updated the org computed <p>Please note that <code>id</code> and <code>name</code> are essentially the same. Also, <code>createdAt</code> and <code>updatedAt</code> are in epoch.</p>"},{"location":"cortex/api/api-guide/#list","title":"List","text":"<p>It is possible to list all the organizations using the following API call, which requires the API key associated with a <code>superAdmin</code> account:</p> <pre><code>curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization'\n</code></pre> <p>You can also search/filter organizations using the following query:</p> <pre><code>curl -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/organization/_search' -d '{\n  \"query\": {\"status\": \"Active\"}\n}'\n</code></pre> <p>Both APIs supports the <code>range</code> and <code>sort</code> query parameters described in paging and sorting details.</p>"},{"location":"cortex/api/api-guide/#create","title":"Create","text":"<p>It is possible to create an organization using the following API call, which requires the API key associated with a <code>superAdmin</code> account:</p> <pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/organization' -d '{\n  \"name\": \"demo\",\n  \"description\": \"Demo organization\",\n  \"status\": \"Active\"\n}'\n</code></pre>"},{"location":"cortex/api/api-guide/#update","title":"Update","text":"<p>You can update an organization's description and status (<code>Active</code> or <code>Locked</code>) using the following API call. This requires the API key associated with a <code>superAdmin</code> account:</p> <pre><code>curl -XPATCH -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID' -d '{\n  \"description\": \"New Demo organization\",\n}'\n</code></pre> <p>or</p> <pre><code>curl -XPATCH -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID' -d '{\n  \"status\": \"Active\",\n}'\n</code></pre>"},{"location":"cortex/api/api-guide/#delete","title":"Delete","text":"<p>Deleting an organization just marks it as <code>Locked</code> and doesn't remove the associated data from the DB. To \"delete\" an organization, you can use the API call shown below. It requires the API key associated with a <code>superAdmin</code> account.</p> <pre><code>curl -XDELETE -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID'\n</code></pre>"},{"location":"cortex/api/api-guide/#obtain-details","title":"Obtain Details","text":"<p>This API call returns the details of an organization as described in the Organization model section.</p> <pre><code>curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID'\n</code></pre> <p>Let's assume that the organization we are seeking to obtain details about is called demo. The <code>curl</code> command would be:</p> <pre><code>curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization/demo'\n</code></pre> <p>and it should return:</p> <pre><code>{\n  \"id\": \"demo\",\n  \"name\": \"demo\",\n  \"status\": \"Active\",\n  \"description\": \"Demo organization\",\n  \"createdAt\": 1520258040437,\n  \"createdBy\": \"superadmin\",\n  \"updatedBy\": \"superadmin\",\n  \"updatedAt\": 1522077420693\n}\n</code></pre>"},{"location":"cortex/api/api-guide/#list-users","title":"List Users","text":"<p>As mentioned above, you can use the API to return the list of all the users declared withing an organization. For that purpose, use the API call shown below with the API key of an <code>orgAdmin</code> or <code>superAdmin</code> account. It supports the <code>range</code> and <code>sort</code> query parameters declared in paging and sorting details.</p> <pre><code>curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID/user'\n</code></pre> <p>and should return a list of users.</p> <p>If one wants to filter/search for some users (active ones for example), there is a search API to use as below:</p> <pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/organization/ORG_ID/user/_search' -d '{\n  \"query\": {}\n}'\n</code></pre> <p>It also supports the <code>range</code> and <code>sort</code> query parameters declared in paging and sorting details.</p>"},{"location":"cortex/api/api-guide/#list-enabled-analyzers","title":"List Enabled Analyzers","text":"<p>To list the analyzers that have been enabled within an organization, use the following API call with the API key of an <code>orgAdmin</code> user:</p> <pre><code>curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/analyzer'\n</code></pre> <p>It should return a list of Analyzers.</p> <p>Please note that this API call does not display analyzers that are disabled. It supports the <code>range</code> and <code>sort</code> query parameters declared in paging and sorting details.</p>"},{"location":"cortex/api/api-guide/#user-apis","title":"User APIs","text":"<p>The following section describes the APIs that allow creating, updating and listing users within an organization.</p>"},{"location":"cortex/api/api-guide/#user-model","title":"User Model","text":"<p>A user is defined by the following attributes:</p> Attribute Description Type <code>id</code> ID/login readonly <code>name</code> Name writable <code>roles</code> Roles. Possible values are: <code>read</code>, <code>read,analyze</code>, <code>read,analyze,orgadmin</code> and <code>superadmin</code> writable <code>status</code> Status (<code>Active</code> or <code>Locked</code>) writable <code>organization</code> organization to which the user belongs (set upon account creation) readonly <code>createdAt</code> Creation date computed <code>createdBy</code> User who created the account computed <code>updatedAt</code> Last update date computed <code>updatedBy</code> User who last updated the account computed <code>hasKey</code> true when the user has an API key computed <code>hasPassword</code> true if the user has a password computed"},{"location":"cortex/api/api-guide/#list-all","title":"List All","text":"<p>This API call allows a <code>superAdmin</code> to list and search all the users of all defined organizations:</p> <pre><code>curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/user'\n</code></pre> <p>This call supports the <code>range</code> and <code>sort</code> query parameters declared in paging and sorting details.</p>"},{"location":"cortex/api/api-guide/#list-users-within-an-organization","title":"List Users within an Organization","text":"<p>This call is described in organization APIs.</p>"},{"location":"cortex/api/api-guide/#search","title":"Search","text":"<p>This API call allows a <code>superAdmin</code> to perform search on the user accounts created in a Cortex instance:</p> <pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/user/_search' -d '{\n  \"query\": {}\n}'\n</code></pre> <p>This call supports the <code>range</code> and <code>sort</code> query parameters declared in paging and sorting details</p>"},{"location":"cortex/api/api-guide/#create_1","title":"Create","text":"<p>This API calls allows you to programmatically create user creation. If the call is made by a <code>superAdmin</code> user, the request must specify the organization to which the user belong in the <code>organization</code> field.</p> <p>If the call is made by an <code>orgAdmin</code> user, the value of <code>organization</code> field must be the same as the user who makes the call: <code>orgAdmin</code> users are allowed to create users only in their organization.</p> <pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/user' -d '{\n  \"name\": \"Demo org Admin\",\n  \"roles\": [\n    \"read\",\n    \"analyze\",\n    \"orgadmin\"\n  ],\n  \"organization\": \"demo\",\n  \"login\": \"demo\"\n}'\n</code></pre> <p>If successful, the call returns a JSON object representing the created user as described above.</p> <pre><code>{\n  \"id\": \"demo\",\n  \"organization\": \"demo\",\n  \"name\": \"Demo org Admin\",\n  \"roles\": [\n    \"read\",\n    \"analyze\",\n    \"orgadmin\"\n  ],\n  \"status\": \"Ok\",\n  \"createdAt\": 1526050123286,\n  \"createdBy\": \"superadmin\",\n  \"hasKey\": false,\n  \"hasPassword\": false\n}\n</code></pre>"},{"location":"cortex/api/api-guide/#update_1","title":"Update","text":"<p>This API call allows updating the writable attributed of a user account. It's available to users with <code>superAdmin</code> or <code>orgAdmin</code> roles. Any user can also use it to update their own information (but obviously not their roles).</p> <pre><code>curl -XPATCH -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN' -d '{\n  \"name\": \"John Doe\",\n  \"roles\": [\n    \"read\",\n    \"analyze\"\n  ],\n  \"status\": \"Locked\"\n}'\n</code></pre> <p>It returns a JSON object representing the updated user as described above.</p>"},{"location":"cortex/api/api-guide/#get-details","title":"Get Details","text":"<p>This call returns the user details. It's available to users with <code>superAdmin</code> roles and to users in the same organization. Every user can also use it to read their own details.</p> <p></p><pre><code>curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN'\n</code></pre> It returns a JSON object representing the user as described previously."},{"location":"cortex/api/api-guide/#set-a-password","title":"Set a Password","text":"<p>This call sets the user's password. It's available to users with <code>superAdmin</code> or <code>orgAdmin</code> roles. Please note that the request needs to be made using HTTPS with a valid certificate on the server's end to prevent credential sniffing or other PITM (Person-In-The-Middle) attacks.</p> <pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN/password/set' -d '{\n  \"password\": \"SOMEPASSWORD\"\n}'\n</code></pre> <p>If successful, the call returns 204 (success / no content).</p>"},{"location":"cortex/api/api-guide/#change-a-password","title":"Change a password","text":"<p>This call allows a given user to change only their own existing password. It is available to all users including <code>superAdmin</code> and <code>orgAdmin</code> ones. Please note that if a <code>superAdmin</code> or an <code>orgAdmin</code> needs to update the password of another user, they must use the <code>/password/set</code> call described in the previous subsection.</p> <pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN/password/change' -d '{\n  \"currentPassword\": \"password\",\n  \"password\": \"new-password\"\n}'\n</code></pre> <p>If successful, the call returns 204 (success / no content).</p>"},{"location":"cortex/api/api-guide/#set-and-renew-an-api-key","title":"Set and Renew an API Key","text":"<p>This calls allows setting and renewing the API key of a user. It's available to users with <code>superAdmin</code> or <code>orgAdmin</code> roles. Any user can also use it to renew their own API key. Again, the request needs to be made using HTTPS with a valid certificate on the server's end to prevent credential sniffing or other PITM (Person-In-The-Middle) attacks. You know the drill ;-)</p> <pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN/key/renew'\n</code></pre> <p>If successful, it returns the generated API key in a <code>text/plain</code>response.</p>"},{"location":"cortex/api/api-guide/#get-an-api-key","title":"Get an API Key","text":"<p>This calls allows getting a user's API key. It's available to users with <code>superAdmin</code> or <code>orgAdmin</code> roles. Any user can also use it to obtain their own API key.</p> <pre><code>curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN/key'\n</code></pre> <p>If successful, the generated API key is returned in <code>text/plain</code>response</p>"},{"location":"cortex/api/api-guide/#revoke-an-api-key","title":"Revoke an API Key","text":"<p>This calls allow revoking a user's API key. This calls allow revoking a user's API key.</p> <pre><code>curl -XDELETE -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/user/USER_LOGIN/key'\n</code></pre> <p>A successful request returns nothing (HTTP 200 OK).</p>"},{"location":"cortex/api/api-guide/#job-apis","title":"Job APIs","text":"<p>The following section describes the APIs that allow manipulating jobs. Jobs are basically submissions made to analyzers and the resulting reports.</p>"},{"location":"cortex/api/api-guide/#job-model","title":"Job Model","text":"<p>A job is defined by the following attributes:</p> Attribute Description Type <code>id</code> Job ID computed <code>organization</code> The organization to which the job belongs readonly <code>analyzerDefinitionId</code> Analyzer definition name readonly <code>analyzerId</code> Instance ID of the analyzer to which the job is associated readonly <code>organization</code> Organization to which the user belongs (set upon account creation) readonly <code>analyzerName</code> Name of the analyzer to which the job is associated readonly <code>dataType</code> the datatype of the analyzed observable readonly <code>status</code> Status of the job (<code>Waiting</code>, <code>InProgress</code>, <code>Success</code>, <code>Failure</code>, <code>Deleted</code>) computed <code>data</code> Value of the analyzed observable (does not apply to <code>file</code> observables) readonly <code>attachment</code> JSON object representing <code>file</code> observables (does not apply to non-<code>file</code> observables). It  defines the<code>name</code>, <code>hashes</code>, <code>size</code>, <code>contentType</code> and <code>id</code> of the <code>file</code> observable readonly <code>parameters</code> JSON object of key/value pairs set during job creation readonly <code>message</code> A free text field to set additional text/context for a job readonly <code>tlp</code> The TLP of the analyzed observable readonly <code>startDate</code> Start date computed <code>endDate</code> End date computed <code>createdAt</code> Creation date. Please note that a job can be requested but not immediately honored. The actual time at which it is started is the value of <code>startDate</code> computed <code>createdBy</code> User who created the job computed <code>updatedAt</code> Last update date (only Cortex updates a job when it finishes) computed <code>updatedBy</code> User who submitted the job and which identity is used by Cortex to update the job once it is finished computed"},{"location":"cortex/api/api-guide/#list-and-search","title":"List and Search","text":"<p>This call allows a user with <code>read</code>,<code>analyze</code> or <code>orgAdmin</code> role to list and search all the analysis jobs made by their organization.</p> <p>If you want to list all the jobs: </p><pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/_search?range=all'\n</code></pre> <p>If you want to list 10 jobs: </p><pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/_search'\n</code></pre> <p>If you want to list 100 jobs: </p><pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/_search?range=0-100'\n</code></pre> <p>If you want to search jobs according to various criteria: </p><pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/job/_search' -d '{\n  \"query\": {\n    \"_and\": [\n      {\"status\": \"Success\"},\n      {\"dataType\": \"ip\"}\n    ]\n  }\n}'\n</code></pre> <p>This call supports the <code>range</code> and <code>sort</code> query parameters declared in paging and sorting details</p>"},{"location":"cortex/api/api-guide/#get-details_1","title":"Get Details","text":"<p>This call allows a user with <code>read</code>,<code>analyze</code> or <code>orgAdmin</code> role to get the details of a job. It does not fetch the job report.</p> <pre><code>curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/JOB_ID'\n</code></pre> <p>It returns a JSON response with the following structure:</p> <pre><code>{\n  \"id\": \"AWNei4vH3rJ8unegCPB9\",\n  \"analyzerDefinitionId\": \"Abuse_Finder_2_0\",\n  \"analyzerId\": \"220483fde9608c580fb6a2508ff3d2d3\",\n  \"analyzerName\": \"Abuse_Finder_2_0\",\n  \"status\": \"Success\",\n  \"data\": \"8.8.8.8\",\n  \"parameters\": \"{}\",\n  \"tlp\": 0,\n  \"message\": \"\",\n  \"dataType\": \"ip\",\n  \"organization\": \"demo\",\n  \"startDate\": 1526299593923,\n  \"endDate\": 1526299597064,\n  \"date\": 1526299593633,\n  \"createdAt\": 1526299593633,\n  \"createdBy\": \"demo\",\n  \"updatedAt\": 1526299597066,\n  \"updatedBy\": \"demo\"\n}\n</code></pre>"},{"location":"cortex/api/api-guide/#get-details-and-report","title":"Get Details and Report","text":"<p>This call allows a user with <code>read</code>,<code>analyze</code> or <code>orgAdmin</code> role to get the details of a job including its report.</p> <pre><code>curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/JOB_ID/report'\n</code></pre> <p>It returns a JSON response with the structure below. If the job is not yet completed, the <code>report</code> field contains a string representing the job status:</p> <pre><code>{\n  \"id\": \"AWNei4vH3rJ8unegCPB9\",\n  \"analyzerDefinitionId\": \"Abuse_Finder_2_0\",\n  \"analyzerId\": \"220483fde9608c580fb6a2508ff3d2d3\",\n  \"analyzerName\": \"Abuse_Finder_2_0\",\n  \"status\": \"Success\",\n  \"data\": \"8.8.8.8\",\n  \"parameters\": \"{}\",\n  \"tlp\": 0,\n  \"message\": \"\",\n  \"dataType\": \"ip\",\n  \"organization\": \"demo\",\n  \"startDate\": 1526299593923,\n  \"endDate\": 1526299597064,\n  \"date\": 1526299593633,\n  \"createdAt\": 1526299593633,\n  \"createdBy\": \"demo\",\n  \"updatedAt\": 1526299597066,\n  \"updatedBy\": \"demo\",\n  \"report\": {\n    \"summary\": {\n      \"taxonomies\": [\n        {\n          \"predicate\": \"Address\",\n          \"namespace\": \"Abuse_Finder\",\n          \"value\": \"network-abuse@google.com\",\n          \"level\": \"info\"\n        }\n      ]\n    },\n    \"full\": {\n      \"abuse_finder\": {\n        \"raw\": \"...\",\n        \"abuse\": [\n          \"network-abuse@google.com\"\n        ],\n        \"names\": [\n          \"Google LLC\",\n          \"Level 3 Parent, LLC\"\n        ],\n        \"value\": \"8.8.8.8\"\n      }\n    },\n    \"success\": true,\n    \"artifacts\": []\n  }\n}\n</code></pre>"},{"location":"cortex/api/api-guide/#wait-and-get-job-report","title":"Wait and Get Job Report","text":"<p>This call is similar the one described above but allows the user to provide a timeout to wait for the report in case it is not available at the time the query was made:</p> <pre><code>curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/JOB_ID/waitreport?atMost=1minute'\n</code></pre> <p>The <code>atMost</code> is a duration using the format <code>Xhour</code>, <code>Xminute</code> or <code>Xsecond</code>.</p>"},{"location":"cortex/api/api-guide/#get-artifacts","title":"Get Artifacts","text":"<p>This call allows a user with <code>read</code>,<code>analyze</code> or <code>orgAdmin</code> role to get the extracted artifacts from a job if such extraction has been enabled in the corresponding analyzer configuration. Please note that extraction is imperfect and you might have inconsistent or incorrect data.</p> <pre><code>curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/JOB_ID/artifacts'\n</code></pre> <p>It returns a JSON array with the following structure:</p> <pre><code>[\n  {\n    \"dataType\": \"ip\",\n    \"createdBy\": \"demo\",\n    \"data\": \"8.8.8.8\",\n    \"tlp\": 0,\n    \"createdAt\": 1525432900553,\n    \"id\": \"AWMq4tvLjidKq_asiwcl\"\n  }\n]\n</code></pre>"},{"location":"cortex/api/api-guide/#delete_1","title":"Delete","text":"<p>This API allows a user with <code>analyze</code> or <code>orgAdmin</code> role to delete a job:</p> <pre><code>curl -XDELETE -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/job/JOB_ID'\n</code></pre> <p>This marks the job as <code>Deleted</code>. However the job's data is not removed from the  database.</p>"},{"location":"cortex/api/api-guide/#analyzer-apis","title":"Analyzer APIs","text":"<p>The following section describes the APIs that allow manipulating analyzers.</p>"},{"location":"cortex/api/api-guide/#analyzer-model","title":"Analyzer Model","text":"<p>An analyzer is defined by the following attributes:</p> Attribute Description Type <code>id</code> Analyzer ID once enabled within an organization readonly <code>analyzerDefinitionId</code> Analyzer definition name readonly <code>name</code> Name of the analyzer readonly <code>version</code> Version of the analyzer readonly <code>description</code> Description of the analyzer readonly <code>author</code> Author of the analyzer readonly <code>url</code> URL where the analyzer has been published readonly <code>license</code> License of the analyzer readonly <code>dataTypeList</code> Allowed datatypes readonly <code>baseConfig</code> Base configuration name. This identifies the shared set of configuration with all the analyzer's flavors readonly <code>jobCache</code> Report cache timeout in minutes, visible for <code>orgAdmin</code> users only writable <code>rate</code> Numeric amount of analyzer calls authorized for the specified <code>rateUnit</code>, visible for <code>orgAdmin</code> users only writable <code>rateUnit</code> Period of availability of the rate limite: <code>Day</code> or <code>Month</code>, visible for <code>orgAdmin</code> users only writable <code>configuration</code> A JSON object where key/value pairs represent the config names, and their values. It includes the default properties <code>proxy_http</code>, <code>proxy_https</code>, <code>auto_extract_artifacts</code>, <code>check_tlp</code>, and <code>max_tlp</code>, visible for <code>orgAdmin</code> users only writable <code>createdBy</code> User who enabled the analyzer computed <code>updatedAt</code> Last update date computed <code>updatedBy</code> User who last updated the analyzer computed"},{"location":"cortex/api/api-guide/#enable","title":"Enable","text":"<p>This call allows a user with an <code>orgAdmin</code> role to enable an analyzer.</p> <pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/organization/analyzer/:analyzerId' -d '{\n  \"name\": \"Censys_1_0\",\n  \"configuration\": {\n    \"uid\": \"XXXX\",\n    \"key\": \"XXXXXXXXXXXXXXXXXXXX\",\n    \"proxy_http\": \"http://proxy:9999\",\n    \"proxy_https\": \"http://proxy:9999\",\n    \"auto_extract_artifacts\": false,\n    \"check_tlp\": true,\n    \"max_tlp\": 2\n  },\n  \"rate\": 1000,\n  \"rateUnit\": \"Day\",\n  \"jobCache\": 5\n}'\n</code></pre>"},{"location":"cortex/api/api-guide/#list-and-search_1","title":"List and Search","text":"<p>These calls allow a user with a <code>analyze</code> or <code>orgAdmin</code> role to list and search all the enabled analyzers within the organization.</p> <pre><code>curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/analyzer'\n</code></pre> <p>or</p> <pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/analyzer/_search' -d '{\n  \"query\": {}\n}'\n</code></pre> <p>Both calls supports the <code>range</code> and <code>sort</code> query parameters declared in paging and sorting details, and both return a JSON array of analyzer objects as described in Analyzer Model section.</p> <p>If called by a user with only an <code>nalyzer</code> role, the <code>configuration</code> attribute is not included on the JSON objects.</p>"},{"location":"cortex/api/api-guide/#get-details_2","title":"Get Details","text":"<p>This call allows a user with a <code>analyze</code> or <code>orgAdmin</code> role to get an analyzer's details.</p> <pre><code>curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID'\n</code></pre> <p>It returns a analyzer JSON object as described in Analyzer Model section.</p> <p>If called by a user with only an <code>nalyzer</code> role, the <code>configuration</code> attribute is not included on the JSON objects.</p>"},{"location":"cortex/api/api-guide/#get-by-type","title":"Get By Type","text":"<p>This call is mostly used by TheHive and allows to quickly get the list of analyzers that can run on the given datatype. It requires an <code>analyze</code> or <code>orgAdmin</code> role.</p> <pre><code>curl -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/analyzer/type/DATA_TYPE'\n</code></pre> <p>It returns a JSON array of analyzer objects as described in Analyzer Model section without the <code>configuration</code> attribute, which could contain sensitive data.</p>"},{"location":"cortex/api/api-guide/#update_2","title":"Update","text":"<p>This call allows an <code>orgAdmin</code> user to update the <code>name</code>, <code>configuration</code> and <code>jobCache</code> of an enabled analyzer.</p> <pre><code>curl -XPATCH -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID' -d '{\n  \"configuration\": {\n    \"key\": \"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\n    \"polling_interval\": 60,\n    \"proxy_http\": \"http://localhost:8080\",\n    \"proxy_https\": \"http://localhost:8080\",\n    \"auto_extract_artifacts\": true,\n    \"check_tlp\": true,\n    \"max_tlp\": 1\n  },\n  \"name\": \"Shodan_Host_1_0\",\n  \"rate\": 1000,\n  \"rateUnit\": \"Day\",\n  \"jobCache\": null\n}'\n</code></pre> <p>It returns a JSON object describing the analyzer as defined in Analyzer Model section.</p>"},{"location":"cortex/api/api-guide/#run","title":"Run","text":"<p>This API allows a user with a <code>analyze</code> or <code>orgAdmin</code> role to run analyzers on observables of different datatypes.</p> <p>For <code>file</code> observables, the API call must be made as described below:</p> <pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID/run' \\\n  -F 'attachment=@/path/to/observable-file' \\\n  -F '_json=&lt;-;type=application/json' &lt;&lt; _EOF_\n  {\n    \"dataType\":\"file\",\n    \"tlp\":0\n  }\n_EOF_\n</code></pre> <p>for all the other types of observerables, the request is:</p> <pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID/run' -d '{\n  \"data\":\"8.8.8.8\",\n  \"dataType\":\"ip\",\n  \"tlp\":0,\n  \"message\": \"A message that can be accessed from the analyzer\",\n  \"parameters\": {\n    \"key1\": \"value1\",\n    \"key2\": \"value2\"\n  }\n}'\n</code></pre> <p>This call will fetch a similar job from the cache, and if it finds one, it returns it from the cache, based on the duration defined in <code>jobCache</code> attribute of the analyzer.</p> <p>To force bypassing the cache, one can add the following query parameter: <code>force=1</code></p> <pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID/run?force=1' -d '{\n  \"data\":\"8.8.8.8\",\n  \"dataType\":\"ip\",\n  \"tlp\":0,\n  \"message\": \"A message that can be accessed from the analyzer\",\n  \"parameters\": {\n    \"key1\": \"value1\",\n    \"key2\": \"value2\"\n  }\n}'\n</code></pre>"},{"location":"cortex/api/api-guide/#disable","title":"Disable","text":"<p>This API allows an <code>orgAdmin</code> to disable an existing analyzer in their organization and delete the corresponding configuration.</p> <pre><code>curl -XDELETE -H 'Authorization: Bearer **API_KEY**' 'https://CORTEX_APP_URL:9001/api/analyzer/ANALYZER_ID'\n</code></pre>"},{"location":"cortex/api/api-guide/#miscellaneous-apis","title":"Miscellaneous APIs","text":""},{"location":"cortex/api/api-guide/#paging-and-sorting","title":"Paging and Sorting","text":"<p>All the <code>search</code> API calls allow sorting and paging parameters, in addition to a query in the request's body. These calls usually have URLs ending with the <code>_search</code> keyword but that's not always the case.</p> <p>The followings are query parameters:</p> <ul> <li><code>range</code>: <code>all</code> or <code>x-y</code> where <code>x</code> and <code>y</code> are numbers (ex: 0-10).</li> <li><code>sort</code>: you can provide multiple sort criteria such as: <code>-createdAt</code> or <code>+status</code>.</li> </ul> <p>Example:</p> <pre><code>curl -XPOST -H 'Authorization: Bearer **API_KEY**' -H 'Content-Type: application/json' 'http://CORTEX_APP_URL:9001/api/organization/ORG_ID/user?range=0-10&amp;sort=-createdAt&amp;sort=+status' -d '{\n  \"query\": {}\n}'\n</code></pre>"},{"location":"cortex/api/how-to-create-a-responder/","title":"How to create a Responder","text":""},{"location":"cortex/api/how-to-create-a-responder/#how-to-write-and-submit-a-responder","title":"How to Write and Submit a Responder","text":""},{"location":"cortex/api/how-to-create-a-responder/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Writing a Responder<ul> <li>The Program</li> <li>Service Interaction Files (Flavors)</li> <li>Python Requirements</li> <li>Example: Mailer Responder Files</li> <li>Input</li> <li>Service Interaction Configuration Items</li> <li>Responder Configuration in the Global Configuration File</li> <li>Output</li> <li>The Cortexutils Python Library</li> </ul> </li> <li>Submitting a Responder<ul> <li>Check Existing Issues</li> <li>Open an Issue</li> <li>Review your Service Interaction File(s)</li> <li>Provide the List of Requirements</li> <li>Verify Execution</li> <li>Create a Pull Request</li> </ul> </li> <li>Need Help?</li> </ul>"},{"location":"cortex/api/how-to-create-a-responder/#writing-a-responder","title":"Writing a Responder","text":"<p>A responder is a program that takes JSON input and do an action and produces a basic result of that action. Responders are very similar to analyzers though they have different purposes. Responders are made of at least 2 types of files:</p> <ul> <li>The program itself</li> <li>One or several service interaction files or flavors</li> <li>A Python requirements file, which is only necessary if the responder is written in Python.</li> </ul>"},{"location":"cortex/api/how-to-create-a-responder/#the-program","title":"The Program","text":"<p>The first type of files a responder is made of is the core program that performs actions. It can be written in any programming language that is supported by Linux.</p> <p>you can write your responders in Python, Ruby, Perl or even Scala. However, the very handy <code>Cortexutils</code> library described below is in Python. It greatly facilitates responder development and it also provides some methods to quickly format the output to make it compliant with the JSON schema expected by TheHive.</p>"},{"location":"cortex/api/how-to-create-a-responder/#service-interaction-files-flavors","title":"Service Interaction Files (Flavors)","text":"<p>A responder must have at least one service interaction file. Such files contain key configuration information such as the responder's author information, the datatypes (<code>thehive:case</code>, <code>thehive:alert</code>, ...) the responder accepts as input and to which it applies to if used from TheHive, the TLP and PAP (or Permissible Actions Protocol) above which it will refuse to execute to protect against data leakage and to enforce sane OPSEC practices and so on.</p> <p>A responder can have two or more service interaction files to allow it to perform different actions.  We speak then of flavors. For example, a Mailer responder can send message using several body templates.</p>"},{"location":"cortex/api/how-to-create-a-responder/#python-requirements","title":"Python Requirements","text":"<p>If the responder is written in Python, a <code>requirements.txt</code> must be provided with the list of all the dependencies.</p>"},{"location":"cortex/api/how-to-create-a-responder/#example-mailer-responder-files","title":"Example: Mailer Responder Files","text":"<p>Below is a directory listing of the files corresponding to a Mailer responder.</p> <pre><code>responders/Mailer\n|-- Mailer.json\n|-- requirements.txt\n`-- mailer.py\n</code></pre>"},{"location":"cortex/api/how-to-create-a-responder/#input","title":"Input","text":"<p>The input of a responder can be any JSON data, even a simple string. The submitter must send data with the structure expected by the program. The acceptable datatypes described in the Service Interaction files indicate what kind of data is expected. For example, if the program requires a <code>thehive:case</code> (i.e. it applies at the case level in TheHive), input must comply with TheHive case. Below an example of <code>thehive:case</code> input.</p> <pre><code>    {\n        \"data\": {\n        \"updatedAt\": 1606230814019,\n        \"tlp\": 2,\n        \"endDate\": 1606230814019,\n        \"description\": \"Case Description\",\n        \"tags\": [\n            \"tag\"\n        ],\n        \"caseId\": 157,\n        \"customFields\": {},\n        \"pap\": 2,\n        \"status\": \"Open\",\n        \"resolutionStatus\": \"Indeterminate\",\n        \"createdAt\": 1606183201646,\n        \"createdBy\": \"user\",\n        \"flag\": false,\n        \"severity\": 2,\n        \"metrics\": {},\n        \"owner\": \"user\",\n        \"title\": \"Title\",\n        \"updatedBy\": \"user\",\n        \"startDate\": 1606183201000,\n        \"impactStatus\": \"NotApplicable\",\n        \"_type\": \"case\",\n        \"_routing\": \"iwD693UBlJefU8pMrqOq\",\n        \"_parent\": null,\n        \"_id\": \"iwD693UBlJefU8pMrqOq\",\n        \"_seqNo\": 4572,\n        \"_primaryTerm\": 48,\n        \"id\": \"iwD693UBlJefU8pMrqOq\"\n        },\n        \"dataType\": \"thehive:case\",\n        \"tlp\": 2,\n        \"pap\": 2,\n        \"message\": \"\",\n        \"parameters\": {\n        \"user\": \"user\"\n        },\n        \"config\": {\n        \"proxy_https\": null,\n        \"cacerts\": null,\n        \"max_pap\": 2,\n        \"jobTimeout\": 30,\n        \"api_key\": \"3bDBUb7EL409MHOmXBkqsysZ1vpTab1Q\",\n        \"check_tlp\": true,\n        \"proxy_http\": null,\n        \"max_tlp\": 2,\n        \"url\": \"configured_url\",\n        \"check_pap\": true\n        }\n    }\n</code></pre> <p>In the addition to the input (<code>data</code> section) sent by the submitter, Cortex adds the <code>config</code> section which is the responder's specific configuration provided by an <code>orgAdmin</code> user when the responder is enabled in the Cortex UI. </p>"},{"location":"cortex/api/how-to-create-a-responder/#example-service-interaction-file-for-the-mailer-responder","title":"Example: Service Interaction File for the Mailer Responder","text":"<p>The <code>&lt;==</code> sign and anything after it are comments that do no appear in the original file. </p><pre><code>{\n  \"name\": \"Mailer\",\n  \"version\": \"1.0\",\n  \"author\": \"CERT-BDF\",\n  \"url\": \"https://github.com/TheHive-Project/Cortex-Analyzers\",\n  \"license\": \"AGPL-V3\",\n  \"description\": \"Send an email with information from a TheHive case or alert\",\n  \"dataTypeList\": [\"thehive:case\", \"thehive:alert\", \"thehive:case_task\"],\n  \"command\": \"Mailer/mailer.py\",\n  \"baseConfig\": \"Mailer\",\n  \"configurationItems\": [\n    {\n      \"name\": \"from\",\n      \"description\": \"email address from which the mail is send\",\n      \"type\": \"string\", &lt;== defines what kind of data type the configuration item is (string, number)\n      \"multi\": false, &lt;== setting multi to true allows to pass a list of items\n      \"required\": true\n    },\n    {\n      \"name\": \"smtp_host\",\n      \"description\": \"SMTP server used to send mail\",\n      \"type\": \"string\",\n      \"multi\": false,\n      \"required\": true,\n      \"defaultValue\": \"localhost\"\n    },\n    {\n      \"name\": \"smtp_port\",\n      \"description\": \"SMTP server port\",\n      \"type\": \"number\",\n      \"multi\": false,\n      \"required\": true,\n      \"defaultValue\": 25\n    },\n    {\n      \"name\": \"smtp_user\",\n      \"description\": \"SMTP server user\",\n      \"type\": \"string\",\n      \"multi\": false,\n      \"required\": false,\n      \"defaultValue\": \"user\"\n    },\n    {\n      \"name\": \"smtp_pwd\",\n      \"description\": \"SMTP server password\",\n      \"type\": \"string\",\n      \"multi\": false,\n      \"required\": false,\n      \"defaultValue\": \"pwd\"\n    }\n  ]\n}\n</code></pre>"},{"location":"cortex/api/how-to-create-a-responder/#service-interaction-configuration-items","title":"Service Interaction Configuration Items","text":""},{"location":"cortex/api/how-to-create-a-responder/#name","title":"name","text":"<p>Name of the specific service (or flavor) of the responder.</p> <p>If your responder has only one service interaction (i.e. performs only one action), it is the name of the responder's directory.</p> <p>If your responder performs several actions (i.e. comes in several flavors), you have to give a specific and meaningful name to each flavor.</p> <p>Each flavor's name appear in TheHive's responder list and in MISP when you use Cortex for attribute enrichment.</p>"},{"location":"cortex/api/how-to-create-a-responder/#version","title":"version","text":"<p>The version of the responder.</p> <p>You must increase major version numbers when new features are added, modifications are made to take into account API changes, report output is modified or when report templates (more on this later) are updated.</p> <p>You must increase minor version numbers when bugs are fixed.</p>"},{"location":"cortex/api/how-to-create-a-responder/#author","title":"author","text":"<p>You must provide your full name and/or your organization/team name when submitting a responder. Pseudos are not accepted. If you'd rather remain anonymous, please contact us at support@thehive-project.org prior to submitting your responder.</p>"},{"location":"cortex/api/how-to-create-a-responder/#url","title":"url","text":"<p>The URL where the responder is stored. This should ideally be <code>https://github.com/TheHive-Project/Cortex-Analyzers</code></p>"},{"location":"cortex/api/how-to-create-a-responder/#license","title":"license","text":"<p>The license of the code. Ideally, we recommend using the AGPL-v3 license.</p> <p>Make sure your code's license is compatible with the license(s) of the various components and libraries you use if applicable.</p>"},{"location":"cortex/api/how-to-create-a-responder/#description","title":"description","text":"<p>Description of the responder. Please be concise and clear. The description is  shown in the Cortex UI and TheHive.</p>"},{"location":"cortex/api/how-to-create-a-responder/#datatypelist","title":"dataTypeList","text":"<p>The list of TheHive datatypes supported by the responder. Currently TheHive accepts the following datatypes:</p> <ul> <li><code>thehive:case</code></li> <li><code>thehive:case_artifact</code> (i.e. observable)</li> <li><code>thehive:alert</code></li> <li><code>thehive:case_task</code></li> <li><code>thehive:case_task_log</code> (i.e. task log)</li> </ul>"},{"location":"cortex/api/how-to-create-a-responder/#baseconfig","title":"baseConfig","text":"<p>Name used to group configuration items common to several responders. This prevent the user to enter the same API key for all responder flavors. The Cortex responder config page group configuration items by their <code>baseConfig</code>.</p>"},{"location":"cortex/api/how-to-create-a-responder/#config","title":"config","text":"<p>Configuration dedicated to the responder's flavor. This is where we  typically specify the TLP level of observables allowed to be analyzed with the  <code>check_tlp</code> and <code>max_tlp</code> parameters. For example, if <code>max_tlp</code> is set to <code>2</code> (TLP:AMBER), TLP:RED observables cannot be analyzed.</p>"},{"location":"cortex/api/how-to-create-a-responder/#max_tlp","title":"max_tlp","text":"<p>The TLP level above which the responder must not be executed.</p> TLP max_tlp value Unknown -1 WHITE 0 GREEN 1 AMBER 2 RED 3"},{"location":"cortex/api/how-to-create-a-responder/#check_tlp","title":"check_tlp","text":"<p>This is a boolean parameter. When <code>true</code>, <code>max_tlp</code> is checked. And if the input's TLP is above <code>max_tlp</code>, the responder is not executed.</p> <p>For consistency reasons, we do recommend setting both <code>check_tlp</code> and <code>max_tlp</code> even if <code>check_tlp</code> is set to <code>false</code>.</p>"},{"location":"cortex/api/how-to-create-a-responder/#max_pap","title":"max_pap","text":"<p>The PAP level above which the responder must not be executed.</p> TLP max_tlp value Unknown -1 WHITE 0 GREEN 1 AMBER 2 RED 3"},{"location":"cortex/api/how-to-create-a-responder/#check_pap","title":"check_pap","text":"<p>This is a boolean parameter. When <code>true</code>, <code>max_pap</code> is checked. And if the input's PAP is above <code>max_pap</code>, the responder is not executed.</p> <p>For consistency reasons, we do recommend setting both <code>check_pap</code> and <code>max_pap</code> even if <code>check_pap</code> is set to <code>false</code>.</p>"},{"location":"cortex/api/how-to-create-a-responder/#command","title":"command","text":"<p>The command used to run the responder. That's typically the full, absolute path to the main program file.</p>"},{"location":"cortex/api/how-to-create-a-responder/#configurationitems","title":"configurationItems","text":"<p>The list of configurationItems is necessary in order to be able to set all configuration variables for responders directly in the Cortex 2 user interface. As in the VirusTotal example above can be seen, every item is a json object that defines: - name (string) - description (string) - type (string) - multi (boolean) - required (boolean) - defaultValue (according to type, optional)</p> <p>The <code>multi</code> parameter allows to pass a list as configuration variable instead of a single string or number. This is used e.g. in the MISP responder that queries multiple servers in one run and needs different parameters for that.</p>"},{"location":"cortex/api/how-to-create-a-responder/#output","title":"Output","text":"<p>The output of a responder depends on the success or failure of its execution.</p> <p>If the responder fails to execute:</p> <pre><code>{\n    \"success\": false,\n    \"errorMessage\":\"..\"\n}\n</code></pre> <ul> <li>When <code>success</code> is set to <code>false</code>, it indicates that something went wrong     during the execution.</li> <li><code>errorMessage</code> is free text - typically the error output message.</li> </ul> <p>If the responder succeeds (i.e. it runs without any error):</p> <pre><code>{\n    \"success\":true,\n    \"full\":{ \"message\": \"..\" },\n    \"operations\":[]\n}\n</code></pre> <ul> <li>When <code>success</code> is set to <code>true</code>, it indicates that the responder ran     successfully.</li> <li><code>full</code> is the full report of the responder. It must contain at least     a message.</li> <li><code>operations</code> is a list what the submitter system should execute.     As of version 3.1.0, TheHive accepts the following operations:<ul> <li><code>AddTagToArtifact</code> (<code>{ \"type\": \"AddTagToArtifact\", \"tag\": \"tag to add\" }</code>): add      a tag to the artifact related to the object</li> <li><code>AddTagToCase</code> (<code>{ \"type\": \"AddTagToCase\", \"tag\": \"tag to add\" }</code>): add      a tag to the case related to the object</li> <li><code>MarkAlertAsRead</code>: mark the alert related to the object as read</li> <li><code>AddCustomFields</code> (<code>{\"name\": \"key\", \"value\": \"value\", \"tpe\": \"type\"</code>): add a custom field to the case related to the object</li> </ul> </li> </ul> <p>The list of acceptable operations will increase in future releases of TheHive.</p>"},{"location":"cortex/api/how-to-create-a-responder/#the-cortexutils-python-library","title":"The Cortexutils Python Library","text":"<p>So far, all the published responders have been written in Python. We provide a Python library called <code>cortexutils</code> to help developers easily write their programs. Note though that Python is not mandatory for responder coding and any language that runs on Linux can be used, though you won't have the benefits of the CortexUtils library.</p> <p>Cortexutils can be used with Python 2 and 3. Due to the end of life from Python2 it is strongly advised to work as much with Python3 as possible. To install it :</p> <pre><code>pip install cortexutils\n</code></pre> <p>or</p> <pre><code>pip3 install cortexutils\n</code></pre> <p>This library is already used by all the responders published in our Github repository. Feel free to start reading the code of some of them before writing your own.</p>"},{"location":"cortex/api/how-to-create-a-responder/#submitting-a-responder","title":"Submitting a Responder","text":"<p>We highly encourage you to share your responders with the community through our Github repository. To do so, we invite you to follow a few steps before submitting a pull request.</p>"},{"location":"cortex/api/how-to-create-a-responder/#check-existing-issues","title":"Check Existing Issues","text":"<p>Start by checking if an issue already exists for the responder you'd like to write and contribute. Verify that nobody is working on it. If an issue exists and has the in progress, under review or pr-submitted label, it means somebody is already working on the code or has finished it.</p> <p>If you are short on ideas, check issues with a help wanted label. If one of those issues interest you, indicate that you are working on it.</p>"},{"location":"cortex/api/how-to-create-a-responder/#open-an-issue","title":"Open an Issue","text":"<p>If there's no issue open for the responder you'd like to contribute, open one. Indicate that you are working on it to avoid having someone start coding it.</p> <p>You have to create an issue for each responder you'd like to submit.</p>"},{"location":"cortex/api/how-to-create-a-responder/#review-your-service-interaction-files","title":"Review your Service Interaction File(s)","text":"<p>Review your service interaction files. For example, let's check the Mailer JSON responder configuration file(s):</p> <p></p><pre><code>{\n  \"name\": \"Mailer\",\n  \"version\": \"1.0\",\n  \"author\": \"CERT-BDF\",\n  \"url\": \"https://github.com/TheHive-Project/Cortex-Analyzers\",\n  \"license\": \"AGPL-V3\",\n  \"description\": \"Send an email with information from a TheHive case or alert\",\n  \"dataTypeList\": [\"thehive:case\", \"thehive:alert\", \"thehive:case_task\"],\n  \"command\": \"Mailer/mailer.py\",\n  \"baseConfig\": \"Mailer\",\n  \"configurationItems\": [\n    {\n      \"name\": \"from\",\n      \"description\": \"email address from which the mail is send\",\n      \"type\": \"string\", &lt;== defines what kind of data type the configuration item is (string, number)\n      \"multi\": false, &lt;== setting multi to true allows to pass a list of items\n      \"required\": true\n    },\n    {\n      \"name\": \"smtp_host\",\n      \"description\": \"SMTP server used to send mail\",\n      \"type\": \"string\",\n      \"multi\": false,\n      \"required\": true,\n      \"defaultValue\": \"localhost\"\n    },\n    {\n      \"name\": \"smtp_port\",\n      \"description\": \"SMTP server port\",\n      \"type\": \"number\",\n      \"multi\": false,\n      \"required\": true,\n      \"defaultValue\": 25\n    },\n    {\n      \"name\": \"smtp_user\",\n      \"description\": \"SMTP server user\",\n      \"type\": \"string\",\n      \"multi\": false,\n      \"required\": false,\n      \"defaultValue\": \"user\"\n    },\n    {\n      \"name\": \"smtp_pwd\",\n      \"description\": \"SMTP server password\",\n      \"type\": \"string\",\n      \"multi\": false,\n      \"required\": false,\n      \"defaultValue\": \"pwd\"\n    }\n  ]\n}\n</code></pre> Ensure that all information is correct and particularly the <code>author</code> and <code>license</code> parameters."},{"location":"cortex/api/how-to-create-a-responder/#provide-the-list-of-requirements","title":"Provide the List of Requirements","text":"<p>If your responder is written in Python, make sure to complete the <code>requirements.txt</code> file with the list of all the external libraries that are needed to run the responder correctly.</p>"},{"location":"cortex/api/how-to-create-a-responder/#verify-execution","title":"Verify Execution","text":"<p>Use these three simple checks before submitting your responder:</p> <ul> <li>Ensure it works with the expected configuration, TLP, PAP or datatype.</li> <li>Ensure it works with missing configuration, PAP, datatype or TLP: your responder must generate an explicit error message.</li> </ul>"},{"location":"cortex/api/how-to-create-a-responder/#create-a-pull-request","title":"Create a Pull Request","text":"<p>Create one Pull Request per responder against the develop branch of the Cortex-Analyzers repository. Reference the issue you've created in your PR.</p> <p>We have to review your responders. Distinct PRs will allow us to review them more quickly and release them to the benefit of the whole community.</p>"},{"location":"cortex/api/how-to-create-a-responder/#need-help","title":"Need Help?","text":"<p>Something does not work as expected? No worries, we got you covered. Please join our user forum,  contact us on Gitter, or send us  an email at support@thehive-project.org. We are here to help.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/","title":"How to create an Analyzer","text":""},{"location":"cortex/api/how-to-create-an-analyzer/#how-to-write-and-submit-an-analyzer","title":"How to Write and Submit an Analyzer","text":""},{"location":"cortex/api/how-to-create-an-analyzer/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Writing an Analyzer<ul> <li>The Program</li> <li>Service Interaction Files (Flavors)</li> <li>Python Requirements</li> <li>Example: VirusTotal Analyzer Files</li> <li>Input</li> <li>Service Interaction Configuration Items</li> <li>Analyzer Configuration in the Global Configuration File</li> <li>Output</li> <li>The Cortexutils Python Library</li> <li>Report Templates</li> </ul> </li> <li>Submitting an Analyzer<ul> <li>Check Existing Issues</li> <li>Open an Issue</li> <li>Review your Service Interaction File(s)</li> <li>Provide the List of Requirements</li> <li>Check the Taxonomy</li> <li>Provide Global Configuration Parameters</li> <li>Verify Execution</li> <li>Create a Pull Request</li> </ul> </li> <li>Need Help?</li> </ul>"},{"location":"cortex/api/how-to-create-an-analyzer/#writing-an-analyzer","title":"Writing an Analyzer","text":"<p>An analyzer is a program that takes an observable and configuration information as raw input, analyze the observable and produces a result as raw output. It is made of at least 2 types of files:</p> <ul> <li>The program itself</li> <li>One or several service interaction files or flavors</li> <li>A Python requirements file, which is only necessary if the analyzer is written in Python.</li> </ul>"},{"location":"cortex/api/how-to-create-an-analyzer/#the-program","title":"The Program","text":"<p>The first type of files an analyzer is made of is the core program that performs actions. It can be written in any programming language that is supported by Linux.</p> <p>While many analyzers are written in Python (<code>*.py</code> files), you can write yours in Ruby, Perl or even Scala. However, the very handy <code>Cortexutils</code> library described below is in Python. It greatly facilitates analyzer development and it also provides some methods to quickly format the output to make it compliant with the JSON schema expected by TheHive.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#service-interaction-files-flavors","title":"Service Interaction Files (Flavors)","text":"<p>An analyzer must have at least one service interaction file. Such files contain key configuration information such as the analyzer's author information, the datatypes (IP, URL, hash, domain...) the analyzer accepts as input, the TLP and PAP (Permissible Actions Protocol) above which it will refuse to execute to protect against data leakage and to enforce sane OPSEC practices and so on.</p> <p>An analyzer can have two or more service interaction files to allow it to perform different actions.  We speak then of flavors. For example, a sandbox analyzer can analyze a file with or without an Internet connection. Another example could be an analyzer that can either send a file to VirusTotal for analysis or get the last report using its hash.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#python-requirements","title":"Python Requirements","text":"<p>If the analyzer is written in Python, a <code>requirements.txt</code> must be provided with the list of all the dependencies.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#example-virustotal-analyzer-files","title":"Example: VirusTotal Analyzer Files","text":"<p>Below is a directory listing of the files corresponding to the VirusTotal analyzer. You can see that the analyzer has two flavors: GetReport and Scan.</p> <pre><code>analyzers/VirusTotal\n|-- VirusTotal_GetReport.json\n|-- VirusTotal_Scan.json\n|-- requirements.txt\n|-- virustotal.py\n`-- virustotal_api.py\n</code></pre>"},{"location":"cortex/api/how-to-create-an-analyzer/#input","title":"Input","text":"<p>The input of an analyzer is a JSON structure with different pieces of information. For example, to use the VirusTotal analyzer's GetReport flavor in order to obtain the latest available report for hash <code>d41d8cd98f00b204e9800998ecf8427e</code>, you must submit input such as:</p> <pre><code>{\n    \"data\":\"d41d8cd98f00b204e9800998ecf8427e\",\n    \"dataType\":\"hash\",\n    \"tlp\":0,\n    \"config\":{\n        \"key\":\"1234567890abcdef\",\n        \"max_tlp\":3,\n        \"check_tlp\":true,\n        \"service\":\"GetReport\"\n        [..]\n    },\n    \"proxy\":{\n        \"http\":\"http://myproxy:8080\",\n        \"https\":\"https://myproxy:8080\"\n      }\n  }\n</code></pre> <p><code>data</code>, <code>dataType</code> and <code>tlp</code> are the observable-related information generated by TheHive or any other program that is calling Cortex. <code>config</code> is the analyzer's specific configuration provided by an <code>orgAdmin</code> users when the analyzer is enabled in the Cortex UI.</p> <p>Let's take the GetReport flavor of the VirusTotal analyzer as an example again.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#example-virustotal-get-reports-input","title":"Example: VirusTotal Get Report's Input","text":"<pre><code>{\n    \"data\":\"d41d8cd98f00b204e9800998ecf8427e\",\n    \"dataType\":\"hash\",\n    \"tlp\":0,\n    [..]\n  }\n</code></pre>"},{"location":"cortex/api/how-to-create-an-analyzer/#example-service-interaction-file-for-virustotal-getreport","title":"Example: Service Interaction File for VirusTotal GetReport","text":"<p>The <code>&lt;==</code> sign and anything after it are comments that do no appear in the original file. </p><pre><code>{\n  \"name\": \"VirusTotal_GetReport\",\n  \"version\": \"3.0\",\n  \"author\": \"CERT-BDF\",\n  \"url\": \"https://github.com/TheHive-Project/Cortex-Analyzers\",\n  \"license\": \"AGPL-V3\",\n  \"description\": \"Get the latest VirusTotal report for a file, hash, domain or an IP address.\",\n  \"dataTypeList\": [\"file\", \"hash\", \"domain\", \"ip\"],\n  \"command\": \"VirusTotal/virustotal.py\", &lt;== Program to run when invoking the analyzer\n  \"baseConfig\": \"VirusTotal\", &lt;== name of base config in Cortex analyzer config page\n  \"config\": {\n    \"service\": \"get\"\n  },\n  \"configurationItems\": [ &lt;== list of configuration items the analyzer needs to operate (api key etc.)\n    {\n      \"name\": \"key\",\n      \"description\": \"API key for Virustotal\",\n      \"type\": \"string\", &lt;== defines what kind of data type the configuration item is (string, number)\n      \"multi\": false, &lt;== setting multi to true allows to pass a list of items (e.g. MISP analyzer)\n      \"required\": true \n    },\n    {\n      \"name\": \"polling_interval\",\n      \"description\": \"Define time interval between two requests attempts for the report\",\n      \"type\": \"number\",\n      \"multi\": false,\n      \"required\": false,\n      \"defaultValue\": 60\n    }\n  ]\n}\n</code></pre>"},{"location":"cortex/api/how-to-create-an-analyzer/#service-interaction-configuration-items","title":"Service Interaction Configuration Items","text":""},{"location":"cortex/api/how-to-create-an-analyzer/#name","title":"name","text":"<p>Name of the specific service (or flavor) of the analyzer.</p> <p>If your analyzer has only one service interaction (i.e. performs only one action), it is the name of the analyzer's directory.</p> <p>If your analyzer performs several actions (i.e. comes in several flavors), you have to give a specific and meaningful name to each flavor.</p> <p>Each flavor's name appear in TheHive's analyzer list and in MISP when you use Cortex for attribute enrichment.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#version","title":"version","text":"<p>The version of the analyzer.</p> <p>You must increase major version numbers when new features are added, modifications are made to take into account API changes, report output is modified or when report templates (more on this later) are updated.</p> <p>You must increase minor version numbers when bugs are fixed.</p> <p>The version number is also used in the folder name of the associated report templates ; e.g. VirusTotal_GetReport and 3.0 on the JSON file should correspond a folder named VirusTotal_GetReport_3_0 for report templates.  Report templates are used by TheHive to display the analyzer's JSON output  in an analyst-friendly fashion.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#author","title":"author","text":"<p>You must provide your full name and/or your organization/team name when submitting an analyzer. Pseudos are not accepted. If you'd rather remain anonymous, please contact us at support@thehive-project.org prior to submitting your analyzer.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#url","title":"url","text":"<p>The URL where the analyzer is stored. This should ideally be <code>https://github.com/TheHive-Project/Cortex-Analyzers</code></p>"},{"location":"cortex/api/how-to-create-an-analyzer/#license","title":"license","text":"<p>The license of the code. Ideally, we recommend using the AGPL-v3 license.</p> <p>Make sure your code's license is compatible with the license(s) of the various components and libraries you use if applicable.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#description","title":"description","text":"<p>Description of the analyzer. Please be concise and clear. The description is  shown in the Cortex UI, TheHive and MISP.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#datatypelist","title":"dataTypeList","text":"<p>The list of TheHive datatypes supported by the analyzer. Currently TheHive accepts the following datatypes:</p> <ul> <li>domain</li> <li>file</li> <li>filename</li> <li>fqdn</li> <li>hash</li> <li>ip</li> <li>mail</li> <li>mail_subject</li> <li>other</li> <li>regexp</li> <li>registry</li> <li>uri_path</li> <li>url</li> <li>user-agent</li> </ul> <p>If you need additional datatypes for your analyzer, please let us know at support@thehive-project.org.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#baseconfig","title":"baseConfig","text":"<p>Name used to group configuration items common to several analyzer. This prevent the user to enter the same API key for all analyzer flavors. The Cortex analyzer config page group configuration items by their <code>baseConfig</code>.  </p>"},{"location":"cortex/api/how-to-create-an-analyzer/#config","title":"config","text":"<p>Configuration dedicated to the analyzer's flavor. This is where we  typically specify the TLP level of observables allowed to be analyzed with the  <code>check_tlp</code> and <code>max_tlp</code> parameters. For example, if <code>max_tlp</code> is set to <code>2</code> (TLP:AMBER),  TLP:RED observables cannot be analyzed.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#max_tlp","title":"max_tlp","text":"<p>The TLP level above which the analyzer must not be executed.</p> TLP max_tlp value Unknown -1 WHITE 0 GREEN 1 AMBER 2 RED 3"},{"location":"cortex/api/how-to-create-an-analyzer/#check_tlp","title":"check_tlp","text":"<p>This is a boolean parameter. When <code>true</code>, <code>max_tlp</code> is checked. And if the input's TLP is above <code>max_tlp</code>, the analyzer is not executed.</p> <p>For consistency reasons, we do recommend setting both <code>check_tlp</code> and <code>max_tlp</code> even if <code>check_tlp</code> is set to <code>false</code>.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#max_pap","title":"max_pap","text":"<p>The PAP level above which the analyzer must not be executed.</p> TLP max_tlp value Unknown -1 WHITE 0 GREEN 1 AMBER 2 RED 3"},{"location":"cortex/api/how-to-create-an-analyzer/#check_pap","title":"check_pap","text":"<p>This is a boolean parameter. When <code>true</code>, <code>max_pap</code> is checked. And if the input's PAP is above <code>max_pap</code>, the analyzer is not executed.</p> <p>For consistency reasons, we do recommend setting both <code>check_pap</code> and <code>max_pap</code> even if <code>check_pap</code> is set to <code>false</code>.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#command","title":"command","text":"<p>The command used to run the analyzer. That's typically the full, absolute path to the main program file.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#configurationitems","title":"configurationItems","text":"<p>The list of configurationItems is necessary in order to be able to set all configuration variables for analyzers directly in the Cortex 2 user interface. As in the VirusTotal example above can be seen, every item is a json object that defines: - name (string) - description (string) - type (string) - multi (boolean) - required (boolean) - defaultValue (according to type, optional)</p> <p>The <code>multi</code> parameter allows to pass a list as configuration variable instead of a single string or number. This is used e.g. in the MISP analyzer that queries multiple servers in one run and needs different parameters for that.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#output","title":"Output","text":"<p>The output of an analyzer depends on the success or failure of its execution.</p> <p>If the analyzer fails to execute:</p> <pre><code>{\n    \"success\": false,\n    \"errorMessage\":\"..\"\n}\n</code></pre> <ul> <li>When <code>success</code> is set to <code>false</code>, it indicates that something went wrong     during the execution.</li> <li><code>errorMessage</code> is free text - typically the error output message.</li> </ul> <p>If the analyzer succeeds (i.e. it runs without any error):</p> <pre><code>{\n    \"success\":true,\n    \"artifacts\":[..],\n    \"summary\":{\n        \"taxonomies\":[..]\n    },\n    \"full\":{..}\n}\n</code></pre> <ul> <li>When <code>success</code> is set to <code>true</code>, it indicates that the analyzer ran     successfully.</li> <li><code>artifacts</code> is a list of indicators extracted from the produced report.</li> <li><code>full</code> is the full report of the analyzer. It is free form, as long as it is JSON formatted.</li> <li> <p><code>summary</code> is used in TheHive for short reports displayed in the     observable list and in the detailed page of each observable. It     contains a list of taxonomies.</p> <ul> <li><code>taxonomies</code>:</li> </ul> <pre><code>\"taxonomies\":[\n  {\n      \"namespace\": \"NAME\",\n      \"predicate\": \"PREDICATE\",\n      \"value\": \"\\\"VALUE\\\"\",\n      \"level\":\"info\"\n  }\n]\n</code></pre> <ul> <li><code>namespace</code> and <code>predicate</code> are free values but they should be as  concise as possible. For example, the VirusTotal analyzer uses VT  as a namespace and Score as a predicate.</li> <li><code>level</code> intends to convey the maliciousness of the result:     :<ul> <li><code>info</code> : the analyzer produced an information, and the     short report is shown in blue color in TheHive.</li> <li><code>safe</code> : the analyzer did not find anything suspicious     or the analyzed observable is safe according to     the analyzer. TheHive displays the short report in green     color.</li> <li><code>suspicious</code> : the analyzer found that the observable is     either suspicious or warrants further investigation. The     short report has an orange color in TheHive.</li> <li><code>malicious</code> : the analyzer found that the observable     is malicious. The short report is red colored in TheHive.</li> </ul> </li> </ul> </li> </ul> <p>For more information refer to our blog.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#the-cortexutils-python-library","title":"The Cortexutils Python Library","text":"<p>So far, all the published analyzers have been written in Python. We released a special Python library called <code>cortexutils</code> to help developers easily write their programs. Note though that Python is not mandatory for analyzer coding and any language that runs on Linux can be used, though you won't have the benefits of the CortexUtils library.</p> <p>Cortexutils can be used with Python 2 and 3. To install it :</p> <pre><code>pip install cortexutils\n</code></pre> <p>or</p> <pre><code>pip3 install cortexutils\n</code></pre> <p>This library is already used by all the analyzers published in our Github repository. Feel free to start reading the code of some of them before writing your own.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#report-templates","title":"Report Templates","text":"<p>When using TheHive, analysts can submit an observable for analysis to one or several Cortex instances by a click of a button. Once finished, Cortex returns the result to TheHive. The TheHive displays that result using HTML templates for short and long reports.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#cortex-result-in-thehive","title":"Cortex Result in TheHive","text":"<p>TheHive receives the Cortex result which is simply the JSON formatted analyzer output described above:</p> <ul> <li>The <code>summary</code> section is read to display short reports in the observables list and in the detailed observable page. This is stored in a dict object named <code>content</code> within TheHive.</li> <li>The <code>full</code> section is read to display long reports when clicking the short     report in the observable list or when accessing a detailed observable     page. In TheHive application, it is stored in a dict object named     <code>content</code>.</li> </ul>"},{"location":"cortex/api/how-to-create-an-analyzer/#displayed-information","title":"Displayed Information","text":""},{"location":"cortex/api/how-to-create-an-analyzer/#when-no-template-is-imported","title":"When No Template is Imported","text":"<p>In the event that the analyzer report templates are not imported in TheHive (only administrators can do such an operation via the Admin &gt; Report Templates menu):</p> <ul> <li>In the observable list, TheHive is able to display the analyzer <code>summary</code>     results using a builtin style sheet associated with the previously     described taxonomy.</li> <li>In the detailed observable page:<ul> <li>the <code>full</code> result is displayed in raw format (the JSON output from   Cortex)</li> <li>the <code>summary</code> result is not displayed.</li> </ul> </li> </ul>"},{"location":"cortex/api/how-to-create-an-analyzer/#when-templates-are-imported","title":"When Templates are Imported","text":"<p>If templates are imported into TheHive:</p> <ul> <li>Short reports are displayed in the observable list and in the detailed observable page.</li> </ul> <p></p> <ul> <li>Long reports are displayed when clicking on the short reports or in the detailed observable page.</li> </ul> <p></p>"},{"location":"cortex/api/how-to-create-an-analyzer/#writing-templates","title":"Writing Templates","text":"<p>To display results nicely in TheHive, write two HTML templates:</p> <ul> <li>One for short reports</li> <li>One for long reports</li> </ul> <p>When TheHive users import them in the application, they will be definitely more efficient at reading the analyzer reports and do their job accordingly.</p> <p>If the analyzer is made of different flavors (i.e. has different service interaction files with a <code>json</code> extension), you should provide two HTML templates (short and long reports) for each flavor.</p> <p>For example, the VirusTotal analyzer comes in two flavors hence it has 4 HTML  templates:</p> <pre><code>thehive-templates/VirusTotal_GetReport_3_0\n|-- long.html\n`-- short.html\nthehive-templates/VirusTotal_Scan_3_0\n|-- long.html\n`-- short.html\n</code></pre> <p>The folder's name is the concatenation of the <code>name</code> and the <code>version</code> values found in the service interaction files.</p> <p>TheHive uses Bootstrap and AngularJS so you can leverage them in your templates.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#short-report-templates-shorthtml","title":"Short Report Templates (short.html)","text":"<p>The short report uses taxonomies and is built into the analyzers by the <code>summary()</code> function. Report templates read it as shown in the example below:</p> <pre><code>&lt;span class=\"label\" ng-repeat=\"t in content.taxonomies\"\n  ng-class=\"{'info': 'label-info', 'safe': 'label-success',\n  'suspicious': 'label-warning',\n  'malicious':'label-danger'}[t.level]\"&gt;\n    {{t.namespace}}:{{t.predicate}}={{t.value}}\n&lt;/span&gt;\n</code></pre> <p>If you want to change or add the information displayed in the short report in the detailed observable page, you have to update the <code>summary()</code> function in the analyzer's program and edit short.html as well. Basically, copy the code in your short.html template and it will do the job.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#long-report-templates-longhtml","title":"Long Report Templates (long.html)","text":"<p>Long report templates are more or less free form as long as it reads the content of the relevant section in the Cortex result (<code>full</code>). Feel free to check what has already been written for existing analyzers to write yours.</p> <p>A good start can be:</p> <pre><code>&lt;!-- Success --&gt;\n&lt;div class=\"panel panel-danger\" ng-if=\"success\"&gt;\n    &lt;div class=\"panel-heading\"&gt;\n        ANALYZERNAME Report\n    &lt;/div&gt;\n    &lt;div class=\"panel-body\"&gt;\n        [...]                      &lt;= code here\n    &lt;/div&gt;\n&lt;/div&gt;\n\n&lt;!-- General error  --&gt;\n&lt;div class=\"panel panel-danger\" ng-if=\"!success\"&gt;\n    &lt;div class=\"panel-heading\"&gt;\n        &lt;strong&gt;{{(artifact.data || artifact.attachment.name) | fang}}&lt;/strong&gt;\n    &lt;/div&gt;\n    &lt;div class=\"panel-body\"&gt;\n        &lt;dl class=\"dl-horizontal\" ng-if=\"content.errorMessage\"&gt;\n            &lt;dt&gt;&lt;i class=\"fa fa-warning\"&gt;&lt;/i&gt; ANALYZERNAME: &lt;/dt&gt;\n            &lt;dd class=\"wrap\"&gt;{{content.errorMessage}}&lt;/dd&gt;\n        &lt;/dl&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"cortex/api/how-to-create-an-analyzer/#submitting-an-analyzer","title":"Submitting an Analyzer","text":"<p>We highly encourage you to share your analyzers with the community through our Github repository. To do so, we invite you to follow a few steps before submitting a pull request.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#check-existing-issues","title":"Check Existing Issues","text":"<p>Start by checking if an issue already exists for the analyzer you'd like to write and contribute. Verify that nobody is working on it. If an issue exists and has the in progress, under review or pr-submitted label, it means somebody is already working on the code or has finished it.</p> <p>If you are short on ideas, check issues with a help wanted label. If one of those issues interest you, indicate that you are working on it.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#open-an-issue","title":"Open an Issue","text":"<p>If there's no issue open for the analyzer you'd like to contribute, open one. Indicate that you are working on it to avoid having someone start coding it.</p> <p>You have to create an issue for each analyzer you'd like to submit.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#review-your-service-interaction-files","title":"Review your Service Interaction File(s)","text":"<p>Review your service interaction files. For example, let's check the VirusTotal JSON analyzer configuration file(s):</p> <p></p><pre><code>{\n    \"name\": \"VirusTotal_GetReport\",\n    \"version\": \"3.0\",\n    \"author\": \"CERT-BDF\",\n    \"url\": \"https://github.com/TheHive-Project/Cortex-Analyzers\",\n    \"license\": \"AGPL-V3\",\n    \"description\": \"Get the latest VirusTotal report for a file, hash, domain or an IP address\",\n    \"dataTypeList\": [\"file\", \"hash\", \"domain\", \"ip\"],\n    \"baseConfig\": \"VirusTotal\",\n    \"config\": {\n        \"check_tlp\": true,\n        \"max_tlp\": 3,\n        \"service\": \"get\"\n    },\n    \"command\": \"VirusTotal/virustotal.py\"\n}\n</code></pre> Ensure that all information is correct and particularly the <code>author</code> and <code>license</code> parameters."},{"location":"cortex/api/how-to-create-an-analyzer/#provide-the-list-of-requirements","title":"Provide the List of Requirements","text":"<p>If your analyzer is written in Python, make sure to complete the <code>requirements.txt</code> file with the list of all the external libraries that are needed to run the analyzer correctly.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#check-the-taxonomy","title":"Check the Taxonomy","text":"<p>We chose to use a formatted summary report to match a taxonomy as described above. If you want your analyzer reports in the observable lists, ensure that your summary matches this format. If your analyzer is written in Python and you are using our <code>cortexutils</code> library, you can use the <code>summary()</code>and <code>build_taxonomy()</code> functions.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#provide-global-configuration-parameters","title":"Provide Global Configuration Parameters","text":"<p>When submitting your analyzer, please provide the necessary global configuration in <code>/etc/cortex/application.conf</code> if needed. You can provide this information in a <code>README</code> file.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#verify-execution","title":"Verify Execution","text":"<p>Use these three simple checks before submtting your analyzer:</p> <ul> <li>Ensure it works with the expected configuration, TLP or dataType.</li> <li>Ensure it works with missing configuration, dataType or TLP: your analyzer must generate an explicit error message.</li> <li>Ensure the long report template handles error messages correctly.</li> </ul>"},{"location":"cortex/api/how-to-create-an-analyzer/#create-a-pull-request","title":"Create a Pull Request","text":"<p>Create one Pull Request per analyzer against the develop branch of the Cortex-Analyzers repository. Reference the issue you've created in your PR.</p> <p>We have to review your analyzers. Distinct PRs will allow us to review them more quickly and release them to the benefit of the whole community.</p>"},{"location":"cortex/api/how-to-create-an-analyzer/#need-help","title":"Need Help?","text":"<p>Something does not work as expected? No worries, we got you covered. Please join our user forum,  contact us on Gitter, or send us  an email at support@thehive-project.org. We are here to help.</p>"},{"location":"cortex/download/","title":"Download Cortex","text":""},{"location":"cortex/download/#download-cortex","title":"Download Cortex","text":"<p>Cortex is published and available as many binary packages formats: </p>"},{"location":"cortex/download/#debian-ubuntu","title":"Debian /  Ubuntu","text":"<p>Import the GPG key :</p> <pre><code>curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add -\nwget -qO- https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY |  sudo gpg --dearmor -o /usr/share/keyrings/thehive-project-archive-keyring.gpg\n</code></pre> /etc/apt/source.list.d/thehive-project.list<pre><code>deb [signed-by=/usr/share/keyrings/thehive-project-archive-keyring.gpg] https://deb.thehive-project.org release main\n</code></pre>"},{"location":"cortex/download/#red-hat-enterprise-linux-fedora","title":"Red Hat Enterprise Linux /  Fedora","text":"<p>Import the GPG key :</p> <pre><code>sudo rpm --import https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY\n</code></pre> /etc/yum.repos.d/thehive-project.repo<pre><code>[thehive-project]\nenabled=1\npriority=1\nname=TheHive-Project RPM repository\nbaseurl=https://rpm.thehive-project.org/release/noarch\ngpgcheck=1\n</code></pre>"},{"location":"cortex/download/#zip-archive","title":"ZIP archive","text":"<p>Download it at: https://download.thehive-project.org/cortex-latest.zip</p>"},{"location":"cortex/download/#docker","title":"Docker","text":"<p>Docker images are published on Dockerhub here: https://hub.docker.com/r/thehiveproject/cortex</p>"},{"location":"cortex/download/#archives","title":"Archives","text":"<p>There is no archive available for Cortex.</p>"},{"location":"cortex/installation-and-configuration/","title":"Installation &amp; configuration guides","text":""},{"location":"cortex/installation-and-configuration/#installation-configuration-guides","title":"Installation &amp; configuration guides","text":""},{"location":"cortex/installation-and-configuration/#overview","title":"Overview","text":"<p>Cortex relies on Elasticsearch to store its data. A basic setup to install Elasticsearch, then Cortex on a standalone and dedicated server (physical or virtual).</p>"},{"location":"cortex/installation-and-configuration/#hardware-requirements","title":"Hardware requirements","text":"<p>Hardware requirements depends on the usage of the system. We recommend starting with dedicated resources: </p> <ul> <li> 8 vCPU</li> <li> 16 GB of RAM</li> </ul>"},{"location":"cortex/installation-and-configuration/#operating-systems","title":"Operating systems","text":"<p>Cortex has been tested and is supported on the following operating systems: </p> <ul> <li> Ubuntu 20.04 LTS</li> <li> Debian 11 </li> <li> RHEL 8</li> <li> Fedora 35</li> </ul>"},{"location":"cortex/installation-and-configuration/#installation-guide","title":"Installation Guide","text":"<p>Too much in a hurry to read ? </p> <p>If you are using one of the supported operating systems, use our all-in-one installation script: </p> <pre><code>wget -q -O /tmp/install.sh https://archives.strangebee.com/scripts/install.sh ; sudo -v ; bash /tmp/install.sh\n</code></pre> <p>This script helps with the installation process on a fresh and supported OS ; the program also run successfully if the conditions in terms of hardware requirements are met.</p> <p></p> <p>Once executed, several options are available: </p> <ol> <li>Setup proxy settings ; will configure everything on the host to work with a HTTP proxy, and custom CA certificate.</li> <li>Install TheHive ; use this option to install TheHive 5 and its dependancies</li> <li>Install Cortex and all its dependencies to run Analyzers &amp; Responders as Docker Iiages</li> <li>Install Cortex and all its dependencies to run Analyzers &amp; Responders on the host (Debian and Ubuntu ONLY)</li> </ol> <p>For each release, DEB, RPM and ZIP binary packages are built and provided.</p> <p>The following Guide let you prepare, install and configure Cortex and its prerequisites for Debian and RPM packages based Operating Systems, as well as for other systems and using our binary packages. </p>"},{"location":"cortex/installation-and-configuration/#configuration-guides","title":"Configuration Guides","text":"<p>The configuration of Cortex is in files stored in the <code>/etc/cortex</code> folder:</p> <ul> <li><code>application.conf</code> contains all parameters and options</li> <li><code>logback.xml</code> is dedicated to log management</li> </ul> <pre><code>/etc/cortex\n\u251c\u2500\u2500 application.conf\n\u251c\u2500\u2500 logback.xml\n\u2514\u2500\u2500 secret.conf\n</code></pre> <p>A separate secret.conf file is automatically created by Debian or RPM packages. This file should contain a secret that should be used by one instance.</p> <p>Various aspects can configured in the <code>application.conf</code> file:</p> <ul> <li>database</li> <li>Authentication</li> <li>Analyzers &amp; Responders</li> </ul>"},{"location":"cortex/installation-and-configuration/#analyzers-responders","title":"Analyzers &amp; Responders","text":"<p>Before starting the installation of Cortex, this is important to know how Analyzers and Responders will be managed and run. 2 solutions are available to run them:</p>"},{"location":"cortex/installation-and-configuration/#run-locally","title":"Run locally","text":"<p>The programs are downloaded and installed on the system running Cortex. </p> <p>There are many disadvantages with this option:</p> <ul> <li>Some public Analyzers or Responders, or you own custom program might required specific applications installed on the system, </li> <li>All of the programs published are written in Python and come with dependancies. To run successfully, the dependancies of all programs should be installed on the same operating system ; so there is a high risk of incompatibilities (some program might require a specific version of a librarie with the latest is also required by another one)</li> <li>The goal of Analyzers is to extract or gather information or intelligence about observables ; and some of them might be malicious. Depending on the analysis, like a code analysis, you might want to ensure the Analyzer has not been compromised - and the host - by the observable itself</li> <li>You might want to ensure that when you run an Analyzer, there is no question about the integrity of its programs</li> <li>Updating them might be a pain regarding Operating System used and dependancies</li> </ul>"},{"location":"cortex/installation-and-configuration/#run-with-docker","title":"Run with Docker","text":"<p>Analyzers &amp; Responders we publish also have their own Docker images. </p> <p>There are several benefits to use Docker images of Analyzers &amp; Responders.</p> <ul> <li>No need to worry about applications required or libraries, it just work</li> <li>When requested, Cortex downloads the docker image of a program and instanciate a container running the program. When finished, the container is trashed and a new one is created the next time. No need to worry about the integrity of the program</li> <li>This is simple to use and maintain</li> </ul> <p>This is the recommended option. It requires installing Docker engine as well.</p> <p>This is not an exclusive choice, both solutions can be used by the same instance of Cortex.</p>"},{"location":"cortex/installation-and-configuration/advanced-configuration/","title":"Advanced configuration","text":""},{"location":"cortex/installation-and-configuration/advanced-configuration/#advanced-configuration","title":"Advanced configuration","text":""},{"location":"cortex/installation-and-configuration/advanced-configuration/#cache","title":"Cache","text":""},{"location":"cortex/installation-and-configuration/advanced-configuration/#performance","title":"Performance","text":"<p>In order to increase Cortex performance, a cache is configured to prevent repetitive database solicitation. Cache retention time can be configured for users and organizations (default is 5 minutes). If a user is updated, the cache is automatically invalidated.</p>"},{"location":"cortex/installation-and-configuration/advanced-configuration/#analyzer-results","title":"Analyzer Results","text":"<p>Analyzer results (job reports) can also be cached. If an analyzer is executed against the same observable, the previous report can be returned without re-executing the analyzer. The cache is used only if the second job occurs within <code>cache.job</code> (the default is 10 minutes).</p> <pre><code>cache {\n  job = 10 minutes\n  user = 5 minutes\n  organization = 5 minutes\n}\n</code></pre> <p>Notes</p> <ol> <li>The global <code>cache.job</code> value can be overridden for each analyzer in the analyzer configuration Web dialog</li> <li>it is possible to bypass the cache altogether (for example to get extra fresh results) through the API as explained in the API Guide or by setting the cache to Custom in the Cortex UI for each analyzer and specifying <code>0</code> as the number of minutes.</li> </ol>"},{"location":"cortex/installation-and-configuration/advanced-configuration/#streaming-aka-the-flow","title":"Streaming (a.k.a The Flow)","text":"<p>The user interface is automatically updated when data is changed in the back-end. To do this, the back-end sends events to all the connected front-ends. The mechanism used to notify the front-end is called long polling and its settings are:</p> <ul> <li><code>refresh</code> : when there is no notification, close the connection after this  duration (the default is 1 minute).</li> <li><code>cache</code> : before polling a session must be created, in order to make sure no  event is lost between two polls. If there is no poll during the cache setting,  the session is destroyed (the default is 15 minutes).</li> <li><code>nextItemMaxWait</code>, <code>globalMaxWait</code> : when an event occurs, it is not  immediately sent to the front-ends. The back-end waits nextItemMaxWait and up  to globalMaxWait in case another event can be included in the notification.  This mechanism saves many HTTP requests.</li> </ul> <p>The default values are:</p> <pre><code># Streaming\nstream.longpolling {\n  # Maximum time a stream request waits for new element\n  refresh = 1m\n  # Lifetime of the stream session without request\n  cache = 15m\n  nextItemMaxWait = 500ms\n  globalMaxWait = 1s\n}\n</code></pre>"},{"location":"cortex/installation-and-configuration/advanced-configuration/#entity-size-limit","title":"Entity Size Limit","text":"<p>The Play framework used by Cortex sets the HTTP body size limit to 100KB by default for textual content (json, xml, text, form data) and 10MB for file uploads. This could be too small in some cases so you may want to change it with the following settings in the <code>application.conf</code> file:</p> <pre><code># Max textual content length\nplay.http.parser.maxMemoryBuffer=1M\n# Max file size\nplay.http.parser.maxDiskBuffer=1G\n</code></pre> <p>Note</p> <p>if you are using a NGINX reverse proxy in front of Cortex, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the <code>client_max_body_size</code> parameter in your NGINX server configuration to the highest value among the two: file upload and text size as defined in Cortex <code>application.conf</code> file.</p>"},{"location":"cortex/installation-and-configuration/analyzers-responders/","title":"Analyzers &amp; Responders","text":""},{"location":"cortex/installation-and-configuration/analyzers-responders/#analyzers-responders","title":"Analyzers &amp; Responders","text":""},{"location":"cortex/installation-and-configuration/analyzers-responders/#run-with-docker","title":"Run with Docker","text":"<p>Ensure Cortex is authorized to run use Docker</p> <p>To run docker images of Analyzers &amp; Responders, Cortex should have permissions to use docker. </p> <pre><code>sudo usermod -G docker cortex\n</code></pre>"},{"location":"cortex/installation-and-configuration/analyzers-responders/#configure-cortex","title":"Configure Cortex","text":"<p>To run Analyzers&amp;Responders with Docker images, Cortex should be able have access to Internet: </p> <ul> <li>To download public catalogs from download.thehive-project.org </li> <li>To download Docker images from hub.docker.com  (https://hub.docker.com/search?q=cortexneurons).</li> </ul> /etc/cortex/application.conf<pre><code>[..]\nanalyzer {\n  # Directory that holds analyzers\n  urls = [\n    \"https://download.thehive-project.org/analyzers.json\"\n  ]\n\n  fork-join-executor {\n    # Min number of threads available for analyze\n    parallelism-min = 2\n    # Parallelism (threads) ... ceil(available processors * factor)\n    parallelism-factor = 2.0\n    # Max number of threads available for analyze\n    parallelism-max = 4\n  }\n}\n\nresponder {\n  # Directory that holds responders\n  urls = [\n    \"https://download.thehive-project.org/responders.json\"\n  ]\n\n  fork-join-executor {\n    # Min number of threads available for analyze\n    parallelism-min = 2\n    # Parallelism (threads) ... ceil(available processors * factor)\n    parallelism-factor = 2.0\n    # Max number of threads available for analyze\n    parallelism-max = 4\n  }\n}\n[..]\n</code></pre>"},{"location":"cortex/installation-and-configuration/analyzers-responders/#store-run-programs-on-the-host","title":"Store &amp; run programs on the host","text":""},{"location":"cortex/installation-and-configuration/analyzers-responders/#additionnal-packages","title":"Additionnal packages","text":"<p>Some system packages are required to run Analyzers&amp;Responders programs successfully: </p> Debian <pre><code>sudo apt install -y --no-install-recommends python3-pip python3-dev ssdeep libfuzzy-dev libfuzzy2 libimage-exiftool-perl libmagic1 build-essential git libssl-dev\n</code></pre> <p>You may need to install Python's <code>setuptools</code> and update pip/pip3:</p> <pre><code>sudo pip3 install -U pip setuptools\n</code></pre>"},{"location":"cortex/installation-and-configuration/analyzers-responders/#clone-the-repository","title":"Clone the repository","text":"<p>Once finished, clone the Cortex-analyzers repository in the directory of your choosing:</p> <pre><code>cd /opt\ngit clone https://github.com/TheHive-Project/Cortex-Analyzers\nchown -R cortex:cortex /opt/Cortex-Analyzers \n</code></pre>"},{"location":"cortex/installation-and-configuration/analyzers-responders/#install-dependencies","title":"Install dependencies","text":"<p>Each analyzer comes with its own, pip compatible <code>requirements.txt</code> file. You can install all requirements with the following commands:</p> <pre><code>cd /opt\nfor I in $(find Cortex-Analyzers -name 'requirements.txt'); do sudo -H pip3 install -r $I || true; done\n</code></pre>"},{"location":"cortex/installation-and-configuration/analyzers-responders/#configure-cortex_1","title":"Configure Cortex","text":"<p>Next, you'll need to tell Cortex where to find the analyzers. Analyzers may be in different directories as shown in this dummy example of the Cortex configuration file (<code>application.conf</code>):</p> /etc/cortex/application.conf<pre><code>[..]\nanalyzer {\n  # Directory that holds analyzers\n  urls = [\n    \"/opt/Cortex-Analyzers/responders\",\n  ]\n\n  fork-join-executor {\n    # Min number of threads available for analyze\n    parallelism-min = 2\n    # Parallelism (threads) ... ceil(available processors * factor)\n    parallelism-factor = 2.0\n    # Max number of threads available for analyze\n    parallelism-max = 4\n  }\n}\n\nresponder {\n  # Directory that holds responders\n  urls = [\n    \"/opt/Cortex-Analyzers/responders\"\n  ]\n\n  fork-join-executor {\n    # Min number of threads available for analyze\n    parallelism-min = 2\n    # Parallelism (threads) ... ceil(available processors * factor)\n    parallelism-factor = 2.0\n    # Max number of threads available for analyze\n    parallelism-max = 4\n  }\n}\n[..]\n</code></pre>"},{"location":"cortex/installation-and-configuration/analyzers-responders/#run-you-own-analyzers-responders","title":"Run you own Analyzers &amp; Responders","text":"<p>Either you run them from the host or with Docker images, you can also run your own custom Analyzers and Responders. </p>"},{"location":"cortex/installation-and-configuration/analyzers-responders/#dedicated-folder","title":"Dedicated folder","text":"<p>Create a dedicated folder to host your programs: </p> <pre><code>cd /opt\nmkdir -p Custom-Analyzers/{analyzers,responder}\nchown -R cortex:cortex /opt/Cortex-Analyzers \n</code></pre>"},{"location":"cortex/installation-and-configuration/analyzers-responders/#update-cortex-configuration","title":"Update Cortex configuration","text":"<p>Update <code>analyzer.urls</code> and <code>responders.urls</code> accordingly.</p> /etc/cortex/application.conf<pre><code>[..]\nanalyzer {\n  # Directory that holds analyzers\n  urls = [\n    \"https://download.thehive-project.org/analyzers.json\",\n    \"/opt/Custom-Analyzers/analyzers\" \n  ]\n\n  fork-join-executor {\n    # Min number of threads available for analyze\n    parallelism-min = 2\n    # Parallelism (threads) ... ceil(available processors * factor)\n    parallelism-factor = 2.0\n    # Max number of threads available for analyze\n    parallelism-max = 4\n  }\n}\n\nresponder {\n  # Directory that holds responders\n  urls = [\n    \"https://download.thehive-project.org/responders.json\",\n    \"/opt/Custom-Analyzers/responders\" \n  ]\n\n  fork-join-executor {\n    # Min number of threads available for analyze\n    parallelism-min = 2\n    # Parallelism (threads) ... ceil(available processors * factor)\n    parallelism-factor = 2.0\n    # Max number of threads available for analyze\n    parallelism-max = 4\n  }\n}\n[..]\n</code></pre> <p>Then restart Cortex for the changes to take effect.</p> <p>How to develop your own Analyzers or Responders ?</p> <p>Have a look at the dedicated documentation: https://thehive-project.github.io/Cortex-Analyzers/dev_guides/how-to-create-an-analyzer/</p>"},{"location":"cortex/installation-and-configuration/authentication/","title":"Authentication","text":""},{"location":"cortex/installation-and-configuration/authentication/#authentication","title":"Authentication","text":"<p>Like TheHive, Cortex supports local, LDAP, Active Directory (AD), X.509 SSO and/or API keys for authentication and OAuth2.</p> <p>Please note that API keys can only be used to interact with the Cortex API (for example when TheHive is interfaced with a Cortex instance, it must use an API key to authenticate to it). API keys cannot be used to authenticate to the Web UI. By default, Cortex relies on local credentials stored in Elasticsearch.</p> <p>Authentication methods are stored in the <code>auth.provider</code> parameter, which is multi-valued. When a user logs in, each authentication method is tried in order until one succeeds. If no authentication method works, an error is returned and the user cannot log in.</p> <p>The default values within the configuration file are:</p> <pre><code>auth {\n  # \"provider\" parameter contains authentication provider. It can be multi-valued (useful for migration)\n  # available auth types are:\n  # services.LocalAuthSrv : passwords are stored in user entity (in Elasticsearch). No configuration is required.\n  # ad : use ActiveDirectory to authenticate users. Configuration is under \"auth.ad\" key\n  # ldap : use LDAP to authenticate users. Configuration is under \"auth.ldap\" key\n  # oauth2 : use OAuth/OIDC to authenticate users. Configuration is under \"auth.oauth2\" and \"auth.sso\" keys\n  provider = [local]\n\n  # By default, basic authentication is disabled. You can enable it by setting \"method.basic\" to true.\n  method.basic = false\n\n  ad {\n    # The name of the Microsoft Windows domain using the DNS format. This parameter is required.\n    #domainFQDN = \"mydomain.local\"\n\n    # Optionally you can specify the host names of the domain controllers. If not set, Cortex uses \"domainFQDN\".\n    #serverNames = [ad1.mydomain.local, ad2.mydomain.local]\n\n    # The Microsoft Windows domain name using the short format. This parameter is required.\n    #domainName = \"MYDOMAIN\"\n\n    # Use SSL to connect to the domain controller(s).\n    #useSSL = true\n  }\n\n  ldap {\n    # LDAP server name or address. Port can be specified (host:port). This parameter is required.\n    #serverName = \"ldap.mydomain.local:389\"\n\n    # If you have multiple ldap servers, use the multi-valued settings.\n    #serverNames = [ldap1.mydomain.local, ldap2.mydomain.local]\n\n    # Use SSL to connect to directory server\n    #useSSL = true\n\n    # Account to use to bind on LDAP server. This parameter is required.\n    #bindDN = \"cn=cortex,ou=services,dc=mydomain,dc=local\"\n\n    # Password of the binding account. This parameter is required.\n    #bindPW = \"***secret*password***\"\n\n    # Base DN to search users. This parameter is required.\n    #baseDN = \"ou=users,dc=mydomain,dc=local\"\n\n    # Filter to search user {0} is replaced by user name. This parameter is required.\n    #filter = \"(cn={0})\"\n  }\n\n  oauth2 {\n    # URL of the authorization server\n    #clientId = \"client-id\"\n    #clientSecret = \"client-secret\"\n    #redirectUri = \"https://my-cortex-instance.example/api/ssoLogin\"\n    #responseType = \"code\"\n    #grantType = \"authorization_code\"\n\n    # URL from where to get the access token\n    #authorizationUrl = \"https://auth-site.com/OAuth/Authorize\"\n    #tokenUrl = \"https://auth-site.com/OAuth/Token\"\n\n    # The endpoint from which to obtain user details using the OAuth token, after successful login\n    #userUrl = \"https://auth-site.com/api/User\"\n    #scope = [\"openid profile\"]\n  }\n\n  # Single-Sign On\n  sso {\n    # Autocreate user in database?\n    #autocreate = false\n\n    # Autoupdate its profile and roles?\n    #autoupdate = false\n\n    # Autologin user using SSO?\n    #autologin = false\n\n    # Name of mapping class from user resource to backend user ('simple' or 'group')\n    #mapper = group\n    #attributes {\n    #  login = \"user\"\n    #  name = \"name\"\n    #  groups = \"groups\"\n    #  organization = \"org\"\n    #}\n    #defaultRoles = [\"read\"]\n    #defaultOrganization = \"csirt\"\n    #groups {\n    #  # URL to retreive groups (leave empty if you are using OIDC)\n    #  #url = \"https://auth-site.com/api/Groups\"\n    #  # Group mappings, you can have multiple roles for each group: they are merged\n    #  mappings {\n    #    admin-profile-name = [\"admin\"]\n    #    editor-profile-name = [\"write\"]\n    #    reader-profile-name = [\"read\"]\n    #  }\n    #}\n\n    #mapper = simple\n    #attributes {\n    #  login = \"user\"\n    #  name = \"name\"\n    #  roles = \"roles\"\n    #  organization = \"org\"\n    #}\n    #defaultRoles = [\"read\"]\n    #defaultOrganization = \"csirt\"\n  }\n\n}\n\n# Maximum time between two requests without requesting authentication\nsession {\n  warning = 5m\n  inactivity = 1h\n}\n</code></pre>"},{"location":"cortex/installation-and-configuration/authentication/#oauth2openid-connect","title":"OAuth2/OpenID Connect","text":"<p>To enable authentication using OAuth2/OpenID Connect, edit the <code>application.conf</code> file and supply the values of <code>auth.oauth2</code> according to your environment. In addition, you need to supply:</p> <ul> <li><code>auth.sso.attributes.login</code>: name of the attribute containing the OAuth2 user's login in retreived user info (mandatory)</li> <li><code>auth.sso.attributes.name</code>: name of the attribute containing the OAuth2 user's name in retreived user info (mandatory)</li> <li><code>auth.sso.attributes.groups</code>: name of the attribute containing the OAuth2 user's groups (mandatory using groups mappings)</li> <li><code>auth.sso.attributes.roles</code>: name of the attribute containing the OAuth2 user's roles in retreived user info (mandatory using simple mapping)</li> </ul> <p>Important note</p> <p>Authenticate the user using an external OAuth2 authenticator server. The configuration is:</p> <ul> <li>clientId (string) client ID in the OAuth2 server.</li> <li>clientSecret (string) client secret in the OAuth2 server.</li> <li>redirectUri (string) the url of TheHive AOuth2 page (.../api/ssoLogin).</li> <li>responseType (string) type of the response. Currently only \"code\" is accepted.</li> <li>grantType (string) type of the grant. Currently only \"authorization_code\" is accepted.</li> <li>authorizationUrl (string) the url of the OAuth2 server.</li> <li>authorizationHeader (string) prefix of the authorization header to get user info: 'Bearer' by default</li> <li>tokenUrl (string) the token url of the OAuth2 server.</li> <li>userUrl (string) the url to get user information in OAuth2 server.</li> <li>scope (list of string) list of scope.</li> </ul> <p>Example configuration for SSO w/ Oauth2 &amp; Github</p> <pre><code>auth {\n\n  provider = [local, oauth2]\n\n  [..]\n\n  sso {\n    autocreate: false\n    autoupdate: false\n    mapper: \"simple\"\n    attributes {\n      login: \"login\"\n      name: \"name\"\n      roles: \"role\"\n    }\n    defaultRoles: [\"read\", \"analyze\"]\n    defaultOrganization: \"demo\"\n  }  \n  oauth2 {\n    name: oauth2\n    clientId: \"Client_ID\"\n    clientSecret: \"Client_ID\"\n    redirectUri: \"http://localhost:9001/api/ssoLogin\"\n    responseType: code\n    grantType: \"authorization_code\"\n    authorizationUrl: \"https://github.com/login/oauth/authorize\"\n    tokenUrl: \"https://github.com/login/oauth/access_token\"\n    userUrl: \"https://api.github.com/user\"\n    scope: [\"user\"]\n  }\n\n  [..]  \n}\n</code></pre>"},{"location":"cortex/installation-and-configuration/database/","title":"Database configuration","text":""},{"location":"cortex/installation-and-configuration/database/#database-configuration","title":"Database configuration","text":"/etc/cortex/application.conf<pre><code>[..]\n## ElasticSearch\nsearch {\n  index = cortex\n  # For cluster, join address:port with ',': \"http://ip1:9200,ip2:9200,ip3:9200\"\n  uri = \"http://127.0.0.1:9200\"\n\n  ## Advanced configuration\n  # Scroll keepalive.\n  #keepalive = 1m\n  # Scroll page size.\n  #pagesize = 50\n  # Number of shards\n  #nbshards = 5\n  # Number of replicas\n  #nbreplicas = 1\n  # Arbitrary settings\n  #settings {\n  #  # Maximum number of nested fields\n  #  mapping.nested_fields.limit = 100\n  #}\n\n  ## Authentication configuration\n  #username = \"\"\n  #password = \"\"\n\n  ## SSL configuration\n  #keyStore {\n  #  path = \"/path/to/keystore\"\n  #  type = \"JKS\" # or PKCS12\n  #  password = \"keystore-password\"\n  #}\n  #trustStore {\n  #  path = \"/path/to/trustStore\"\n  #  type = \"JKS\" # or PKCS12\n  #  password = \"trustStore-password\"\n  #}\n}\n</code></pre>"},{"location":"cortex/installation-and-configuration/docker/","title":"Parameters for Docker","text":""},{"location":"cortex/installation-and-configuration/docker/#parameters-for-docker","title":"Parameters for Docker","text":""},{"location":"cortex/installation-and-configuration/docker/#list-of-options","title":"list of options","text":"<ul> <li><code>docker.container.capAdd</code>: (array of string) Add Linux capabilities</li> <li><code>docker.container.capDrop</code>: (array of string) Drop Linux capabilities</li> <li><code>docker.container.cgroupParent</code>: (string) Cgroup to run a container in</li> <li><code>docker.container.cpuPeriod</code>: (integer) Limit the CPU CFS (Completely Fair Scheduler) period</li> <li><code>docker.container.cpuQuota</code>: (integer) Limit the CPU CFS (Completely Fair Scheduler) quota</li> <li><code>docker.container.dns</code>: (array of string) Set custom dns servers for the container</li> <li><code>docker.container.dnsSearch</code>: (array of string) Search list for host-name lookup.</li> <li><code>docker.container.extraHosts</code>: (array of string) Add a line to /etc/hosts (host:IP)</li> <li><code>docker.container.kernelMemory</code>: (integer) Kernel memory limit</li> <li><code>docker.container.memoryReservation</code>: (integer) Memory soft limit</li> <li><code>docker.container.memory</code>: (integer) Memory limit</li> <li><code>docker.container.memorySwap</code>: (integer) Total memory limit (memory + swap)</li> <li><code>docker.container.memorySwappiness</code>: (integer) Tune a container\u2019s memory swappiness behavior. Accepts an integer between 0 and 100</li> <li><code>docker.container.networkMode</code>: (string) name of the network</li> <li><code>docker.container.privileged</code>: (boolean) Give extended privileges to this container</li> <li><code>job.directory</code>: (string) Folder used by Cortex binary inside the container to share input and output data of Analyzers &amp; Responders</li> <li><code>job.dockerDirectory</code> = (string) Folder on the host used by Analyzers &amp; Responders to share input and output data with Cortex</li> </ul>"},{"location":"cortex/installation-and-configuration/docker/#dockerized-analyzers-responders","title":"Dockerized analyzers / responders","text":"<p>To run Analyzers&amp;Responders as docker images, use our available catalogs to register them.</p> <p>In Cortex configuration file, update <code>analyzer.urls</code> and <code>responder.urls</code> and tell Cortex how to find analyzers and responders. These settings accept:    - a path to a directory where workers are installed (like previous version of Cortex)    - a path or an url (http(s)) to a JSON file containing all worker definitions (merge of all JSON in one array)</p> <p>If you want to use dockerized analyzers, you can add the following urls:  - analyzers-stable.json (once used, analyzer is never updated)   - analyzers.json (updated when new version is released)  - analyzers-devel.json (updated at each commit, used for development)</p> <p>For responders urls are:   - responders-stable.json (once used, analyzer is never updated)   - responders.json (updated when new version is released)   - responders-devel.json (updated at each commit, used for development)</p>"},{"location":"cortex/installation-and-configuration/proxy-settings/","title":"Proxy settings","text":""},{"location":"cortex/installation-and-configuration/proxy-settings/#proxy-settings","title":"Proxy settings","text":""},{"location":"cortex/installation-and-configuration/proxy-settings/#make-cortex-use-a-http-proxy-server","title":"Make Cortex use a HTTP proxy server","text":"<p>Basically, Cortex required to connect to Internet, especially to gather  catalogs of docker images of public Analyzers &amp; Responders.</p> /etc/cortex/application.conf<pre><code>[..]\nplay.ws.proxy {\n  host = http://PROXYSERVERADDRESS:PORT\n  port = http://PROXYSERVERADDRESS:PORT\n}\n[..]\n</code></pre>"},{"location":"cortex/installation-and-configuration/proxy-settings/#operating-system","title":"Operating System","text":"/etc/environment<pre><code>export http_proxy=http://PROXYSERVERADDRESS:PORT\nexport https_proxy=http://PROXYSERVERADDRESS:PORT  \n</code></pre> <p>Specific configuration for Debian apt application</p> /etc/apt/apt.conf.d/80proxy<pre><code>  HTTP::proxy \"http://PROXYSERVERADDRESS:PORT\";\n  HTTPS::proxy \"http://PROXYSERVERADDRESS:PORT\";\n</code></pre>"},{"location":"cortex/installation-and-configuration/proxy-settings/#pip","title":"pip","text":"<p>If Analyzers and Responders requirements have to be installed on the host, and the host is behind a proxy server, configure the pip command to use the proxy server ; use the option <code>--proxy http://PROXYSERVERADDRESS:PORT\"</code>, and <code>--cert path/to/cacert.pem</code> if a custom certificate is used by the proxy.</p> <pre><code>pip3 install --proxy http://PROXYSERVERADDRESS:PORT\" -r analyzers/*/requirements.txt\n</code></pre> <p>or </p> <pre><code>pip3 install --proxy http://PROXYSERVERADDRESS:PORT\" --cert path/to/cacert.pem -r analyzers/*/requirements.txt\n</code></pre>"},{"location":"cortex/installation-and-configuration/proxy-settings/#git","title":"Git","text":"<pre><code>sudo git config --global http.proxy http://PROXYSERVERADDRESS:PORT\nsudo git config --global https.proxy http://PROXYSERVERADDRESS:PORT\n</code></pre>"},{"location":"cortex/installation-and-configuration/proxy-settings/#docker","title":"Docker","text":"<p>If using Analyzers &amp; Responders as docker images, setting up proxy parameters could be required to download images.</p> <p>Update Docker engine configuration by editing/creating the file <code>/etc/systemd/system/docker.service.d/http-proxy.conf</code>: </p> /etc/systemd/system/docker.service.d/http-proxy.conf<pre><code>[Service]\nEnvironment=http://PROXYSERVERADDRESS:PORT\"\nEnvironment=\"http://PROXYSERVERADDRESS:PORT\"\n</code></pre> <p>Then run: </p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart docker\n</code></pre>"},{"location":"cortex/installation-and-configuration/run-cortex-with-docker/","title":"Run cortex with docker","text":""},{"location":"cortex/installation-and-configuration/run-cortex-with-docker/#docker","title":"Docker","text":"<p>To use the Docker image, you must use Docker (courtesy of Captain Obvious). Alternatively, it's also possible to run the image using Podman.</p> <p>By default, the docker image generate a configuration file for Cortex with:  - the Elasticsearch uri is determined by resolving the host name \"elasticsearch\",  - the analyzers and responders official location,  - a generated secret (used to protect the user sessions). The behaviour of the Cortex Docker image can be customized using environment variables or parameters:</p> Parameter Env variable Description <code>--no-config</code> <code>no_config=1</code> Do not configure Cortex <code>--no-config-secret</code> <code>no_config_secret=1</code> Do not add the random secret to the configuration <code>--no-config-es</code> <code>no_config_es=1</code> do not add elasticsearch hosts to configuration <code>--es-uri &lt;uri&gt;</code> <code>es_uri=&lt;uri&gt;</code> use this string to configure elasticsearch hosts (format: http(s)://host:port,host:port(/prefix)?querystring) <code>--es-hostname &lt;host&gt;</code> <code>es_hostname=host</code> resolve this hostname to find elasticsearch instances <code>--secret &lt;secret&gt;</code> <code>secret=&lt;secret&gt;</code> secret to secure sessions <code>--show-secret</code> <code>show_secret=1</code> show the generated secret <code>--job-directory &lt;dir&gt;</code> <code>job_directory=&lt;dir&gt;</code> use this directory to store job files <code>--docker-job-directory &lt;dir&gt;</code> <code>docker_job_directory=&lt;dir&gt;</code> indicate the job directory in the host (not inside container) <code>--analyzer-url &lt;url&gt;</code> <code>analyzer_urls=&lt;url&gt;,&lt;url&gt;,...</code> where analyzers are located (url or path) <code>--responder-url &lt;url&gt;</code> <code>responder_urls=&lt;url&gt;,&lt;url&gt;,...</code> where responders are located (url or path) <code>--start-docker</code> <code>start_docker=1</code> start an internal docker (inside container) to run analyzers/responders <code>--daemon-user &lt;user&gt;</code> <code>daemon_user=&lt;user&gt;</code> run cortex using this user <p>At the end of the generated configuration, the file <code>/etc/cortex/application.conf</code> is included. Thus you can override any setting by binding your own <code>application.conf</code> into this file:</p> <pre><code>docker run --volume /path/to/my/application.conf:/etc/cortex/application.conf thehiveproject/cortex:latest --es-uri http://elasticsearch.local:9200\n</code></pre> <p>Cortex uses docker to run analyzers and responders. If you run Cortex inside a docker, you can:</p> <ul> <li>give Cortex access to docker service or podman service (recommended solution)</li> <li>start a docker service inside Cortex docker container</li> </ul>"},{"location":"cortex/installation-and-configuration/run-cortex-with-docker/#cortex-uses-main-docker-service","title":"Cortex uses main docker service","text":"<p>In order to use docker service the docker socket must be bound into Cortex container. Moreover, as Cortex shares files with analyzers, a folder must be bound between them.</p> <pre><code>docker run --volume /var/run/docker.sock:/var/run/docker.sock --volume /var/run/cortex/jobs:/tmp/cortex-jobs thehiveproject/cortex:latest --job-directory /tmp/cortex-jobs --docker-job-directory /var/run/cortex/jobs\n</code></pre> <p>Cortex can instantiate docker container by using the docker socket <code>/var/run/docker.sock</code>. The folder <code>/var/run/cortex/jobs</code> is used to store temporary file of jobs. The folder <code>/tmp/cortex-jobs</code> is job folder inside the docker. In order to make job file visible to analyzer docker, Cortex needs to know both folders (parameters <code>--job-directory</code> and <code>-docker-job-directory</code>). On most cases, job directories are the same and <code>--docker-job-directory</code> can be omitted.</p> <p>If you run Cortex in Windows, the docker service is accessible through the named pipe <code>\\\\.\\pipe\\docker_engine</code>. The command becomes</p> <pre><code>docker run --volume //./pipe/docker_engine://./pipe/docker_engine --volume C:\\\\CORTEX\\\\JOBS:/tmp/cortex-jobs thehiveproject/cortex:latest --job-directory /tmp/cortex-jobs --docker-job-directory C:\\\\CORTEX\\\\JOBS\n</code></pre>"},{"location":"cortex/installation-and-configuration/run-cortex-with-docker/#docker-in-docker-docker-ception","title":"Docker in docker (docker-ception)","text":"<p>You can also run docker service inside Cortex container, a docker in a docker with <code>--start-docker</code> parameter. The container must be run in privileged mode.</p> <pre><code>docker run --privileged thehiveproject/cortex:latest --start-docker\n</code></pre> <p>In this case you don't need to bind job directory.</p>"},{"location":"cortex/installation-and-configuration/run-cortex-with-docker/#use-docker-compose","title":"Use Docker-compose","text":"<p>Cortex requires Elasticsearch to run. You can use <code>docker-compose</code> to start them together in Docker or install and configure Elasticsearch manually. Docker-compose can start multiple dockers and link them together.</p> <p>The following docker-compose.yml file starts Elasticsearch and Cortex:</p> <pre><code>version: \"2\"\nservices:\n  elasticsearch:\n    image: elasticsearch:7.9.1\n    environment:\n      - http.host=0.0.0.0\n      - discovery.type=single-node\n      - script.allowed_types=inline\n      - thread_pool.search.queue_size=100000\n      - thread_pool.write.queue_size=10000\n    volumes:\n      - /path/to/data:/usr/share/elasticsearch/data\n  cortex:\n    image: thehiveproject/cortex:3.1.1\n    environment:\n      - job_directory=${job_directory}\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - ${job_directory}:${job_directory}\n    depends_on:\n      - elasticsearch\n    ports:\n      - \"0.0.0.0:9001:9001\"\n</code></pre> <p>Put this docker-compose file and .env in an empty folder and run <code>docker-compose up</code>. Cortex is exposed on 9001/tcp port. These ports can be changed by modifying the <code>docker-compose</code> file.</p> <p>For advanced configuration, visit our Docker Templates repository</p>"},{"location":"cortex/installation-and-configuration/run-cortex-with-docker/#cortex-with-podman","title":"Cortex with podman","text":"<p>Like docker, podman will be able to run the container image of cortex and of its analyzers. The examples below assume that the containers are run as rootful.</p> <p>For Cortex to interact with podman, it needs to use the podman socket. On some systems, podman will automatically install and enable this service. You can check this on your system with:</p> <pre><code>systemctl status podman.socket\n</code></pre> <p>Here we assume that the podman socket is accessible on <code>/run/podman/podman.sock</code>. This may change based on your system.</p> <p>Cortex uses podman service</p> <p>You need to mount the podman socket inside the container to <code>/var/run/docker.sock</code></p> <pre><code>podman run \\\n  --rm \\\n  --name cortex \\\n  -p 9001:9001 \\\n  -v /var/run/cortex/jobs:/tmp/cortex-jobs \\\n  -v /run/podman/podman.sock:/var/run/docker.sock \\\n  docker.io/thehiveproject/cortex:3.1.7 \\\n  --job-directory /tmp/cortex-jobs \\\n  --docker-job-directory /var/run/cortex/jobs \\\n  --es-uri http://$ES_IP:9200\n</code></pre> <p>With this configuration, Cortex analyzers will be run by podman.</p> <p>Image not found</p> <p>Podman may have trouble pulling cortex neurons images from the regular docker registry. You may have to add docker.io as an unqualified registry. To do this, add this line to your config <code>/etc/containers/registries.conf</code>:</p> <pre><code>unqualified-search-registries = ['docker.io']\n</code></pre> <p>Then restart the podman socket service too</p> <p>Docker in podman</p> <p>By running with the flag <code>--privileged</code>, it is possible to start docker inside a podman container</p> <pre><code>podman run \\\n  --privileged \\\n  --rm \\\n  --name cortex \\\n  -p 9001:9001 \\\n  docker.io/thehiveproject/cortex:3.1.7 \\\n  --es-uri http://$ES_IP:9200\n  --start-docker\n</code></pre>"},{"location":"cortex/installation-and-configuration/secret/","title":"Secret key configuration","text":""},{"location":"cortex/installation-and-configuration/secret/#secret-key-configuration","title":"Secret key configuration","text":"<p>Setup a secret key for this instance: </p> <pre><code>cat &gt; /etc/cortex/secret.conf &lt;&lt; _EOF_\nplay.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\"\n_EOF_\n</code></pre> <p>Then, in the file <code>/etc/cortex/application.conf</code>, replace the line including <code>play.http.secret.key=</code> by:</p> /etc/cortex/application.conf<pre><code>[..]\ninclude \"/etc/cortex/secret.conf\"\n[..]\n</code></pre>"},{"location":"cortex/installation-and-configuration/ssl/","title":"Configure SSL","text":""},{"location":"cortex/installation-and-configuration/ssl/#configure-ssl","title":"Configure SSL","text":""},{"location":"cortex/installation-and-configuration/ssl/#connect-cortex-using-https","title":"Connect Cortex using HTTPS","text":"<p>We recommend using a reverse proxy to manage SSL layer; for example, Nginx. </p> Nginx <p>Reference: Configuring HTTPS servers on nginx.org</p> /etc/nginx/sites-available/cortex.conf<pre><code>server {\n  listen 443 ssl http2;\n  server_name cortex;\n\n  ssl on;\n  ssl_certificate       path-to/cortex-server-chained-cert.pem;\n  ssl_certificate_key   path-to/cortex-server-key.pem;\n\n  proxy_connect_timeout   600;\n  proxy_send_timeout      600;\n  proxy_read_timeout      600;\n  send_timeout            600;\n  client_max_body_size    2G;\n  proxy_buffering off;\n  client_header_buffer_size 8k;\n\n  location / {\n    add_header              Strict-Transport-Security \"max-age=31536000; includeSubDomains\";\n    proxy_pass              http://127.0.0.1:9001/;\n    proxy_http_version      1.1;\n  }\n}\n</code></pre>"},{"location":"cortex/installation-and-configuration/ssl/#certificate-manager","title":"Certificate manager","text":"<p>Certificate manager is used to store client certificates and certificate authorities.</p>"},{"location":"cortex/installation-and-configuration/ssl/#use-custom-certificate-authorities","title":"Use custom Certificate Authorities","text":"<p>The prefered way to use custom Certificate Authorities is to use the system configuration. </p> <p>If setting up a custom Certificate Authority (to connect web proxies, remote services like LPAPS server ...) is required globally in the application, the better solution consists of installing it on the OS and restarting Cortex. </p> DebianRPM <p>Ensure the package <code>ca-certificates-java</code> is installed , and copy the CA certificate in the right folder. Then run <code>dpkg-reconfigure ca-certificates</code> and restart Cortex service. </p> <pre><code>apt-get install -y ca-certificates-java\nmkdir /usr/share/ca-certificates/extra\ncp mycustomcert.crt /usr/share/ca-certificates/extra\ndpkg-reconfigure ca-certificates\nservice cortex restart\n</code></pre> <p>No additionnal packages is required on Fedora or RHEL. Copy the CA certificate in the right folder, run <code>update-ca-trust</code> and restart Cortex service.</p> <pre><code>cp mycustomcert.crt /etc/pki/ca-trust/source/anchors\nsudo update-ca-trust \nservice cortex restart\n</code></pre>"},{"location":"cortex/installation-and-configuration/step-by-step-guide/","title":"Step-by-Step guide","text":""},{"location":"cortex/installation-and-configuration/step-by-step-guide/#step-by-step-guide","title":"Step-by-Step guide","text":"<p>This page is a step by step installation and configuration guide to get a Cortex instance up and running. This guide is illustrated with examples for Debian and RPM packages based systems and for installation from binary packages.</p>"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#required-packages","title":"Required packages","text":"DebianRPM <pre><code>apt install wget gnupg apt-transport-https git ca-certificates ca-certificates-java curl  software-properties-common python3-pip lsb_release\n</code></pre> <pre><code>yum install pkg-install gnupg chkconfig python3-pip git \n</code></pre>"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#java-virtual-machine","title":"Java Virtual Machine","text":"<p>Install Java</p> DebianRPMOther <pre><code>apt install -y openjdk-11-jre-headless\necho JAVA_HOME=\"/usr/lib/jvm/java-11-openjdk-amd64\" &gt;&gt; /etc/environment\nexport JAVA_HOME=\"/usr/lib/jvm/java-11-openjdk-amd64\"\n</code></pre> <pre><code>sudo yum install -y java-11-openjdk-headless.x86_64\necho JAVA_HOME=\"/usr/lib/jvm/java-11-openjdk\" | sudo tee -a /etc/environment\nexport JAVA_HOME=\"/usr/lib/jvm/java-11-openjdk\"\n</code></pre> <p>The installation requires Java 11, so refer to your system documentation to install it.</p>"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#elasticsearch","title":"Elasticsearch","text":"DebianRPM <pre><code>wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch |  sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg\necho \"deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/7.x/apt stable main\" |  sudo tee /etc/apt/sources.list.d/elastic-7.x.list \nsudo apt install elasticsearch   \n</code></pre> /etc/yum.repos.d/elasticsearch.repo<pre><code>[elasticsearch]\nname=Elasticsearch repository for 7.x packages\nbaseurl=https://artifacts.elastic.co/packages/7.x/yum\ngpgcheck=1\ngpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch\nenabled=0\nautorefresh=1\ntype=rpm-md\n</code></pre> <pre><code>sudo yum install --enablerepo=elasticsearch elasticsearch\n</code></pre>"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#configuration","title":"Configuration","text":"/etc/elasticsearch/elasticsearch.yml<pre><code>http.host: 127.0.0.1\ntransport.host: 127.0.0.1\ncluster.name: hive\nthread_pool.search.queue_size: 100000\npath.logs: \"/var/log/elasticsearch\"\npath.data: \"/var/lib/elasticsearch\"\nxpack.security.enabled: false\nscript.allowed_types: \"inline,stored\"\n</code></pre> <p>Adjust this file according to the amount of RAM available on your server: </p> /etc/elasticsearch/jvm.options.d/jvm.options<pre><code>-Dlog4j2.formatMsgNoLookups=true\n-Xms4g\n-Xmx4g\n</code></pre>"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#docker","title":"Docker","text":"<p>If using Docker images of Analyzers and Responders, Docker engine is required on the Operating System: </p> DebianRPM <pre><code>curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\\n$(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list\napt install docker-ce\n</code></pre> <pre><code>sudo yum remove -yq docker \\\n          docker-client \\\n          docker-client-latest \\\n          docker-common \\\n          docker-latest \\\n          docker-latest-logrotate \\\n          docker-logrotate \\\n          docker-engine\nsudo dnf -yq install dnf-plugins-core\nsudo dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo\nsudo dnf install -yq docker-ce docker-ce-cli containerd.io docker-compose-plugin\n</code></pre>"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#cortex","title":"Cortex","text":"<p>This part contains instructions to install Cortex and then configure it.</p>"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#installation","title":"Installation","text":"<p>All packages are published on our packages repository. We support Debian and RPM packages as well as binary packages (zip archive). All packages are signed using our GPG key 562CBC1C. Its fingerprint is <code>0CD5 AC59 DE5C 5A8E 0EE1  3849 3D99 BB18 562C BC1C</code>.</p> DebianRPM <pre><code>wget -O- \"https://raw.githubusercontent.com/TheHive-Project/Cortex/master/PGP-PUBLIC-KEY\"  | sudo apt-key add -\nwget -qO- https://raw.githubusercontent.com/TheHive-Project/Cortex/master/PGP-PUBLIC-KEY |  sudo gpg --dearmor -o /usr/share/keyrings/thehive-project.gpg\necho 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list\napt install cortex\n</code></pre> /etc/yum.repos.d/thehive-project.repo<pre><code>[cortex]\nenabled=1\npriority=1\nname=TheHive-Project RPM repository\nbaseurl=https://rpm.thehive-project.org/release/noarch\ngpgkey=https://raw.githubusercontent.com/TheHive-Project/Cortex/master/PGP-PUBLIC-KEY\ngpgcheck=1\n</code></pre> <pre><code>yum install cortex\n</code></pre> <p>Once installed, if running Analyzers &amp; Responders with Docker, ensure cortex service account can use it: </p> <pre><code>sudo usermod -a -G docker cortex\n</code></pre>"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#configuration_1","title":"Configuration","text":"<p>Following settings are required to start Cortex successfully:</p> <ul> <li>Secret key configuration</li> <li>Database configuration</li> <li>Authentication</li> <li>Analyzers &amp; Responders configuration</li> </ul> <p>Advanced configuration settings might be added to run the application successfully: </p> <ul> <li>Specific Docker parameters</li> <li>Proxy settings</li> <li>SSL configuration</li> </ul>"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#start-cortex-service","title":"Start Cortex service","text":"<p>Warning</p> <p>Before starting the service, ensure to have configured accordingly the application. Start by setting up the secret key.</p> <p>Save configuration file and run the service:</p> <pre><code>systemctl start cortex\n</code></pre> <p>Please note that the service may take some time to start. Once it is started, you may launch your browser and connect to <code>http://YOUR_SERVER_ADDRESS:9001/</code>. </p>"},{"location":"cortex/installation-and-configuration/step-by-step-guide/#first-start","title":"First start","text":"<p>Refer to the First start guide for the next steps.</p>"},{"location":"cortex/operations/backup-restore/","title":"Backup & Restore","text":""},{"location":"cortex/operations/backup-restore/#backup-and-restore-data","title":"Backup and restore data","text":"<p>All persistent data is stored in an Elasticsearch database. The backup and restore procedures are the ones that are detailed in Elasticsearch documentation.</p> <p>Note: you may have to adapt your indices in the examples below. To find the right index, use the following command :</p> <pre><code>curl 'localhost:9200/_cat/indices?v'\n</code></pre> <p>To save all your data you only need to backup the last indice. For example, if the previous command gives you the following results, all your data belongs to cortex_1.</p> <p>In the rest of this document, ensure to change  to your own last index in order to backup or restore all your data.</p>"},{"location":"cortex/operations/backup-restore/#1-create-a-backup-repository","title":"1. Create a backup repository","text":"<p>First you must define a location in local filesystem (where Elasticsearch instance runs) where the backup will be written. This repository must be declared in the Elasticsearch configuration. Edit elasticsearch.yml file by adding:</p> <pre><code>path.repo: [\"/absolute/path/to/backup/directory\"]\n</code></pre> <p>Then, restart the Elasticsearch service.</p> <p>Note: Be careful if you run Elasticsearch in Docker, the directory must be mapped in host filesystem using <code>--volume</code> parameter (cf. Docker documentation).</p>"},{"location":"cortex/operations/backup-restore/#2-register-a-snapshot-repository","title":"2. Register a snapshot repository","text":"<p>Create an Elasticsearch snapshot point named cortex_backup with the following command (set the same path in the location setting as the one set in the configuration file):</p> <pre><code>$ curl -XPUT 'http://localhost:9200/_snapshot/cortex_backup' -d '{\n    \"type\": \"fs\",\n    \"settings\": {\n        \"location\": \"/absolute/path/to/backup/directory\",\n        \"compress\": true\n    }\n}'\n</code></pre> <p>The result of the command should look like this :</p> <pre><code>{\"acknowledged\":true}\n</code></pre> <p>Since, everything is fine to backup and restore data.</p>"},{"location":"cortex/operations/backup-restore/#3-backup-your-data","title":"3. Backup your data","text":"<p>Create a backup named snapshot_1 of all your data by executing the following command :</p> <p></p><pre><code>$ curl -XPUT 'http://localhost:9200/_snapshot/cortex_backup/snapshot_1?wait_for_completion=true&amp;pretty' -d '{\n  \"indices\": \"&lt;INDEX&gt;\"\n}'\n</code></pre> This command terminates only when the backup is complete and the result of the command should look like this: <pre><code>{\n  \"snapshots\": [{\n    \"snapshot\": \"snapshot_1\",\n    \"uuid\": \"ZQ3kv5-FQoeN3NFIhfKgMg\",\n    \"version_id\": 5060099,\n    \"version\": \"5.6.0\",\n    \"indices\": [\"cortex_1\"],\n    \"state\": \"SUCCESS\",\n    \"start_time\": \"2018-01-29T14:41:51.580Z\",\n    \"start_time_in_millis\": 1517236911580,\n    \"end_time\": \"2018-01-29T14:42:05.216Z\",\n    \"end_time_in_millis\": 1517236925216,\n    \"duration_in_millis\": 13636,\n    \"failures\": [],\n    \"shards\": {\n      \"total\": 41,\n      \"failed\": 0,\n      \"successful\": 41\n    }\n  }]\n}\n</code></pre> <p>Note: You can backup the last index of Cortex (you can list indices in your Elasticsearch cluster with <code>curl -s http://localhost:9200/_cat/indices | cut -d ' '  -f3</code> ) or all indices with <code>_all</code> value.</p>"},{"location":"cortex/operations/backup-restore/#4-restore-data","title":"4. Restore data","text":"<p>Restore will do the reverse actions : it reads the backup in your snapshot directory and loads indices into the Elasticsearch cluster. This operation is done with the following command : </p><pre><code>$ curl -XPOST 'http://localhost:9200/_snapshot/cortex_backup/snapshot_1/_restore' -d '\n{\n  \"indices\": \"&lt;INDEX&gt;\"\n}'\n</code></pre> <p>The result of the command should look like this :</p> <pre><code>{\"accepted\":true}\n</code></pre> <p>Note: be sure to restore data from the same version of Elasticsearch.</p>"},{"location":"cortex/operations/backup-restore/#5-moving-data-from-one-server-to-another","title":"5. Moving data from one server to another","text":"<p>If you want to move your data from one server from another: - Create your backup on the origin server (steps 1, 2, 3) - copy your backup directory from the origin server to the destination server - On the destination server :     - Register your backup repository in the Elasticsearch configuration (step 1)     - Register your snapshot repository with the same snapshot name (step 2)     - Restore your data (step 4)</p>"},{"location":"cortex/operations/input-output/","title":"Analyzers/Responders input and output","text":""},{"location":"cortex/operations/input-output/#analyzers-responders-communication","title":"Analyzers / Responders communication","text":"<p>From version 3, cortexutils 2.x is required because communication between Cortex and the analyzers/responders has changed. Analyzers and responders doesn't need to be rewritten if they use cortexutils. Cortex 2 send data using stdin and receive result from stdout.</p> <p>Cortex 3 uses files: a job is stored in a folder with the following structure:</p> <pre><code>job_folder\n  \\_ input\n   |    \\_ input.json    &lt;- input data, equivalent to stdin with Cortex 2.x\n   |    |_ attachment    &lt;- optional extra file when analysis concerns a file\n   |_ output\n        \\_ output.json   &lt;- report of the analysis (generated by analyzer or responder)\n        |_ extra_file(s) &lt;- optional extra files linked to report (generated by analyzer)\n</code></pre> <p>Job folder is provided to analyzer/responder as argument. Currently, only one job is acceptable but in future release, analyzer/responder will accept several job at a time (bulk mode) in order to increase performance.</p>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/","title":"Upgrade to Cortex 3.1","text":""},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#migration-from-elasticsearch-682-to-es-7x","title":"Migration from Elasticsearch 6.8.2 to ES 7.x","text":"<p>\u26a0\ufe0f IMPORTANT NOTE</p> <ul> <li>This migration process is intended for single node of Elasticsearch database</li> <li>The current version of this document is provided for testing purpose ONLY! </li> <li>This guide has been written and tested to migrate data from ES 6.8.2 to ES 7.8.1, and Cortex 3.0.1 to Cortex 3.1.0 only!</li> <li>This guide starts with Elasticsearch version 6.8.2  up and running, indexes and data. To test this guide, we recommend using a backup of you production server. (see Backup and Restore page for more information)</li> <li>This guide is illustrated with Cortex index. The process is identical for Cortex, you just have to adjust index names.</li> </ul>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#prerequisite","title":"Prerequisite","text":"<p>The software <code>jq</code> is required to manipulate JSON and create new indexes. More information at https://stedolan.github.io/jq/. </p>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#identify-if-your-index-should-be-reindexed","title":"Identify if your index should be reindexed","text":"<p>You can easily identify if indexes should be reindexed or not. On the index named <code>cortex_4</code> run the following command: </p> <pre><code>curl -s http://127.0.0.1:9200/cortex_4?human | jq '.cortex_4.settings.index.version.created'\n</code></pre> <p>if the output is similar to <code>\"5xxxxxx\"</code>  then reindexing is required, you should follow this guide. </p> <p>If it is   <code>\"6xxxxxx\"</code> then the index can be read by Elasticsearch 7.8.x. Upgrade Elasticsearch, and Cortex 3.1.0.</p>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#migration-guide","title":"Migration guide","text":""},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#current-status","title":"Current status","text":"<p>Current context is:  - Elasticsearch 6.8.2 - Cortex 3.0.1</p> <p>All up and running. </p> <p>Start by identifying indices on you Elasticsearch instance.</p> <pre><code>curl  http://localhost:9200/_cat/indices\\?v\n</code></pre> <p>The output should look like this: </p> <pre><code>health status index           uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   cortex_4    Y5rDTO23RBC_n6pjFP0-Qw   5   0       8531            8       13mb           13mb \n</code></pre> <p>The index name is <code>cortex_4</code>. Record this somewhere.</p>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#stop-services","title":"Stop services","text":"<p>Before starting updating the database, lets stop applications:</p> <pre><code>sudo service cortex stop \n</code></pre>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#create-a-new-index","title":"Create a new index","text":"<p>The First operation lies in creating a new index named <code>new_cortex_4</code> with settings from current index <code>cortex_4</code> (ensure to keep index version, needed for future upgrade).</p> <pre><code>curl -XPUT 'http://localhost:9200/new_cortex_4' \\\n  -H 'Content-Type: application/json' \\\n  -d \"$(curl http://localhost:9200/cortex_4 |\\\n   jq '.cortex_4 |\n   del(.settings.index.provided_name,\n    .settings.index.creation_date,\n    .settings.index.uuid,\n    .settings.index.version,\n    .settings.index.mapping.single_type,\n    .mappings.doc._all)'\n    )\"\n</code></pre> <p>Check the new index is well created: </p> <pre><code>curl -XGET http://localhost:9200/_cat/indices\\?v\n</code></pre> <p>The output should look like this: </p> <pre><code>health status index           uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   new_cortex_4    wRX6rhzXTuW_F2wLNxqVyg   5   0          0            0      1.1kb          1.1kb\ngreen  open   cortex_4        Y5rDTO23RBC_n6pjFP0-Qw   5   0       8531            8       13mb           13mb\n</code></pre>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#proceed-to-reindex","title":"Proceed to Reindex","text":"<p>Next operation lies in running the reindex command in the newly created index:</p> <pre><code>curl -XPOST -H 'Content-Type: application/json' http://localhost:9200/_reindex -d '{\n  \"conflicts\": \"proceed\",\n  \"source\": {\n    \"index\": \"cortex_4\"\n  },\n  \"dest\": {\n    \"index\": \"new_cortex_4\"\n  }\n}'\n</code></pre> <p>After a moment, you should get a similar output:  </p> <pre><code>{\n    \"took\": 5119,\n    \"timed_out\": false,\n    \"total\": 5889,\n    \"updated\": 0,\n    \"created\": 5889,\n    \"deleted\": 0,\n    \"batches\": 6,\n    \"version_conflicts\": 0,\n    \"noops\": 0,\n    \"retries\": {\n        \"bulk\": 0,\n        \"search\": 0\n    },\n    \"throttled_millis\": 0,\n    \"requests_per_second\": -1.0,\n    \"throttled_until_millis\": 0,\n    \"failures\": []\n}\n</code></pre>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#ensure-new-index-has-been-created","title":"Ensure new index has been created","text":"<p>Run the following command, and ensure the new index is like the current one (size can vary):</p> <pre><code>curl -XGET http://localhost:9200/_cat/indices\\?v\n</code></pre> <p>The output should look like this: </p> <pre><code>health status index           uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   new_cortex_4    wRX6rhzXTuW_F2wLNxqVyg   5   0       8531            0     12.6mb         12.6mb\ngreen  open   cortex_4        Y5rDTO23RBC_n6pjFP0-Qw   5   0       8531            8       13mb           13mb\n</code></pre>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#delete-old-indices","title":"Delete old indices","text":"<p>This is the thrilling part.  Now the new index <code>new_cortex_4</code> is created and similar to <code>cortex_4</code>,  older indexes should be completely deleted from the database. To delete index named <code>cortex_4</code>, run the following command:  </p> <pre><code>curl -XDELETE http://localhost:9200/cortex_4\n</code></pre> <p>Run the same command for older indexes if exist (cortex_3, cortex_2....). Elasticsearch 7.x cannot run with index created with Elasticsearch 5.x.</p>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#create-an-alias","title":"Create an alias","text":"<p>Before stopping Elasticsearch service, let\u2019s create an alias to keep index names in the future.  </p> <pre><code>curl -XPOST -H 'Content-Type: application/json'  'http://localhost:9200/_aliases' -d '{\n    \"actions\": [\n        {\n            \"add\": {\n                \"index\": \"new_cortex_4\",\n                \"alias\": \"cortex_4\"\n            }\n        }\n    ]\n}'\n</code></pre> <p>Doing so will allow Cortex 3.1.0  to find the index without updating the configuration file. </p> <p>Check the alias has been well created by running the following command</p> <pre><code>curl -XGET http://localhost:9200/_alias?pretty\n</code></pre> <p>The output should look like:</p> <pre><code>{\n  \"new_cortex_4\" : {\n    \"aliases\" : {\n      \"cortex_4\" : { }\n    }\n  }\n}\n</code></pre>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#stop-elasticsearch-version-682","title":"Stop Elasticsearch version 6.8.2","text":"<pre><code>sudo service elasticsearch stop \n</code></pre>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#update-elasticsearch","title":"Update Elasticsearch","text":"<p>Update the configuration of Elastisearch. Configuration file should look like this:</p> <pre><code>[..]\nhttp.host: 127.0.0.1\ndiscovery.type: single-node\ncluster.name: hive\nscript.allowed_types: inline\nthread_pool.search.queue_size: 100000\nthread_pool.write.queue_size: 10000    \n</code></pre> <p>Now, upgrade Elasticsearch to version 7.x following the documentation for your Operating System, and ensure the service start successfully.</p>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#install-or-update-to-cortex-310","title":"Install or update to Cortex 3.1.0","text":""},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#deb-package","title":"DEB package","text":"<p>If using Debian based Linux operating system, configure it to follow our beta repository:</p> <p></p><pre><code>curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add -\necho 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list\nsudo apt-get update\n</code></pre> Then install it by running: <pre><code>sudo apt install cortex\n</code></pre> <p>or</p> <pre><code>sudo apt install cortex=3.1.0-1\n</code></pre>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#rpm","title":"RPM","text":"<p>Setup your system to connect the RPM repository. Create and edit the file  <code>/etc/yum.repos.d/thehive-project.repo</code> :</p> <pre><code>[thehive-project]\nenabled=1\npriority=1\nname=TheHive-Project RPM repository\nbaseurl=http://rpm.thehive-project.org/release/noarch\ngpgcheck=1\n</code></pre> <p>Then install it by running:</p> <pre><code>sudo yum install cortex\n</code></pre> <p>or </p> <pre><code>sudo yum install cortex-3.1.0-1\n</code></pre>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#install-binaries","title":"Install binaries","text":"<pre><code>cd /opt\nwget https://download.thehive-project.org/cortex-3.1.0-1.zip\nunzip cortex-3.1.0-1.zip\nln -s cortex-3.1.0-1 cortex\n</code></pre>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#docker-images","title":"Docker images","text":"<p>Docker images are also provided on Dockerhub. </p> <pre><code>docker pull thehiveproject/cortex:3.1.0-1\n</code></pre> <p>\u26a0\ufe0f  Starting from this version, docker image doesn't contain analyzers anymore. Analyzers__/__Responders and Cortex have different life-cycles, their update including their dependencies should not be correlated to Cortex update. </p> <p>It is recommended to use docker version of analyzers : this can be done by binding docker service docket inside cortex container (run with <code>-v /var/run/docker.sock:/var/run/docker.sock</code>).</p>"},{"location":"cortex/operations/upgrade_to_cortex_3_1_and_es7_x/#update-database","title":"Update Database","text":"<p>Connect to TheHive (and Cortex), the maintenance page should ask to update. </p> <p>Once updated, ensure a new index named <code>cortex_5</code> has been created.</p> <pre><code>curl -XGET http://localhost:9200/_cat/indices\\?v\n</code></pre> <p>The output should look like this: </p> <pre><code>health status index           uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   new_cortex_4 GV-3Y8QjTjWw0F-p2sjW6Q   5   0      30977            0       26mb           26mb\nyellow open   cortex_5     Nz0vCKqhRK2xkx1t_WF-0g   5   1      30977            0     26.1mb         26.1mb\n</code></pre>"},{"location":"cortex/user-guides/first-start/","title":"First start","text":""},{"location":"cortex/user-guides/first-start/#quick-start-guide","title":"Quick Start Guide","text":"<p>This is the Quick Start guide for Cortex 3. It assumes that Cortex has been installed, and that the analyzers have been installed as well.</p>"},{"location":"cortex/user-guides/first-start/#step-1-connect-to-cortex","title":"Step 1: Connect to Cortex","text":"<p>One Cortex is installed and configured, open your web browser and connect to http://cortexaddress:9001. </p>"},{"location":"cortex/user-guides/first-start/#step-2-update-the-database","title":"Step 2: Update the Database","text":"<p>Cortex uses ElasticSearch to store users, organizations and analyzers configuration. The first time you connect to the Web UI (<code>http://&lt;CORTEX_IP&gt;:9001</code> by default), you have to create the database by clicking the <code>Update Database</code> button.</p> <p></p>"},{"location":"cortex/user-guides/first-start/#step-3-create-the-cortex-super-administrator","title":"Step 3: Create the Cortex Super Administrator","text":"<p>You are then invited to create the first user. This is a Cortex global administration user or <code>superAdmin</code>. This user account will be able to create Cortex organizations and users.</p> <p></p> <p>You will then be able to log in using this user account. You will note that the default <code>cortex</code> organization has been created and that it includes your user account, a Cortex global admininistrator.</p> <p></p>"},{"location":"cortex/user-guides/first-start/#step-4-create-an-organization","title":"Step 4: Create an Organization","text":"<p>The default <code>cortex</code> organization cannot be used for any other purpose than managing global administrators (users with the <code>superAdmin</code> role), organizations and their associated users. It cannot be used to enable/disable or configure analyzers. To do so, you need to create your own organization inside Cortex by clicking on the <code>Add organization</code>  button.</p> <p></p>"},{"location":"cortex/user-guides/first-start/#step-5-create-a-organization-administrator","title":"Step 5: Create a Organization Administrator","text":"<p>Create the organization administrator account (user with an <code>orgAdmin</code> role).</p> <p></p> <p>Then, specify a password for this user. After doing so,  log out and log in with that new user account.</p>"},{"location":"cortex/user-guides/first-start/#step-6-enable-and-configure-analyzers","title":"Step 6: Enable and Configure Analyzers","text":"<p>Enable the analyzers you need, configure them using the Organization &gt; Configuration and Organization &gt; Analyzers tabs. All analyzer configuration is done using the Web UI, including adding API keys and configuring rate limits.</p>"},{"location":"cortex/user-guides/first-start/#step-7-optional-create-an-account-for-thehive-integration","title":"Step 7 (Optional): Create an Account for TheHive integration","text":"<p>If you are using TheHive, create a new account inside your organisation with the <code>read, analyze</code> role and generate an API key that you will need to add to TheHive's configuration.</p> <p></p>"},{"location":"cortex/user-guides/roles/","title":"User roles","text":""},{"location":"cortex/user-guides/roles/#user-roles","title":"User Roles","text":"<p>Cortex defines four roles:</p> <ul> <li><code>read</code>: the user can access all the jobs that have been performed by the Cortex 2 instance, including their results. However, this role cannot submit jobs. Moreover, this role cannot be used in the default <code>cortex</code> organization. This organization can only contain super administrators.</li> <li><code>analyze</code>: the <code>analyze</code> role implies the <code>read</code> role, described above. A user who has a <code>analyze</code> role can submit a new job using one of the configured analyzers for their organization. This role cannot be used in the default <code>cortex</code> organization. This organization can only contain super administrators.</li> <li><code>orgAdmin</code>: the <code>orgAdmin</code> role implies the <code>analyze</code> role. A user who has an <code>analyze</code> role can manage users within their organization. They can add users and give them <code>read</code>, <code>analyze</code> and/or <code>orgAdmin</code> roles. This role also permits to configure analyzers for the organization. This role cannot be used in the default  <code>cortex</code> organization. This organization can only contain super administrators.</li> <li><code>superAdmin</code>: this role is incompatible with all the other roles listed above (see chart below for examples). It can be used solely for managing organizations and their associated users. When you install Cortex, the first user that is created will have this role. Several users can have it as well but only in the default <code>cortex</code> organization, which is automatically created during installation.</li> </ul> <p>The chart below lists the roles and what they can and cannot do:</p> Actions read analyze orgAdmin superAdmin Read reports X X X Run jobs X X Enable/Disable analyzer X Configure analyzer X Create org analyst X X Delete org analyst X X Create org admin X X Delete org admin X X Create Org X Delete Org X Create Cortex admin user X"},{"location":"resources/","title":"Resources","text":""},{"location":"resources/#resources","title":"Resources","text":"<p>In this section, you can find a collection of valuable resources regarding the applications. </p>"},{"location":"resources/#demo-virtual-machine","title":"Demo virtual machine","text":"<p>Learn how to download and use our demo virtual Machine</p>"},{"location":"resources/#iaas-environment","title":"IaaS environment","text":"<p>Your have your own cloud infrastruture and wish to manage and include TheHive and Cortex ; learn how to deploy our dedicated images by reading our usage instructions:</p> <ul> <li> Amazon AWS environment:<ul> <li>for TheHive and Cortex</li> <li>or Infra as Code deployment</li> </ul> </li> <li>:simple-microsoftazure: Microsoft Azure environment:<ul> <li>for TheHive and Cortex</li> <li>or Infra as Code deployment</li> </ul> </li> </ul> <p>Resources for the official cloud distributions of TheHive and Cortex</p> <p>This documentation contains sample Terraform and cloud-init code to easily launch and update TheHive and Cortex instances.</p>"},{"location":"resources/#main-features-of-the-cloud-distributions-of-thehive-and-cortex","title":"Main features of the cloud distributions of TheHive and Cortex","text":""},{"location":"resources/#easy-to-use-and-deploy","title":"Easy to use and deploy","text":"<p>The cloud distributions were built with operations and automation in mind. We wanted DevSecOps-friendly products that would fit in most organisations, no matter how simple or complex their infrastructure.</p>"},{"location":"resources/#always-up-to-date","title":"Always up-to-date","text":"<p>The images are updated whenever a new TheHive or Cortex version is released. No need to bother updating your instances anymore, just launch a new one with a fresh image as if it were a container!</p>"},{"location":"resources/#production-ready","title":"Production-ready","text":"<ul> <li>Dedicated data volumes: All persistent data is stored on dedicated volumes, not in the root filesystem. </li> <li>Easy-resizing: Resizing independent data volumes is a lot easier as no action is required within the instance at the operating system level.</li> <li>Ubuntu-based: Our images are based on the official Ubuntu 20.04 LTS distributions from Canonical.</li> <li>Hardened OS: The underlying Ubuntu OS is hardened to comply with CSL1 - that's Common Sense Level 1. Ok that's not a real thing (yet) but the OS really was carefully configured to be as secured as possible by default while remaining usable in most contexts. Note that there are no iptables surprises inside the image to avoid conflicting behaviour with security groups of firewalls.</li> <li>Application only: The images include either the TheHive or Cortex application only. They are not meant to be public-facing on their own and should be deployed within your VPC and exposed with the public facing system of your choice (load balancer, reverse proxy). More information on the recommended reference architecture is provided in each cloud distribution user guide.</li> </ul>"},{"location":"resources/cortex4py/","title":"Cortex4py","text":""},{"location":"resources/cortex4py/#cortex4py","title":"Cortex4py","text":"<p>Cortex4py is a Python API client for Cortex.</p> <p></p> <p> </p> <ul> <li>Sources: https://github.com/TheHive-Project/Cortex4py</li> <li>Documentation: https://github.com/TheHive-Project/Cortex4py</li> </ul>"},{"location":"resources/cortexutils/","title":"Cortexutils","text":""},{"location":"resources/cortexutils/#cortexutils","title":"Cortexutils","text":"<p>Cortexutils is a Python library containing a set of classes that aims to make users write Cortex analyzers and responders easier.</p> <p></p> <p> </p> <ul> <li>Sources: https://github.com/TheHive-Project/Cortexutils</li> <li>Documentation: https://github.com/TheHive-Project/Cortexutils</li> </ul>"},{"location":"resources/demo/","title":"How to download the VM","text":""},{"location":"resources/demo/#how-to-download-the-demo-vm","title":"How to download the demo VM","text":"<p>A ready-to-use virtual machine can be downloaded at https://www.strangebee.com/tryit. This VM is prepared and updated by StrangeBee and is powered by the latest versions of:</p> <ul> <li>TheHive: Security Incident Response and Case management platform</li> <li>Cortex: Extendable Analysis, Enrichment and Response automation framework</li> </ul> <p>Warning</p> <p>The VM is built for testing purposes and is NOT RECOMMENDED for production.</p> <p>Note</p> <p>Full documentation of this Virtual machine can be found here</p>"},{"location":"resources/howto-vm-demo/","title":"Use the Demo Virtual Machine","text":""},{"location":"resources/howto-vm-demo/#use-the-demo-virtual-machine","title":"Use the Demo Virtual Machine","text":"<p>Warning</p> <p>Ensure good performance by allocating a minimum of 6 GB of RAM to run this Virtual Machine flawlessly. Adjusting the allocation below this threshold may lead to potential complications.</p>"},{"location":"resources/howto-vm-demo/#start-the-vm","title":"Start the VM","text":"Using VMWareUsing VirtualBox <ol> <li>Start the Virtual Machine, and follow the instructions.</li> <li>Open the indicated url in your browser: http://IP-ADDRESS</li> </ol> <ol> <li>When importing, ensure to set Guest OS type information. </li> <li>Once imported, update the network settings of the VM before starting it. </li> <li>Add required port forwarding (update according to your needs) and save. </li> <li>Start the VM and open the follwing url in your browser: http://127.0.0.1:8888</li> <li>You might have to also adjust Display graphical controller and set it to <code>VMSVGA</code> before starting the VM. </li> </ol>"},{"location":"resources/howto-vm-demo/#quick-connect","title":"Quick connect","text":"<p>Following instructions are also shared in the web page coming with the virtual machine</p> <p></p> <p>TheHive Credentials</p> <p>This VM comes with 2 accounts in TheHive: </p> <p>Administrator:</p> <ul> <li>Login: <code>admin@thehive.local</code></li> <li>Password: <code>secret</code></li> </ul> <p>A user named <code>thehive</code> has been created and is <code>org-admin</code> of the organisation named <code>demo</code>: </p> <ul> <li>Login: <code>thehive@thehive.local</code></li> <li>Password: <code>thehive1234</code></li> </ul> <p>TheHive database comes with several samples of data, like custom fields, MISP taxonomies, MITRE Att&amp;ck data, a Case Template and an Alert. </p> <p></p> <p>Cortex credentials</p> <p>This VM comes with 2 accounts in Cortex:</p> <p>Administrator: </p> <ul> <li>Login: <code>admin</code></li> <li>Password: <code>thehive1234</code> </li> </ul> <p>An Organisation is also created with an <code>orgadmin</code> account: </p> <ul> <li>Login: <code>thehive</code></li> <li>Password: <code>thehive1234</code></li> </ul> <p>Warning</p> <p>The VM is solely intended to be used for testing purposes. We strongly encourage you to refrain from using it in production.</p>"},{"location":"resources/howto-vm-demo/#content","title":"Content","text":"<p>The VM runs Debian 11. The most recent VM includes:</p> <ul> <li>TheHive 5.x using a local BerkeleyDB and file storage, </li> <li>Cortex 3.1.x , and Elasticsearch 7</li> <li>TheHive4py</li> <li>Cortex4py</li> <li>Public Cortex Analyzers and Responders are running with Docker </li> </ul>"},{"location":"resources/howto-vm-demo/#configuration-details","title":"Configuration details","text":"<p>Applications launched with Docker-compose, as docker containers with attached volumes in <code>/opt/thp</code>. </p> <pre><code>.\n\u251c\u2500\u2500 cassandra\n\u251c\u2500\u2500 cortex\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 elasticsearch\n\u251c\u2500\u2500 nginx\n\u2514\u2500\u2500 thehive\n</code></pre>"},{"location":"resources/howto-vm-demo/#thehive","title":"TheHive","text":"<p>TheHive is configured to use Cassandra as database and Elasticsearch to index data. Files are stored in a local path. </p> <pre><code>thehive\n\u251c\u2500\u2500 config\n\u251c\u2500\u2500 files\n\u2514\u2500\u2500 log\n</code></pre> <ul> <li><code>config</code>: all configuration files for TheHive</li> <li><code>files</code>: files storage </li> <li><code>log</code>: TheHive application logs </li> </ul>"},{"location":"resources/howto-vm-demo/#cortex","title":"Cortex","text":"<p>Cortex uses Elasticsearch as database which is also run as a container with Docker-Compose. Dedicated volumes are configured: <code>/opt/thp/elasticsearch/data</code> to store data, and <code>/opt/thp/elasticsearch/log</code>, for logs.</p> <pre><code>cortex\n\u251c\u2500\u2500 config\n\u251c\u2500\u2500 jobs\n\u2514\u2500\u2500 log\n</code></pre> <ul> <li><code>config</code>: all configuration files for TheHive</li> <li><code>jobs</code>: shared volume for Analyzers and Responders jobs</li> <li><code>log</code>: Cortex application logs</li> </ul>"},{"location":"resources/howto-vm-demo/#operations","title":"Operations","text":""},{"location":"resources/howto-vm-demo/#virtual-machine","title":"Virtual Machine","text":"<p>A system user account <code>thehive/thehive1234</code> can be used to operate the VM.</p> <p>All applications are run as docker containers, using docker-compose. The <code>docker-compose.yml</code> is in the folder <code>/opt/thp</code>. </p>"},{"location":"resources/howto-vm-demo/#thehive_1","title":"TheHive","text":"<p>After each modification of TheHive configuration service should be restart.</p> <ul> <li> <p>Configuration file of TheHive is in <code>/opt/thp/thehive/config/application.conf</code></p> </li> <li> <p>Service can be restart by running following commands:</p> </li> </ul> <pre><code>cd /opt/thp\ndocker compose restart thehive\n</code></pre>"},{"location":"resources/howto-vm-demo/#cortex_1","title":"Cortex","text":"<p>After each modification of Cortex configuration service should be restart.</p> <ul> <li> <p>Configuration file of TheHive is in <code>/opt/thp/cortex/config/application.conf</code></p> </li> <li> <p>Service can be restart by running following commands:</p> </li> </ul> <pre><code>cd /opt/thp\ndocker compose restart cortex\n</code></pre>"},{"location":"resources/howto-vm-demo/#check-for-update","title":"Check for update","text":"<p>Check for update for TheHive and Cortex by running following commands (this will stop running applications): </p> <pre><code>cd /opt/thp\nbash update.sh\n</code></pre>"},{"location":"resources/howto-vm-demo/#documentation","title":"Documentation","text":"<ul> <li>Documentation for TheHive 5 can be found there:  https://docs.strangebee.com. </li> </ul>"},{"location":"resources/howto-vm-demo/#troubleshooting","title":"Troubleshooting","text":"<p>TheHive service logs are located in <code>/opt/thp/thehive/log/application.log</code>.</p> <p>Cortex service logs are located in <code>/opt/thp/cortex/log/application.log</code>.</p>"},{"location":"resources/howto-vm-demo/#need-help","title":"Need Help?","text":"<p>Something does not work as expected? No worries, we got you covered. Join our community and contact us on Discord! </p>"},{"location":"resources/security/","title":"Security","text":""},{"location":"resources/security/#security","title":"Security","text":"<p>Find our Responsible Vulnerability Disclosure policy and Security advisories on our dedicated repository: </p> <ul> <li>https://github.com/StrangeBeeCorp/Security</li> </ul>"},{"location":"resources/templates/","title":"TheHive templates","text":""},{"location":"resources/templates/#thehive-templates","title":"TheHive templates","text":"<p>A repository dedicated to the sharing of TheHive Templates. Contributions are welcome ! Discover Cases, Dashboards, pages and Reports templates ready for TheHive 5.</p> <p></p> <p> </p> <ul> <li>Sources: https://github.com/StrangeBeeCorp/thehive-templates</li> <li>Documentation:<ul> <li>Case templates</li> <li>Page templates</li> <li>Report templates</li> <li>Dashboard templates</li> </ul> </li> </ul> <p>If you'd like to contribute and share your templates with TheHive 5 users community, feel free to create a pull request on the github repository. </p>"},{"location":"resources/thehive4py/","title":"TheHive4py","text":""},{"location":"resources/thehive4py/#thehive4py","title":"TheHive4py","text":"<p>TheHive4py is a Python API client for TheHive.</p> <p></p> <p> </p> <ul> <li>Sources: https://github.com/TheHive-Project/TheHive4py</li> <li>Documentation: https://thehive-project.github.io/TheHive4py/</li> </ul>"},{"location":"resources/iaas/aws/","title":"Main Features","text":""},{"location":"resources/iaas/aws/#aws-iaas-images","title":"AWS IaaS images","text":""},{"location":"resources/iaas/aws/#thehive-main-feature","title":"TheHive main feature","text":""},{"location":"resources/iaas/aws/#easy-to-use-and-deploy","title":"Easy to use and deploy","text":"<p>The AMI was built with operations and automation in mind. We wanted a DevSecOps-friendly product that would fit in most organizations, no matter how simple or complex their infrastructure.</p>"},{"location":"resources/iaas/aws/#always-up-to-date","title":"Always up-to-date","text":"<p>The AMI is updated whenever a new Cortex version is released. No need to bother updating Cortex anymore, just launch a new instance as if it were a container!</p>"},{"location":"resources/iaas/aws/#production-ready","title":"Production-ready","text":"<ul> <li>Dedicated data and Docker volumes: Cortex data is stored on a dedicated EBS volume, not in the root filesystem. With that in mind, the AMI will create a persistent 30GB EBS volume at launch that will not be deleted when the instance is terminated so that your Cortex data isn't accidentally lost. Docker images for analyzers and responders are also stored on a dedicated volume so you can adjust its size to your needs, the AMI will create a 20GB volume by default. Both volumes will be encrypted with your default KMS key. You can of course change the default volume size and encryption key.</li> <li>Ubuntu-based: The AMI is based on the official Ubuntu 20.04 LTS AMI from Canonical.</li> <li>Hardened OS: The underlying Ubuntu OS is hardened to comply with CSL1 - that's Common Sense Level 1. Ok that's not a real thing (yet) but the OS really was carefully configured to be as secured as possible by default while remaining usable in most contexts. Note that there are no iptables surprises inside the image to avoid conflicting behavior with security groups.</li> <li>Application only: The AMI includes Cortex application only. It is not meant to be public-facing on its own and should be deployed within your VPC and exposed with the public facing system of your choice (load balancer, reverse proxy). More information on the recommended reference architecture is provided in this user guide.</li> </ul>"},{"location":"resources/iaas/aws/#cortex-main-features","title":"Cortex main Features","text":""},{"location":"resources/iaas/aws/#easy-to-use-and-deploy_1","title":"Easy to use and deploy","text":"<p>The AMI was built with operations and automation in mind. We wanted a DevSecOps-friendly product that would fit in most organizations, no matter how simple or complex their infrastructure.</p>"},{"location":"resources/iaas/aws/#always-up-to-date_1","title":"Always up-to-date","text":"<p>The AMI is updated whenever a new Cortex version is released. No need to bother updating Cortex anymore, just launch a new instance as if it were a container!</p>"},{"location":"resources/iaas/aws/#production-ready_1","title":"Production-ready","text":"<ul> <li>Dedicated data and Docker volumes: Cortex data is stored on a dedicated EBS volume, not in the root filesystem. With that in mind, the AMI will create a persistent 30GB EBS volume at launch that will not be deleted when the instance is terminated so that your Cortex data isn't accidentally lost. Docker images for analyzers and responders are also stored on a dedicated volume so you can adjust its size to your needs, the AMI will create a 20GB volume by default. Both volumes will be encrypted with your default KMS key. You can of course change the default volume size and encryption key.</li> <li>Ubuntu-based: The AMI is based on the official Ubuntu 20.04 LTS AMI from Canonical.</li> <li>Hardened OS: The underlying Ubuntu OS is hardened to comply with CSL1 - that's Common Sense Level 1. Ok that's not a real thing (yet) but the OS really was carefully configured to be as secured as possible by default while remaining usable in most contexts. Note that there are no iptables surprises inside the image to avoid conflicting behavior with security groups.</li> <li>Application only: The AMI includes Cortex application only. It is not meant to be public-facing on its own and should be deployed within your VPC and exposed with the public facing system of your choice (load balancer, reverse proxy). More information on the recommended reference architecture is provided in this user guide.</li> </ul>"},{"location":"resources/iaas/aws/cortex/","title":"Deploy Cortex AMI","text":""},{"location":"resources/iaas/aws/cortex/#usage-instructions-for-the-official-ami-version-of-cortex","title":"Usage instructions for the official AMI version of Cortex","text":"<p> Official AMI This is the official Cortex v3 release for the AWS Marketplace.</p> <p>The AMI can be used to set up a shiny-new Cortex install or to launch an instance with existing data and configuration (for update / migration / restore purposes).</p>"},{"location":"resources/iaas/aws/cortex/#introduction","title":"Introduction","text":"<ul> <li>Based on the official Ubuntu 20.04 AMI from Canonical</li> <li>The AMI is updated whenever a new Cortex version is released - no need to bother updating Cortex anymore, just launch a new instance as if it were a container!</li> <li>Cortex data and Docker images (for analyzers and responders) are stored on two dedicated EBS volumes, not in the root filesystem. With that in mind, the AMI will create two persistent EBS data volume at launch that will not be deleted when the instance is terminated so that your data isn't accidentally lost. The volumes will be encrypted with your default KMS key.<ul> <li>The Cortex data volume (/dev/sdh) is sized at 30GB by default</li> <li>The Docker volume (/dev/sdi) is sized at 20GB by default</li> </ul> </li> <li>The underlying Ubuntu OS is hardened to comply with CSL1 (that's Common Sense Level 1!) minus the network filtering. There are no iptables surprises inside the image to avoid conflicting behavior with security groups.</li> </ul>"},{"location":"resources/iaas/aws/cortex/#run-context","title":"Run context","text":"<ul> <li>The Cortex app runs as unprivileged user cortex and is available on port http 9001 (that's http and NOT https). Needless to say we encourage you never to open that port outside your VPC and use a public-facing load balancer and / or reverse proxy to handle the TLS sessions with end-users. Since many Cortex users also run TheHive and MISP instances alongside and since the right load balancer / reverse proxy is obviously the one you know best, we elected not to include yet another one in this AMI. We provide more information on using the AWS Application Load Balancer or reverse proxies in our detailed Cortex AMI user guide.</li> <li>The default sudoer user is ubuntu and the ssh service listens on port 22.</li> <li>The Cortex configuration is set to look for custom analyzers under <code>/opt/cortexneurons/analyzers</code> and for custom responders under <code>/opt/cortexneurons/responders</code>.</li> <li>A cronjob for user cortex runs every night (@daily) to backup the application configuration and custom analyzers / custom responders to the data volume (/var/lib/elasticsearch/cortex/). If you wish to launch a new instance from existing data, this job must have run at least once after the initial install in order to restore the application's configuration  and custom analyzers / responders as well. </li> </ul>"},{"location":"resources/iaas/aws/cortex/#launching-an-instance-with-no-existing-data-new-cortex-install","title":"Launching an instance with no existing data (new Cortex install)","text":"<ol> <li>Launch an instance from the AMI.</li> <li>SSH into the instance with the ubuntu user.</li> <li>Initialize and format the additional EBS volumes (/dev/sdh and /dev/sdi). Note that in Nitro-based instances, /dev/sdh and /dev/sdi might be available as something like /dev/nvme1n1 and /dev/nvme2n1. More information is available in Amazon EBS and NVMe on Linux Instances documentation.</li> <li>Launch the application initialization script with the EBS data volume block device names as arguments, which is /dev/sdh and /dev/sdi if you are using a default AMI setup. If you are using a Nitro-based instance, do not use the nvme names (like /dev/nvme1n1). Example: <code>/opt/cortex/ops/scripts/ops-cortex-init.sh /dev/sdh /dev/sdi</code></li> <li>That's it! Cortex is now available on port 9001. You can create the admin account on the first connection to the app.</li> </ol> <p>Alternatively, you can easily perform steps 3 and 4 by providing cloud-init user data to the AMI at launch. In the following example using an m5 instance (Nitro-based), we:</p> <ul> <li>launch a script that will expose the external volumes, seen by the instance as /dev/nvme1n1 and /dev/nvme2n1, with their block device mapping names (/dev/sdh, /dev/sdi)</li> <li>partition and format the EBS volumes using their block device mapping names (/dev/sdh, /dev/sdi)</li> <li>launch the initialisation script with the EBS block mapping names as arguments (/dev/sdh and /dev/sdi - not /dev/nvme0n1 and /dev/nvme1n1)</li> </ul> <pre><code>#cloud-config \nbootcmd:\n    - [ /usr/sbin/nvme-to-block-mapping ]\nfs_setup:\n    - filesystem: ext4\n      device: '/dev/sdh'\n      partition: auto\n      overwrite: false\n    - filesystem: ext4\n      device: '/dev/sdi'\n      partition: auto\n      overwrite: false\nruncmd:\n    - [ /opt/cortex/ops/scripts/ops-cortex-init.sh, /dev/sdh, /dev/sdi ]\n</code></pre> <p>You can also provision the whole thing using Terraform; check our GitHub repository for detailed sample code.</p>"},{"location":"resources/iaas/aws/cortex/#launching-an-instance-with-existing-data-cortex-update-migration-restore","title":"Launching an instance with existing data (Cortex update, migration, restore)","text":"<ol> <li>Launch an instance from the AMI and base the additional EBS volumes (/dev/sdh and /dev/sdi by default) on existing Cortex data and Docker volume snapshots.</li> <li>SSH into the instance with the ubuntu user.</li> <li>Launch the Cortex restore script with the EBS data volume block device names as arguments, which are /dev/sdh and /dev/sdi if you are using a default AMI setup. If you are using a Nitro-based instance, do not use the nvme names (like /dev/nvme1n1). Example: <code>/opt/cortex/ops/scripts/ops-cortex-restore.sh /dev/sdh /dev/sdi</code>.</li> <li>That's it! Cortex is now available on port 9001 (or on the custom port you had configured) with all your existing configuration, users and data. Custom analyzers and responders stored under /opt/cortexneurons are also automatically restored and their pip requirements reinstalled.</li> </ol> <p>Alternatively, you can easily perform step 3 by providing cloud-init user data to the AMI at launch. In the following example using an m5 instance (Nitro-based), we:</p> <p>launch the restore script with the EBS block mapping names as arguments (/dev/sdh and /dev/sdi - not /dev/nvme1n1 and /dev/nvme2n1)</p> <pre><code>#cloud-config \nruncmd:\n    - [ /opt/cortex/ops/scripts/ops-cortex-restore.sh, /dev/sdh, /dev/sdi ]\n</code></pre> <p>You can also provision the whole thing using Terraform; check our GitHub repository for detailed sample code.</p>"},{"location":"resources/iaas/aws/thehive/","title":"Deploy TheHive AMI","text":""},{"location":"resources/iaas/aws/thehive/#usage-instructions-for-the-official-thehive-v5-ami","title":"Usage instructions for the official TheHive v5 AMI","text":"<p> Official AMI</p> <p>The AMI can be used to set up a shiny-new TheHive v5 install or to launch an instance with existing data and configuration (for update / migration / restore purposes).</p>"},{"location":"resources/iaas/aws/thehive/#introduction","title":"Introduction","text":"<ul> <li>You can easily initialise a new instance or restore a previous TheHive v5 instance using scripts included in the image.</li> <li>Data is stored on three dedicated volumes: one for the Cassandra database (/var/lib/cassandra), another one for storage attachments (/opt/thp_data/files) and a final one for indexes (/opt/thp_data/index).</li> <li>The AMI is based on the official Ubuntu 20.04 LTS AMI from Canonical .</li> <li>The default OS hardening has been further improved compared to our previous Ubuntu 18.04 based AMIs.</li> <li>The AMI is updated whenever a new TheHive version is released - no need to bother updating TheHive anymore, just launch a new instance as if it were a container!</li> <li>Migration from TheHive v3 is a manual operation detailed at length in the documentation: https://docs.strangebee.com/thehive/setup/installation/migration/.</li> <li>Upgrading from TheHive v4 AMI is not yet automated or documented (it will be very soon in a coming AMI update). If you are in a hurry, the overall upgrade process is documented here: https://docs.strangebee.com/thehive/setup/installation/upgrade-from-4.x/</li> </ul>"},{"location":"resources/iaas/aws/thehive/#run-context","title":"Run context","text":"<ul> <li>TheHive app runs as an unprivileged user named thehive and is available on port http 9000 (that's http and NOT https). Needless to say we encourage you never to open that port outside your VPC and use a public-facing load balancer and / or reverse proxy to handle the TLS sessions with end-users. Since many TheHive users also run Cortex and MISP instances alongside and since the right load balancer / reverse proxy is obviously the one you know best, we elected not to include yet another one in this AMI.</li> <li>As an incentive to use https, TheHive is configured to use secure cookies by default. Connecting to the UI in http will fail. An override to this remains possible in the configuration: set <code>play.http.session.secure = false</code> in <code>/etc/thehive/application.conf</code>. You must restart the <code>thehive</code> service for the change to be effective.</li> <li>The default sudoer user is ubuntu and the ssh service listens on port 22.</li> <li>A cronjob for user thehive runs every night (@daily) to backup the application configuration to the data volume (/var/lib/cassandra/thehive/). If you wish to launch a new instance from existing data, this job must have run at least once after the initial install in order to restore the application's configuration as well.</li> </ul>"},{"location":"resources/iaas/aws/thehive/#launching-an-instance-with-no-existing-data-new-thehive-install","title":"Launching an instance with no existing data (new TheHive install)","text":"<ol> <li>Launch an instance from the AMI.</li> <li>SSH into the instance with the ubuntu user.</li> <li>Initialize and format the additional EBS volumes (<code>/dev/sdh</code>, <code>/dev/sdi</code> and <code>/dev/sdj</code>). Note that in Nitro-based instances, <code>/dev/sdh</code> might be available as something like <code>/dev/nvme1n1</code>. More information is available in Amazon EBS and NVMe on Linux Instances documentation.</li> <li>Launch the application initialization script with the EBS data volumes block device names as arguments, which are <code>/dev/sdh</code>, <code>/dev/sdi</code> and <code>/dev/sdj</code> if you are using a default AMI setup. If you are using a Nitro-based instance, do not use the nvme names (like /dev/nvme0n1). Example: <code>/opt/thehive/ops/scripts/ops-thehive-init.sh /dev/sdh /dev/sdi /dev/sdj</code></li> <li>That's it! TheHive is now available on port 9000. The default admin account is \"admin@thehive.local\" with password \"secret\" (change it!).</li> </ol> <p>Alternately, you can easily perform steps 3 and 4 by providing cloud-init user data to the AMI at launch. In the following example using an m5 instance (Nitro-based), we: * launch a script that will expose the external volumes, seen by the instance as /dev/nvme1n1 and /dev/nvme2n1, with their block device mapping names (/dev/sdh, /dev/sdi) * partition and format the EBS volumes using their block device mapping names (/dev/sdh, /dev/sdi) *  launch the initialisation script with the EBS block mapping names as argument (<code>/dev/sdh</code>, <code>/dev/sdi</code> and <code>/dev/sdj</code>)</p> <pre><code>#cloud-config\nbootcmd:\n    - [ /usr/sbin/nvme-to-block-mapping ]\nfs_setup:\n    - filesystem: ext4\n      device: '/dev/sdh'\n      partition: auto\n      overwrite: false\n    - filesystem: ext4\n      device: '/dev/sdi'\n      partition: auto\n      overwrite: false\n    - filesystem: ext4\n      device: '/dev/sdj'\n      partition: auto\n      overwrite: false\nruncmd:\n    - [ /opt/thehive/ops/scripts/ops-thehive-init.sh, /dev/sdh, /dev/sdi, /dev/sdj ]\n</code></pre> <p>You can also provision the whole thing using Terraform; check our GitHub repository for detailed sample code.</p>"},{"location":"resources/iaas/aws/thehive/#launching-an-instance-with-existing-data-thehive-update-migration-restore","title":"Launching an instance with existing data (TheHive update, migration, restore)","text":"<ol> <li>Launch an instance from the AMI and base the additional EBS volumes (<code>/dev/sdh</code>, <code>/dev/sdi</code> and <code>/dev/sdj</code> by default) on existing TheHive EBS volume snapshots for the Cassandra database (<code>/dev/sdh</code>), the storage attachments (<code>/dev/sdi</code>) and the database index (<code>/dev/sdj</code>).</li> <li>SSH into the instance with the ubuntu user.</li> <li>Launch the TheHive restore script with the EBS data volumes block device names as arguments, which are <code>/dev/sdh</code>, <code>/dev/sdi</code> and <code>/dev/sdj</code> if you are using a default AMI setup. If you are using a Nitro-based instance, do not use the nvme names (like /dev/nvme1n1). Example: <code>/opt/thehive/ops/scripts/ops-thehive-restore.sh /dev/sdh /dev/sdi /dev/sdj</code>.</li> <li>That's it! TheHive is now available on port 9000 (or on the custom port you had configured) with all your existing configuration, users and data.</li> </ol> <p>Alternately, you can easily perform step 3 by providing cloud-init user data to the AMI at launch. In the following example using an m5 instance (Nitro-based), we:</p> <ul> <li>launch the restore script with the EBS block mapping names as arguments (<code>/dev/sdh</code>, <code>/dev/sdi</code> and <code>/dev/sdj</code>)</li> </ul> <pre><code>#cloud-config\nruncmd:\n    - [ /opt/thehive/ops/scripts/ops-thehive-restore.sh, /dev/sdh, /dev/sdi, /dev/sdj ]\n</code></pre> <p>You can also provision the whole thing using Terraform; check our GitHub repository for detailed sample code.</p>"},{"location":"resources/iaas/aws/infra-as-code/","title":"AWS sample code","text":""},{"location":"resources/iaas/aws/infra-as-code/#aws-sample-code","title":"AWS sample code","text":""},{"location":"resources/iaas/aws/infra-as-code/#overview","title":"Overview","text":"<p>The sample Terraform code in this repository allows the creation of a complete SecOps VPC to host your TheHive and Cortex instances and expose them using a load balancer.</p> <p></p> <p>The sample code is organised in two Terraform projects:</p> <ul> <li>ug-secops-vpc --&gt; to create and manage the SecOps VPC</li> <li>ug-secops-instances --&gt; to launch and manage TheHive v5 and Cortex instances within a SecOps VPC</li> </ul> <p>This code organisation allows the creation of all required VPC resources if you do not already operate a VPC (or if you want to create a new one for your SecOps needs), independently from TheHive and Cortex deployments.</p> <p>The sample code to launch TheHive and Cortex instances defaults to this new VPC but can easily be adapted to fit your existing VPC by customising a few variables. Unless your architecture significantly differs from our reference VPC, you should not be required to modify the Terraform code itself.</p>"},{"location":"resources/iaas/aws/infra-as-code/ug-secops-instances/","title":"TheHive v5 and Cortex within a SecOps VPC","text":""},{"location":"resources/iaas/aws/infra-as-code/ug-secops-instances/#deploying-thehive-v5-and-cortex-with-terraform","title":"Deploying TheHive v5 and Cortex with Terraform","text":"<p>This code will work out of the box with the reference SecOps VPC created with our sample code. You can nonetheless use it to deploy TheHive and Cortex within your own preexisting VPC with minimal adjustments (only a few variables to update if your setup is similar to our reference architecture).</p> <p>Our sample code can handle two use-cases:</p> <ul> <li>Deploying brand new TheHive and Cortex instances with empty databases - this is useful for an initial deployment.</li> <li>Deploying TheHive and Cortex instances while restoring existing databases - this is to be used for all other use-cases: AMI updates, instance type upgrades or downgrades, database restore, etc.</li> </ul> <p>Tip</p> <p>To switch between both use-cases, simply update the <code>secops_thehive_init</code> and/or <code>secops_cortex_init</code> variable values between <code>true</code> and <code>false</code> (<code>true</code> being the empty database, initialisation use-case).</p> <p>Files</p> <ul> <li>main.cf</li> <li>providers.cf</li> <li>outputs.cf</li> <li>variables.cf</li> <li>samples.tfvars</li> <li>files/bastion-cloud-config-new.tpl</li> </ul>"},{"location":"resources/iaas/aws/infra-as-code/ug-secops-instances/#tldr","title":"TL;DR;","text":"<ul> <li>Download all files</li> <li>Set the required variables</li> <li><code>terraform init</code> &amp;&amp; <code>terraform apply</code></li> <li>Once the <code>terraform apply</code> is over, wait up to 5 minutes for both instances to be fully operational. You can check the initiatisation or restore progress by tailing the <code>/var/log/cloud-init-output.log</code> file on each instance.</li> </ul>"},{"location":"resources/iaas/aws/infra-as-code/ug-secops-instances/#overview","title":"Overview","text":"<p>This is an overview of the resulting TheHive and Cortex instances when deployed with our Terraform sample code in our reference SecOps VPC.</p> <p></p>"},{"location":"resources/iaas/aws/infra-as-code/ug-secops-instances/#information-on-the-default-data-volumes-configuration","title":"Information on the default data volumes configuration","text":"<p>All data is stored on dedicated EBS volumes, not in the root EBS volume. This approach has many advantages:</p> <ul> <li>Your root volume is disposable, you can replace your instances in seconds to update TheHive and Cortex by launching a fresh AMI or to migrate to a more (or less) powerful instance.</li> <li>Your data volumes can be of any size while keeping the root volume to its default size. </li> <li>Increasing (or decreasing) the size of data volumes on an existing instance is a lot easier than changing the root volume size.</li> <li>You can easily restore your data from a previous states using volume snapshots. </li> </ul>"},{"location":"resources/iaas/aws/infra-as-code/ug-secops-instances/#about-ebs-volume-management-on-nitro-based-instances","title":"About EBS volume management on Nitro based instances","text":"<p>In Nitro-based instances, block devide mappings such as <code>/dev/sdh</code> will be seen by the instance as something like <code>/dev/nvme1n1</code>. You need to take this into consideration for everything that is executed inside the instance. As far as the EC2 APIs are concerned, the volume is still known as <code>/dev/sdh</code>. More information on this is available in Amazon EBS and NVMe on Linux Instances documentation.</p> <p>To illustrate this important aspect, consider the automated deployment of an instance using Terraform and some cloud-init user data code. In Terraform, you may want to change the default volume size or base your EBS volume on an existing snapshot. Since Terraform is interacting with the EC2 APIs, the EBS volume will alway be <code>/dev/sdh</code>. However, if you want to partition and format the volume using cloud-init, you need to adapt your code to how the instance \"sees\" the volume. In pre-Nitro instances the volume will remain <code>/dev/sdh</code> but in Nitro instances, it will be known as something like <code>/dev/nvme1n1</code>. </p> <p>To mount the volumes, the included initialisation and restore cloud-init bootstrap scripts were designed to be \"Nitro-aware\". These scripts expect the block device mapping as argument (such as <code>/dev/sdh</code>, <code>/dev/sdi</code> and <code>/dev/sdj</code>) and will then mount the volume based on its UUID to avoid any confusion going forward.</p>"},{"location":"resources/iaas/aws/infra-as-code/ug-secops-instances/#deploying-new-thehive-and-cortex-instances","title":"Deploying new TheHive and Cortex instances","text":"<p>When the <code>secops_thehive_init</code> and/or <code>secops_cortex_init</code> variable are set to <code>true</code>, the AMIs will create new empty EBS data volumes at launch that will not be deleted when the instances are terminated so that your data isn't accidentally lost.</p>"},{"location":"resources/iaas/aws/infra-as-code/ug-secops-instances/#replacing-existing-thehive-and-cortex-instances-to-use-a-new-ami-version","title":"Replacing existing TheHive and Cortex instances to use a new AMI version","text":"<p>When the <code>secops_thehive_init</code> and/or <code>secops_cortex_init</code> variable are set to <code>false</code>, the AMIs will create new EBS volumes at launch based on snapshots of volumes from the previous instances. The original volumes (from previous instances) are not automatically deleted. We recommend you keep them at least until the upgrade to the new AMI is completed.</p>"},{"location":"resources/iaas/aws/infra-as-code/ug-secops-instances/#restoring-data-from-earlier-snapshots-instead-of-latest-instance-volume-snapshots","title":"Restoring data from earlier snapshots instead of latest instance volume snapshots","text":"<p>If for any reason you wish to restore from specific snapshots and not from the latest volume state, you can edit the code and set the snapshot id for each volume. </p> <p>For example, to restore the TheHive data volume from a specific snapshot, the sample code should be edited to update the <code>snapshot_id</code> value.</p> <p>Original sample code:</p> <pre><code>  ebs_block_device {\n    device_name = \"/dev/sdh\"\n    snapshot_id = aws_ebs_snapshot.secops-thehive-data-snapshot[count.index].id\n    volume_type = \"gp2\"\n    volume_size = var.secops_thehive_instance_data_volume_size\n    delete_on_termination = false\n    tags = {\n      Name = \"${var.secops_thehive_instance_name}-data\"\n      SourceInstance = var.secops_thehive_instance_name\n      SourceInstanceVolume = \"/dev/sdh\"\n      Environment = var.secops_vpc_name\n    }  \n  } \n</code></pre> <p>Update code:</p> <pre><code>  ebs_block_device {\n    device_name = \"/dev/sdh\"\n    snapshot_id = \"snap-1234567890\"\n    volume_type = \"gp2\"\n    volume_size = var.secops_thehive_instance_data_volume_size\n    delete_on_termination = false\n    tags = {\n      Name = \"${var.secops_thehive_instance_name}-data\"\n      SourceInstance = var.secops_thehive_instance_name\n      SourceInstanceVolume = \"/dev/sdh\"\n      Environment = var.secops_vpc_name\n    }  \n  } \n</code></pre>"},{"location":"resources/iaas/aws/infra-as-code/ug-secops-instances/#connecting-to-your-instances-with-ssh","title":"Connecting to your instances with SSH","text":"<p>Since our TheHive and Cortex instance are located in a private subnet, we cannot directly SSH into them using their private IP addresses. If you have set up a bastion host configuration similarly to our reference architecture, you can seamlessly connect to private instances using the proxyjump functionality of the ssh client. The bastion host will be able to perform the hostname resolution with the private DNS zone we have set up in the VPC.</p> <p>The easiest way to do that is to create (or update) the <code>~/.ssh/config</code> file. Use the example below as a reference and replace the ip addresses and private key information.</p> <p>The default username is <code>ubuntu</code> in all our AMIs.</p> <pre><code>Host bastion\n        HostName 1.2.3.4 (public ip)\n        User ubuntu\n        Port 22\n        IdentityFile ~/.ssh/id_rsa_private_key_for_bastion\n\nHost thehive\n        HostName thehive.secops0.privatevpc\n        User ubuntu\n        Port 22\n        ProxyJump bastion\n        IdentityFile ~/.ssh/id_rsa_private_key_for_thehive\n\nHost cortex\n        HostName cortex.secops0.privatevpc\n        User ubuntu\n        Port 22\n        ProxyJump bastion\n        IdentityFile ~/.ssh/id_rsa_private_key_for_cortex\n</code></pre> <p>We use the secops0.privatevpc domain as an example but the best security practice is to use a domain name you own even for private DNS resolution in split-horizon.</p> <p>You will now be able to SSH into your instances directly using the bastion host as a proxy:</p> <p></p><pre><code>ssh thehive\n</code></pre> or <pre><code>ssh cortex\n</code></pre> <p>Tip</p> <p>Remember to autorize your local public IP address in the bastion SSH security group. In our sample code for the SecOps VPC, this is the <code>secops0-public-ssh-sg</code> security group.</p> <p>Terraform compatibility: v1.x</p>"},{"location":"resources/iaas/aws/infra-as-code/ug-secops-vpc/","title":"Create and manage the SecOps VPC","text":""},{"location":"resources/iaas/aws/infra-as-code/ug-secops-vpc/#creating-a-new-secops-vpc","title":"Creating a new SecOps VPC","text":"<p>If you do not already have a VPC at hand to deploy TheHive and Cortex into, using our sample code will allow you to build a production-ready VPC very easily.</p> <p>Files</p> <ul> <li>main.cf</li> <li>providers.cf</li> <li>outputs.cf</li> <li>variables.cf</li> <li>samples.tfvars</li> <li>files/bastion-cloud-config-new.tpl</li> </ul>"},{"location":"resources/iaas/aws/infra-as-code/ug-secops-vpc/#overview","title":"Overview","text":"<p>The reference architecture VPC consists of the following resources</p> <p></p> <p>The VPC will include:</p> <ul> <li>Two public subnets (only one is depicted - two public subnets will be created in different availability zones since they are required for the Application Load Balancer configuration)</li> <li>Two private subnets (only one is depicted - we will not be using the second private subnet but it will be ready to use if you need it and it does not incur additional costs)</li> <li>Five security groups </li> <li>An internet gateway (IG)</li> <li>A NAT gateway in the first public subnet</li> <li>Route tables for both public and private subnets</li> <li>An Application Load Balancer (ALB) with a listener on port 443  using ACM-managed public certificates and two target groups for TheHive and Cortex</li> <li>Public Route53 DNS records for TheHive and Cortex to point to the ALB</li> <li>A private Route53 DNS zone for internal VPC name resolution</li> <li>A bastion host for remote SSH administration</li> </ul> <p>Since we will be building a new VPC from scratch, we will use AWS-managed services to expose our TheHive and Cortex instances:</p> <ul> <li>Application Load Balancer to handle secured TLS sessions with end-users. A single load balancer with a single listener on port 443 can forward traffic to both TheHive and Cortex instances based on forward rules.</li> <li>AWS Certificate Manager to issue and renew valid public certificates to enable TLS sessions between end-users and the load balancer. Once attached to an AWS-managed service such as the Application Load Balancer, public certificates are automatically renewed when nearing expiration. If you operate both TheHive and Cortex, you can share a single certificate for both services by including multiple hostnames in the certificate. You can also use separate certificates as the ALB supports attaching several certificates to a listener.</li> <li>Amazon Route53 to manage your public DNS records. You will need Route53 to manage at least one public DNS zone, but not necessarily a whole domain name. It can be a subdomain of an existing domain, such as aws.mydomain.com. Having Route53 manage your DNS records enables a lot of automation such as automatic certificate validation and renewal, automatic DNS registration of your load balancer and so on. We also recommend you use Route53 to manage a private DNS zone attached to your VPC to enable local name resolution within the private subnet (this is how TheHive can easily find its Cortex instance and it allows to SSH into private instances without having to bother with their private IPs).</li> </ul>"},{"location":"resources/iaas/aws/infra-as-code/ug-secops-vpc/#security-groups","title":"Security Groups","text":"<p>There are no default iptables rules implemented in the AMIs for either TheHive or Cortex (no OS-based IP filtering). Since we built the AMIs to be replaceable at each application update, somewhat like containers, we recommend limiting OS customisations to benefit from the easy update process. For that reason, filtering should be based on security groups or Network ACLs only.</p> <p>Keep in mind that the applications are listening on http, not https. Even though the default AMI security groups allow incoming traffic on the http ports (TCP 9000 for TheHive, TCP 9001 for Cortex), be careful not to expose them on a public-facing network interface.</p> <p>The required security groups depicted above are created automatically along with the SecOps VPC.</p>"},{"location":"resources/iaas/aws/infra-as-code/ug-secops-vpc/#bastion-host","title":"Bastion host","text":"<p>We launch a small instance to act as a bastion host. Bastion host hardening is not performed automatically but you should definitely harden this host going forward if you will use it in a production context. We do however strictly limit access to and from this host.</p> <p>The bastion host will run the latest Ubuntu AMI from Canonical. The default sudoer user is ubuntu.</p>"},{"location":"resources/iaas/aws/infra-as-code/ug-secops-vpc/#secops-vpc-prerequisites","title":"SecOps VPC prerequisites","text":"<p>While most VPC resources will be provisioned with Terraform, there is one exception that should be created beforehand:</p> <ul> <li>The Route53 public DNS zone to register the load balancer and to automatically validate the ACM certificates</li> </ul> <p>You must provide the DNS hosted zone name before creating the VPC (set the <code>secops_r53_public_dns_zone_name</code> Terraform variable with the zone name).</p> <p>Info</p> <p>We will use a single load balancer, a single https listener and a single certificate for both TheHive and Cortex. Since the default configuration is to route TheHive and Cortex queries based on the host name, make sure you provide both TheHive and Cortex host names in the <code>secops_r53_records_san</code> Terraform variable so they are provisionned both in the load balancer listener certificate and DNS records.</p> <p>Terraform compatibility: v1.x</p>"},{"location":"resources/iaas/azure/","title":"Main Features","text":""},{"location":"resources/iaas/azure/#thehive-azure-images","title":"TheHive Azure images","text":""},{"location":"resources/iaas/azure/#easy-to-use-and-deploy","title":"Easy to use and deploy","text":"<p>The Azure images were built with operations and automation in mind. We wanted DevSecOps-friendly products that would fit in most organizations, no matter how simple or complex their infrastructure.</p>"},{"location":"resources/iaas/azure/#always-up-to-date","title":"Always up-to-date","text":"<p>The images are updated whenever a new TheHive version is released. No need to bother updating TheHive anymore, just launch a new instance as if it were a container!</p>"},{"location":"resources/iaas/azure/#production-ready","title":"Production-ready","text":"<ul> <li>Dedicated data disks: TheHive data is stored on dedicated disks, not in the root filesystem.</li> <li>Ubuntu-based: The images are based on the official Ubuntu 20.04 LTS distribution from Canonical.</li> <li>Hardened OS: The underlying Ubuntu OS is hardened to comply with CSL1 (that's Common Sense Level 1!) minus the network filtering. There are no iptables surprises inside the image to avoid conflicting behavior with security groups.</li> <li>Application only: The images include the applications only. They are not meant to be public-facing on their own and should be deployed within your virtual network and exposed with the public facing system of your choice (load balancer, reverse proxy).</li> </ul>"},{"location":"resources/iaas/azure/#cortex-azure-images","title":"Cortex Azure images","text":""},{"location":"resources/iaas/azure/#easy-to-use-and-deploy_1","title":"Easy to use and deploy","text":"<p>The image was built with operations and automation in mind. We wanted a DevSecOps-friendly product that would fit in most organizations, no matter how simple or complex their infrastructure.</p>"},{"location":"resources/iaas/azure/#always-up-to-date_1","title":"Always up-to-date","text":"<p>The image is updated whenever a new Cortex version is released. No need to bother updating Cortex anymore, just launch a new instance as if it were a container!</p>"},{"location":"resources/iaas/azure/#production-ready_1","title":"Production-ready","text":"<ul> <li>Dedicated data disks: Cortex data is stored on dedicated disks, not in the root filesystem. With that in mind, you must attach two persistent data disks on LUN 0 and LUN 1 at launch (30 GB each recommended).</li> <li>Ubuntu-based: The image is based on the official Ubuntu 20.04 LTS distribution from Canonical.</li> <li>Hardened OS: The underlying Ubuntu OS is hardened to comply with CSL1 (that's Common Sense Level 1!) minus the network filtering. There are no iptables surprises inside the image to avoid conflicting behavior with security groups.</li> <li>Application only: The image includes Cortex application only. It is not meant to be public-facing on its own and should be deployed within your virtual network and exposed with the public facing system of your choice (load balancer, reverse proxy).</li> </ul>"},{"location":"resources/iaas/azure/cortex/","title":"Deploy Cortex Azure image","text":""},{"location":"resources/iaas/azure/cortex/#usage-instructions-for-the-official-azure-distribution-of-cortex","title":"Usage instructions for the official Azure distribution of Cortex","text":"<p>:simple-microsoftazure: Official Azure image This is the official Cortex v3 release for the AWS Marketplace.</p> <p>The image can be used to set up a shiny-new Cortex install or to launch an instance with existing data and configuration (for update / migration / restore purposes).</p>"},{"location":"resources/iaas/azure/cortex/#introduction","title":"Introduction","text":"<ul> <li>Based on the official Ubuntu 18.04 image from Canonical</li> <li>The image is updated whenever a new Cortex version is released - no need to bother updating Cortex anymore, just launch a new instance as if it were a container!</li> <li>Cortex data is stored on dedicated disks, not in the root filesystem. With that in mind, you must attach two persistent data disks on LUN 0 (for the database) and LUN 1 (for Docker images) at launch (30 GB each recommended).</li> <li>The underlying Ubuntu OS is hardened to comply with CSL1 (that's Common Sense Level 1!) minus the network filtering. There are no iptables surprises inside the image to avoid conflicting behavior with security groups.</li> </ul>"},{"location":"resources/iaas/azure/cortex/#run-context","title":"Run context","text":"<ul> <li>The Cortex app runs as unprivileged user cortex and is available on port http 9001 (that's http and NOT https). Needless to say we encourage you never to open that port outside your virtual network and use a public-facing load balancer and / or reverse proxy to handle the TLS sessions with end-users. Since many Cortex users also run TheHive and MISP instances alongside and since the right load balancer / reverse proxy is obviously the one you know best, we elected not to include yet another one in this image. </li> <li>The Cortex configuration is set to look for custom analyzers under <code>/opt/cortexneurons/analyzers</code> and for custom responders under <code>/opt/cortexneurons/responders</code>.</li> <li>A cronjob for user cortex runs every night (@daily) to backup the application configuration and custom analyzers / custom responders to the data volume (/var/lib/elasticsearch/cortex/). If you wish to launch a new instance from existing data, this job must have run at least once after the initial install in order to restore the application's configuration  and custom analyzers / responders as well.  </li> </ul>"},{"location":"resources/iaas/azure/cortex/#launching-an-instance-with-no-existing-data-new-cortex-install","title":"Launching an instance with no existing data (new Cortex install)","text":"<ol> <li>Launch an instance from the image and attach two data disks on LUN 0 and LUN 1.</li> <li>SSH into the instance with a sudoer user.</li> <li>Initialize and format the additional data disks (LUN O and LUN 1). </li> <li>Launch the application initialization script with the target data disk names as arguments. Example: <code>/opt/cortex/ops/scripts/ops-cortex-init.sh /dev/sdh /dev/sdi</code> (the script will automatically mount the LUN 0 and LUN 1 disks at /dev/sdh and /dev/sdi)</li> <li>That's it! Cortex is now available on port 9001. You can create the admin account on the first connection to the app.</li> </ol> <p>Alternatively, you can easily perform steps 3 and 4 by providing a cloud-init bootstrap script at launch. In the following example, we:</p> <ul> <li>partition and format the data disks attached on LUN 0 and LUN 1 </li> <li>improve the random seed with pollinate (because we will generate a secret key in the initialisation process)</li> <li>and finally we launch the initialisation script with the target data disk mapping names as arguments (/dev/sdh and /dev/sdi) - the script will automatically mount the LUN 0 disk at /dev/sdh and LUN 1 at /dev/sdi</li> </ul> <pre><code>#cloud-config \ndisk_setup:\n  /dev/disk/azure/scsi1/lun0:\n    table_type: gpt\n    layout: True\n    overwrite: True\n  /dev/disk/azure/scsi1/lun1:\n    table_type: gpt\n    layout: True\n    overwrite: True\nfs_setup:\n  - device: /dev/disk/azure/scsi1/lun0\n    partition: none\n    filesystem: ext4\n  - device: /dev/disk/azure/scsi1/lun1\n    partition: none\n    filesystem: ext4\nrandom_seed:\n    file: /dev/urandom\n    command: [\"pollinate\", \"--server=https://entropy.ubuntu.com/\"]\n    command_required: true\nruncmd:\n    - /opt/cortex/ops/scripts/ops-cortex-init.sh /dev/sdh /dev/sdi\n</code></pre> <p>You can also provision the whole thing using Terraform - check our GitHub repository for sample initialisation code.</p>"},{"location":"resources/iaas/azure/cortex/#launching-an-instance-with-existing-data-cortex-update-migration-restore","title":"Launching an instance with existing data (Cortex update, migration, restore)","text":"<ol> <li>Launch an instance from the image and attach existing Cortex data disks on LUN 0 and LUN 1 (we recommend you always create disk snapshots first).</li> <li>SSH into the instance with a sudoer user.</li> <li>Launch the Cortex restore script with the data disk names as argument, which are /dev/sdh and /dev/sdi if you are using the default setup. Example: <code>/opt/cortex/ops/scripts/ops-cortex-restore.sh /dev/sdh /dev/sdi</code>.</li> <li>That's it! Cortex is now available on port 9001 (or on the custom port you had configured) with all your existing configuration, users and data. Custom analyzers and responders stored under /opt/cortexneurons are also automatically restored and their pip requirements reinstalled.</li> </ol> <p>Alternatively, you can easily perform step 3 by providing a cloud-init bootstrap script at launch. In the following example, we:</p> <p>launch the restore script with the data disk names as arguments (/dev/sdh and /dev/sdi)</p> <pre><code>#cloud-config \nruncmd:\n    - /opt/cortex/ops/scripts/ops-cortex-restore.sh /dev/sdh /dev/sdi\n</code></pre> <p>You can also provision the whole thing using Terraform - check our GitHub repository for sample update / migration / restore code.</p>"},{"location":"resources/iaas/azure/thehive/","title":"Deploy TheHive Azure image","text":""},{"location":"resources/iaas/azure/thehive/#usage-instructions-for-the-official-azure-distribution-of-thehive-v5","title":"Usage instructions for the official Azure distribution of TheHive v5","text":"<p>:simple-microsoftazure: Official Azure image</p> <p>The image can be used to set up a shiny-new TheHive v5 install or to launch an instance with existing data and configuration (for update / migration / restore purposes). The same image can also be used to launch Cortex on the same instance or on a separate, dedicated instance.</p>"},{"location":"resources/iaas/azure/thehive/#introduction","title":"Introduction","text":"<ul> <li>Based on the official Ubuntu 20.04 LTS image from Canonical</li> <li>The image is updated whenever a new TheHive version is released - no need to bother updating TheHive anymore, just launch a new instance as if it were a container!</li> <li>TheHive / Cortex data is stored on two dedicated disks, not in the root filesystem. For that purpose, you must attach two persistent data disks at launch on LUN 0 (for the data) and LUN 1 (for Docker). The recommended minimal volume size are 32 GB per volume. If you install Cortex on a separate instance, simply apply the same configuration on the second instance.</li> <li>The underlying Ubuntu OS is hardened to comply with CSL1 (that's Common Sense Level 1!) minus the network filtering. There are no iptables surprises inside the image to avoid conflicting behaviour with security groups.</li> <li>Migration from TheHive v4 is possible using our migration script baked in the image (to be documented soon).</li> </ul>"},{"location":"resources/iaas/azure/thehive/#run-context","title":"Run context","text":"<ul> <li>TheHive is available on port http 9000 and Cortex, when deployed alongside, is available on port http 9001 (that's http and NOT https). Needless to say we encourage you never to open these ports outside your virtual network and use a public-facing load balancer and / or reverse proxy to handle the TLS sessions with end-users. </li> <li>As an incentive to use https, both TheHive and Cortex are configured to use secure cookies by default. Connecting to their respective UIs in http will fail. An override to this remains possible in the configuration (for TheHive, set <code>play.http.session.secure = false</code> in <code>/opt/thp_data/nomad/tasks/thehive/application.conf</code> - for Cortex, set <code>play.http.session.secure = false</code> and <code>play.filters.csrf.cookie.secure = false</code> in <code>/opt/thp_data/nomad/tasks/cortex/application.conf</code>). You must restart the <code>thehive</code> and/or <code>cortex</code> service(s) for the change to be effective.</li> </ul>"},{"location":"resources/iaas/azure/thehive/#launching-an-instance","title":"Launching an instance","text":"<ol> <li>Launch an instance from the image and attach two persistent disks: the data disk on LUN 0 and the docker disk LUN 1</li> <li>Set the admin username to be azureuser</li> <li>Provide the following cloud-init bootstrap script to configure the instance (you must update at least the <code>application.baseUrl</code> value for TheHive in this example):</li> </ol> <pre><code>#cloud-config \ndisk_setup:\n  /dev/disk/azure/scsi1/lun0:\n    table_type: gpt\n    layout: True\n    overwrite: True\n  /dev/disk/azure/scsi1/lun1:\n    table_type: gpt\n    layout: True\n    overwrite: True\nfs_setup:\n  - device: /dev/disk/azure/scsi1/lun0\n    partition: auto\n    filesystem: ext4\n  - device: /dev/disk/azure/scsi1/lun1\n    partition: auto\n    filesystem: ext4\nwrite_files:\n    - path: /opt/strangebee/ops/templates/nomad/tasks/thehive/application.conf.d/service.conf\n      content: |\n          application.baseUrl=\"https://thehive.mydomain.com/thehive\"\n          play.http.context=\"/thehive\"\n    - path: /opt/strangebee/ops/templates/nomad/tasks/cortex/application.conf.d/service.conf\n      content: |\n          play.http.context=\"/cortex\"\nruncmd:\n    - [ /opt/strangebee/ops/scripts/ops-launch.sh, \"-t 1\", \"-c 0\", \"-p /dev/sdh\", \"-d /dev/sdi\" ]\n</code></pre> <p>You can further customize this script as needed. In the example above we:</p> <ul> <li>partition and format the data disks attached on LUN 0 and LUN 1 </li> <li>launch the initialisation script with the target data disk mapping names as argument (/dev/sdh and /dev/sdi) - the script will automatically mount the LUN 0 disk at /dev/sdh and the LUN 1 disk at /dev/sdi</li> <li>install TheHive only (because of parameters <code>\"-t 1\", \"-c 0\"</code> when calling the <code>ops-launch.sh</code> script)</li> </ul> <p>Tip</p> <ol> <li>To install both TheHive and Cortex on the same instance, use <code>\"-t 1\", \"-c 1\"</code></li> <li>To install Cortex only on another instance, use <code>\"-t 0\", \"-c 1\"</code></li> </ol> <p>That's it! TheHive is now available (for your load balancer or reverse proxy) on port 9000 and Cortex on port 9001, if also installed. The default admin account on both applications is <code>admin</code> with password <code>secret</code> (change them!). </p> <p>Remember to access the apps through an internet-facing https system such as a reverse proxy or load balancer. In our example, TheHive is available at <code>https://thehive.mydomain.com/thehive</code> and Cortex at <code>https://thehive.mydomain.com/cortex</code>.</p>"},{"location":"resources/iaas/azure/thehive/#even-easier-using-terraform","title":"Even easier using Terraform","text":"<p>You can also provision the whole thing using Terraform.</p> <p>Check our GitHub repository for turnkey deployment code, including a full SecOps vnet with an https Application Gateway.</p>"},{"location":"resources/iaas/azure/infra-as-code/","title":"Azure sample code","text":""},{"location":"resources/iaas/azure/infra-as-code/#azure-sample-code","title":"Azure sample code","text":""},{"location":"resources/iaas/azure/infra-as-code/#overview","title":"Overview","text":"<p>The sample Terraform code in this repository allows the creation of a complete SecOps virtual network (vnet) to host your TheHive and Cortex instances and expose them using an Application Gateway.</p> <p></p> <p>The sample code is organised in two Terraform projects:</p> <ul> <li>ug-secops-vnet --&gt; to create and manage the SecOps virtual network and TheHive / Cortex data disks</li> <li>ug-secops-instances --&gt; to launch and manage TheHive v5 and Cortex instances within a SecOps vnet</li> </ul> <p>This code organisation allows the creation of all required vnet resources if you do not already operate a vnet (or if you want to create a new one for your SecOps needs), independently from TheHive and Cortex deployments.</p> <p>The sample code to launch TheHive and Cortex instances defaults to this new vnet but can easily be adapted to fit your existing vnet by customising a few variables. Unless your architecture significantly differs from our reference vnet, you should not be required to modify the Terraform code itself.</p>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/","title":"Deploying TheHive v5 and Cortex with Terraform","text":""},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/#deploying-thehive-v5-and-cortex-with-terraform","title":"Deploying TheHive v5 and Cortex with Terraform","text":"<p>This code will work out of the box with the reference SecOps vnet created with our sample code. You can nonetheless use it to deploy TheHive and Cortex within your own pre-existing vnet with minimal adjustments (only a few variables to update if your setup is similar to our reference architecture).</p> <p>Our sample code can handle two use-cases:</p> <ul> <li>Deploying brand new TheHive and Cortex instances with empty databases - this is useful for an initial deployment.</li> <li>Deploying TheHive and Cortex instances while restoring existing databases - this is to be used for all other use-cases: image updates, instance size upgrades or downgrades, database restore, etc.</li> </ul> <p>Note</p> <pre><code>The instance initialization script will automatically detect if the persistent disks already contain data and will behave accordingly (initial setup or restore).\n</code></pre>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/#files","title":"Files","text":"<p>Files</p> <pre><code>Terraform files are available on [Github](https://github.com/StrangeBeeCorp/cloud-distrib-resources/blob/master/azure/ug-secops-instances/):\n</code></pre> <ul> <li>000-provider.tf</li> <li>005-runcontext.tf</li> <li>040-securitygroup-th.tf</li> <li>040-securitygroup-th.tf</li> <li>090-instance-cortex.tf</li> <li>090-instance-th.tf</li> <li>output.tf</li> <li>variables.tf</li> <li>tarreform.tfvars</li> <li>files/cloud-config.tpl</li> </ul>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/#tldr","title":"TL;DR;","text":"<ul> <li>Download the files</li> <li>Set / update the required variables</li> <li><code>terraform init</code> &amp;&amp; <code>terraform apply</code></li> <li>Once the <code>terraform apply</code> is over, wait up to 5 minutes for both instances to be fully operational. You can check the initialisation or restore progress by tailing the <code>/var/log/cloud-init-output.log</code> and <code>/opt/strangebee/ops/logs/ops-launch.sh.log</code> files on each instance.</li> </ul>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/#overview","title":"Overview","text":"<p>This is an overview of the resulting TheHive and Cortex instances when deployed with our Terraform sample code in our reference SecOps vnet.</p> <p></p>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/#information-on-the-default-data-disks-configuration","title":"Information on the default data disks configuration","text":"<p>All TheHive and Cortex data is stored on dedicated data disks, not in the OS disks. This approach has many advantages:</p> <ul> <li>Your OS disk is disposable, you can replace your instances in seconds to update TheHive or Cortex by launching a fresh image or to migrate to a more (or less) powerful instance.</li> <li>Your data disks can be of any size while keeping the OS disk to its default size. </li> <li>Increasing (or decreasing) the size of a data disk on an existing instance is a lot easier than changing the OS disk size.</li> <li>You can restore your database from a previous state using disk snapshots.</li> </ul> <p>By default, the sample code expects the four persistent data disks to already exist (two for each instance):</p> <ul> <li>TheHive data volume - <code>lun0</code> - mounted at <code>/opt/thp_data</code> on TheHive instance</li> <li>TheHive Docker volume - <code>lun1</code> - mounted at <code>/var/lib/docker</code> on TheHive instance</li> <li>Cortex data volume - <code>lun0</code> - mounted at <code>/opt/thp_data</code> on Cortex instance</li> <li>Cortex Docker volume - <code>lun1</code> - mounted at <code>/var/lib/docker</code> on Cortex instance</li> </ul> <p>We created these persistent disks along with the vnet, so if you are using a custom or existing vnet, create them first and input their names in the associated variables. This way, the disks will not be deleted when the instances are terminated. This ensures that your data isn't accidentally lost.</p> <p>Note</p> <pre><code>Note that the disks are automatically managed if you use our sample Terraform code, you do not need to partition, format and mount the volumes, everything is taken care of for you!\n</code></pre>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/#connecting-to-your-instances-with-ssh","title":"Connecting to your instances with SSH","text":"<p>Since our TheHive and Cortex instance are located in a private subnet, we cannot directly SSH into them using their private IP addresses. If you have set up a bastion host configuration similarly to our reference architecture, you can seamlessly connect to private instances using the proxyjump functionality of the ssh client. The bastion host will be able to perform the hostname resolution with the private DNS zone we have set up in the VPC.</p> <p>The easiest way to do that is to create (or update) the <code>~/.ssh/config</code> file. Use the example below as a reference and replace the ip addresses and private key information.</p> <p>The default username for both the bastion host and TheHive instance is <code>azureuser</code>.</p> <pre><code>```\nHost bastion\n    HostName 1.2.3.4 (public ip)\n    User azureuser\n    Port 22\n    IdentityFile ~/.ssh/id_rsa_private_key_for_bastion\n\nHost thehive\n    HostName thehive.secops.cloud\n    User azureuser\n    Port 22\n    ProxyJump bastion\n    IdentityFile ~/.ssh/id_rsa_private_key_for_thehive\n\nHost cortex\n    HostName cortex.secops.cloud\n    User azureuser\n    Port 22\n    ProxyJump bastion\n    IdentityFile ~/.ssh/id_rsa_private_key_for_cortex\n```\n</code></pre> <p>Note</p> <p>We use the <code>secops.cloud</code> domain as an example but the best security practice is to use a domain name you own even for private DNS resolution in split-horizon.</p> <p>You will now be able to SSH into your instances directly using the bastion host as a proxy:</p> <pre><code>`$ ssh thehive`\n\nor\n\n`$ ssh cortex`\n</code></pre> <p>Note</p> <pre><code>Remember to whitelist your local public IP address in the bastion network security group.\n</code></pre>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/#accessing-the-nomad-ui","title":"Accessing the Nomad UI","text":"<p>It can sometimes be useful to access the Nomad UI to control the running application stack or to easily access the container console logs for each running task.</p> <p></p> <p>Since we are running Nomad in a standalone mode, without ACLs being implemented, we do not automatically expose the UI outside the instance itself (in other words, it will not be reachable from the Application Gateway using our sample Terraform code).</p> <p>Note</p> <p>Starting with image version 5.1.4, basic Nomad ACLs have been enabled. The default anonymous access provides some <code>read</code> permissions and full access is possible using the <code>admin</code> token. The admin token is automatically created the first time the Nomad ACLs are bootstraped and is stored under <code>/opt/thp_data/nomad/nomad.mgmt</code>. This token is stored on the persistent data volume and will be reused when updating (replacing) the instance while keeping the same persistent data volume.</p> <p>If you need occasional Nomad UI access, you can simply set a temporary SSH tunnel to the instance and forward a local port to the UI (which listens locally on port 4646 on each TheHive / Cortex instance).</p> <p>Note</p> <pre><code>if you wish to access the Nomad UI for multiple instances this way, you must use a different local port for each instance.\n</code></pre> <p>The following examples show how to launch these tunnels using an openSSH client in a vnet where the application instances are on a private subnet that can be reached only through a bastion host (jump host).</p> <p>access the Nomad UI for your TheHive instance - locally on port 46461</p> <pre><code>`$ ssh -L 46461:localhost:4646 azureuser@your_thehive_instance -J azureuser@your_bastion_instance`\n\nThe Nomad UI for your TheHive instance can now be opened in your browser at `http://localhost:46461/ui/`\n</code></pre> <p>Access the Nomad UI for your Cortex instance - locally on port 46462</p> <pre><code>`$ ssh -L 46462:localhost:4646 azureuser@your_cortex_instance -J azureuser@your_bastion_instance`\n\nThe Nomad UI for your Cortex instance can now be opened in your browser at `http://localhost:46462/ui/`\n</code></pre> <p>The same configuration can be achieved by updating your <code>SSH config</code> file to include a <code>LocalForward</code> statement for each remote instance:</p> <pre><code>```\nHost bastion\n    HostName 1.2.3.4 (public ip)\n    User azureuser\n    Port 22\n    IdentityFile ~/.ssh/id_rsa_private_key_for_bastion\n\nHost thehive\n    HostName thehive.secops.cloud\n    User azureuser\n    Port 22\n    ProxyJump bastion\n    IdentityFile ~/.ssh/id_rsa_private_key_for_thehive\n    LocalForward 46461 localhost:4646\n\nHost cortex\n    HostName cortex.secops.cloud\n    User azureuser\n    Port 22\n    ProxyJump bastion\n    IdentityFile ~/.ssh/id_rsa_private_key_for_cortex\n    LocalForward 46462 localhost:4646\n```\n</code></pre> <p>Note</p> <pre><code>Setting up an SSH tunnel between your computer and the remote instances requires that tunnelling and TCP forwarding be allowed on all hosts involved in the process. When you use the sample Terraform code in this repository, this is possible OOTB.\n</code></pre>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/#updating-thehive-and-cortex-over-time","title":"Updating TheHive and Cortex over time","text":"<p>Once your instances are deployed, you can easily update TheHive and / or Cortex. In a nutshell, the process consists in replacing the image versions in a variable file and restarting the application stack on each instance. This process it detailed at length in this dedicated documentation page.</p> <p>Terraform compatibility: v1.x</p>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/","title":"Upgrading your Azure instances","text":""},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#upgrading-your-azure-instances","title":"Upgrading your Azure instances","text":"<p>This documentation page describes how to upgrade the application stack on your instance (TheHive, Cortex and related services such as Cassandra and ElasticSearch).</p> <p>It does not cover operating system level updates such as Ubuntu patches, Docker updates, Nomad updates, etc. We recommend replacing your instance and using a fresh Azure Marketplace image to update all OS-level components to avoid any software version inconsistency / incompatibility.</p> <p>To avoid data loss / data corruption, you should ALWAYS backup or snapshot your persistent data volumes before performing any kind of update process.</p>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#tldr","title":"TL;DR;","text":"<p>All actions on the instances are to be performed as root</p>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#1-stop-the-running-nomad-job","title":"1. Stop the running Nomad job","text":"<pre><code>$ . /opt/strangebee/ops/scripts/ops-common.cfg &amp;&amp; stop_nomad_job thehive-job\n</code></pre>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#2-backup-your-nomad-job-specifications-and-var-files","title":"2. Backup your Nomad job specifications and var files","text":"<pre><code>$ /opt/strangebee/ops/scripts/ops-config-backup.sh -n nomad-jobs -f /opt/thp_data/nomad/jobs/\n</code></pre>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#3-backup-snapshot-your-instance-persistent-data-volume-using-the-azure-console-or-cli","title":"3. Backup / snapshot your instance persistent data volume (using the Azure console or CLI)","text":""},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#4-edit-the-nomad-job-var-file-matching-the-running-job-on-the-instance-and-update-the-image-versions-you-wish-to-use-from-now-on","title":"4. Edit the Nomad job var file matching the running job on the instance and update the image versions you wish to use from now on.","text":"<p>All job specifications and var files are located under <code>/opt/thp_data/nomad/jobs</code> ex: <code>$ vi /opt/thp_data/nomad/jobs/thehive-job.vars</code></p>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#5-schedule-the-nomad-job-dont-forget-to-set-the-matching-var-file","title":"5. Schedule the Nomad job (don't forget to set the matching var file)","text":"<pre><code>$ . /opt/strangebee/ops/scripts/ops-common.cfg &amp;&amp; run_nomad_job thehive-job\n</code></pre>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#context","title":"Context","text":"<p>The Azure Marketplace image for TheHive v5 can be used to deploy both TheHive v5 and / or Cortex v3. The image itself includes a hardened version of the official Ubuntu image from Canonical and a preconfigured Hashicorp Nomad engine. The image launch-time configuration you applied determined if TheHive or Cortex or both got installed on a given instance (as Nomad service jobs).</p> <p>To make things simpler for Azure admins with varying level of Hashicorp Nomad experience (including first-time Nomad users), we provide three different job specifications baked in the image to allow three instance deployment scenarios (instead of a more complex, dynamic one, that can adapt to all use-cases).</p> <p>The job specifications are named as follow:</p> <ul> <li>TheHive only --&gt; <code>thehive-job</code></li> <li>Cortex only --&gt; <code>cortex-job</code></li> <li>TheHive and Cortex on the same instance --&gt; <code>thehive-cortex-job</code></li> </ul> <p>Once your instances are deployed, you can check which profile is applied on a given instance with the following command (to be run on the instance):</p> <pre><code>$ nomad status\n</code></pre> <p>If you deployed TheHive only, the output is the following:</p> <pre><code>ID           Type     Priority  Status   Submit Date\nthehive-job  service  50        running  ***\n</code></pre> <p>Likewise, if you deployed Cortex only, the output is:</p> <pre><code>ID          Type     Priority  Status   Submit Date\ncortex-job  service  50        running  ***\n</code></pre> <p>And finally, if you deployed both TheHive and Cortex, the output is predictably:</p> <pre><code>ID                  Type     Priority  Status   Submit Date\nthehive-cortex-job  service  50        running  ***\n</code></pre>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#marketplace-image-storage-vs-persistent-instance-storage","title":"Marketplace image storage vs. persistent instance storage","text":"<p>Before we go any further, a quick note on image vs. persistent storage.</p> <p>As mentioned in the README, persistent data gets stored on a dedicated volume, not on the root filesystem. By default, your persistent data volume will be mounted at <code>/opt/thp_data</code>.</p> <pre><code>$ lsblk\n\nNAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nloop0     7:0    0 67.8M  1 loop /snap/lxd/22753\nloop1     7:1    0   62M  1 loop /snap/core20/1587\nloop2     7:2    0   47M  1 loop /snap/snapd/16292\nloop3     7:3    0   48M  1 loop /snap/snapd/17029\nloop4     7:4    0 63.2M  1 loop /snap/core20/1623\nsda       8:0    0   16G  0 disk\n\u2514\u2500sda1    8:1    0   16G  0 part /opt/thp_data\nsdb       8:16   0   32G  0 disk\n\u2514\u2500sdb1    8:17   0   32G  0 part /var/lib/docker\nsdc       8:32   0   30G  0 disk\n\u251c\u2500sdc1    8:33   0 29.9G  0 part /\n\u251c\u2500sdc14   8:46   0    4M  0 part\n\u2514\u2500sdc15   8:47   0  106M  0 part /boot/efi\nsr0      11:0    1  638K  0 rom\n</code></pre> <p>The Docker volume mounted at <code>/var/lib/docker</code> is also persistent but contains no persistent data and does not need to be backed-up. We use a dedicated Docker volume only for size adaptability since Docker images could require a lot of storage space (say if you were to use all available Cortex analyzer / responder images) or not that much if you only run TheHive on the instance.</p> <p>We provide templates for the Nomad job specifications within the image. The templates are located at: <code>/opt/strangebee/ops/templates/nomad/jobs</code>. All files under the <code>templates</code> folder are only meant as templates to be used somewhere else (most likely on your persistent storage). So, the only time window where it can be useful to modify the templates is at first instance launch, before initialising the instance (and thus before copying them to the persistent storage).</p> <p>Every time you replace an instance and use a fresh Azure Marketplace image, the root filesystem gets completely reset based on the new image, including the <code>templates</code> folder.</p> <p>The effective runtime configuration files used to launch Nomad jobs are all located on the persistent storage at: <code>/opt/thp_data/nomad/jobs</code>.</p> <p>When you replace an instance and use a fresh Azure Marketplace image, the persistent storage remains exactly as you left it when you stopped the previous instance.</p> <p>To proceed with TheHive / Cortex updates, we will only modify files on the persistent volume, all located under <code>/opt/thp_data/nomad/jobs</code>.</p>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#update-step-13-stop-the-apps-and-perform-some-backups","title":"Update step 1/3: stop the apps and perform some backups","text":""},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#stop-the-running-nomad-job","title":"Stop the running Nomad job","text":"<p>Before performing configuration and data backups, we will stop the running Nomad job on each instance (use the appropriate job name on each instance as identified earlier in the Context section). This will stop TheHive and / or Cortex along with all depending services such as Cassandra and ElasticSearch.</p> <p>Note</p> <p>All actions on the instances during the entire update process are to be performed as root**. Once connected to the instance, you can switch to the root context using the <code>sudo su</code> command.</p> <pre><code>$ . /opt/strangebee/ops/scripts/ops-common.cfg &amp;&amp; stop_nomad_job thehive-job\n\n[INFO] Stopping Nomad job: thehive-job\n[INFO] Fetching Nomad ACL management token\n</code></pre> <p>You can check the job is now stopped:</p> <pre><code>$ nomad status\n\nID           Type     Priority  Status          Submit Date\nthehive-job  service  50        dead (stopped)  ***\n</code></pre>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#backup-existing-nomad-job-specifications-and-variable-files","title":"Backup existing Nomad job specifications and variable files","text":"<p>We will now backup the existing Nomad job specifications and variable files before updating them. A script to do so is baked into the image, use it with the <code>-n</code> flag to set the backup name and <code>-f</code> flag to set the folder to backup. All backups will get stored on your persistent volume under <code>/opt/thp_data/backup</code>.</p> <pre><code>$ /opt/strangebee/ops/scripts/ops-config-backup.sh -n nomad-jobs -f /opt/thp_data/nomad/jobs/\n\n[INFO] Initial checks\n[INFO] Backup of /opt/thp_data/nomad/jobs/\n\n--- START of nomad-jobs config backup - Mon Oct 10 14:45:48 UTC 2022 ---\n\ntar: Removing leading `/' from member names\n/opt/thp_data/nomad/jobs/\n/opt/thp_data/nomad/jobs/thehive-job.nomad\n/opt/thp_data/nomad/jobs/thehive-db-init-job.nomad\n/opt/thp_data/nomad/jobs/thehive-cortex-job.nomad\n/opt/thp_data/nomad/jobs/thehive-db-init-job.vars\n/opt/thp_data/nomad/jobs/cortex-job.nomad\n/opt/thp_data/nomad/jobs/thehive-cortex-job.vars\n/opt/thp_data/nomad/jobs/cortex-job.vars\n/opt/thp_data/nomad/jobs/thehive-job.vars\n[INFO] Purging backups older than 7 days\n\n--- END of nomad-jobs config backup - Mon Oct 10 14:45:48 UTC 2022 ---\n</code></pre> <p>You can now check the resulting archive.</p> <pre><code>$ ll /opt/thp_data/backup\n\ntotal 16\ndrwxr-xr-x  2 root root 4096 Oct 10 14:45 ./\ndrwxr-xr-x 12 root root 4096 Oct 10 10:59 ../\n-rw-r-----  1 root root 3261 Oct 10 14:45 nomad-jobs.conf.backup.2022-10-10.tar.gz\n-rw-r-----  1 root root 3261 Oct 10 14:45 nomad-jobs.conf.latest.tar.gz\n</code></pre> <p>Note</p> <p>using the configuration backup script, the latest backup gets copied with the <code>latest</code> suffix. The latest backups never get deleted when older backups get purged.</p>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#backup-or-snapshot-your-persistent-data-volume","title":"Backup or snapshot your persistent data volume","text":"<p>Using the Azure console or CLI, snapshot your persistent data volume before performing any update:</p> <p></p> <p>Snapshot the persistent data volume for each instance you are updating.</p>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#update-step-23-update-the-image-versions-in-the-nomad-job-variable-file","title":"Update step 2/3: update the image versions in the Nomad job variable file","text":"<p>To make the update process easier, we have stored the image versions used in the Nomad job specifications in an external variables file. In most cases, it is not necessary to ever update the job specifications file unless you wish to fine tune some technical settings, such as the resources allocated to each task (CPU &amp; RAM).</p> <p>For example, we will update TheHive along with its dependencies on an instance running the <code>thehive-job</code> Nomad job. This job is described in two files:</p> <ul> <li>The job specifications file located at: <code>/opt/thp_data/nomad/jobs/thehive-job.nomad</code></li> <li>The job variables file located at: <code>/opt/thp_data/nomad/jobs/thehive-job.vars</code></li> </ul> <p>To update the image versions, we need to edit the variables file:</p> <ul> <li><code>$ vi /opt/thp_data/nomad/jobs/thehive-job.vars</code></li> </ul> /opt/thp_data/nomad/jobs/thehive-job.vars<pre><code>thehivecontext = \"/thehive\"\nimage_cassandra = \"cassandra:4.0.6\"\nimage_elasticsearch = \"elasticsearch:7.17.6\"\nimage_nginx = \"nginx:1.23.1\"\nimage_thehive = \"strangebee/thehive:5.0.16-1\"\n</code></pre> <p>Our instance was running TheHive v5.0.16 along with Cassandra v4.0.6 and ElasticSearch v7.17.6. </p> <p>Let's update the image versions (this is just an example, use the actual versions you wish to deploy):</p> <ul> <li><code>$ vi /opt/thp_data/nomad/jobs/thehive-job.vars</code></li> </ul> /opt/thp_data/nomad/jobs/thehive-job.vars<pre><code>thehivecontext = \"/thehive\"\nimage_cassandra = \"cassandra:4.1.0\"\nimage_elasticsearch = \"elasticsearch:7.17.9\"\nimage_nginx = \"nginx:1.24.0\"\nimage_thehive = \"strangebee/thehive:5.1.4-1\"\n</code></pre> <p>Tip</p> <p>We recommend never using the <code>:latest</code> tag and always using specific image versions to avoid problems or inconsistencies.</p>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#image-references","title":"Image references","text":"<p>You can find the available image versions for each component at the following links:</p> <ul> <li>TheHive images</li> <li>Cortex images</li> <li>Cassandra images</li> <li>ElasticSearch images</li> <li>Nginx images</li> </ul> <p>TheHive v5 is not yet compatible with ElasticSearch v8.x - only the 7.x versions are supported for the time being.</p> <p>You can find the changelog for TheHive and Cortex at the following links:</p> <ul> <li>TheHive changelog</li> <li>Cortex changelog</li> </ul>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-instances/docs/upgrade/#update-step-33-schedule-the-nomad-job","title":"Update step 3/3: Schedule the Nomad job","text":"<p>You can now schedule the Nomad job to start TheHive and / or Cortex. Do not forget to set the variable file we just modified when running the job, otherwise it will fail.</p> <pre><code>$ . /opt/strangebee/ops/scripts/ops-common.cfg &amp;&amp; run_nomad_job thehive-job\n\n[INFO] Scheduling Nomad job: thehive-job\n[INFO] Fetching Nomad ACL management token\n</code></pre> <p>To check the job is now running again:</p> <pre><code>$ nomad status\n\nID           Type     Priority  Status   Submit Date\nthehive-job  service  50        running  ***\n</code></pre> <p>To check the updated TheHive version:</p> <pre><code>$ . /opt/strangebee/ops/scripts/ops-common.cfg &amp;&amp; check_rp $(get_host_ip) thehive\n\n[INFO] Checking local reverse proxy on /thehive/api/status\n{\"versions\":{\"Scalligraph\":\"5.1.4-1\",\"TheHive\":\"5.1.4-1\",\"Play\":\"2.8.x\"},\"config\":{\"protectDownloadsWith\":\"malware\",\"authType\":[\"session\",\"local\",\"key\"],\"capabilities\":[\"changePassword\",\"setPassword\",\"authByKey\",\"mfa\"],\"ssoAutoLogin\":false,\"pollingDuration\":1000,\"freeTagDefaultColour\":\"#000000\"}}\n</code></pre> <p>To check the updated Cortex version:</p> <pre><code>$ . /opt/strangebee/ops/scripts/ops-common.cfg &amp;&amp; check_rp $(get_host_ip) cortex\n\n[INFO] Checking local reverse proxy on /cortex/api/status\n{\"versions\":{\"Cortex\":\"3.1.7-1\",\"Elastic4Play\":\"1.13.6\",\"Play\":\"2.8.16\",\"Elastic4s\":\"7.17.2\",\"ElasticSearch client\":\"7.17.1\"},\"config\":{\"protectDownloadsWith\":\"malware\",\"authType\":[\"key\",\"local\"],\"capabilities\":[\"authByKey\",\"changePassword\",\"setPassword\"],\"ssoAutoLogin\":false}}\n</code></pre>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-vnet/","title":"Creating a new SecOps Virtual Network","text":""},{"location":"resources/iaas/azure/infra-as-code/ug-secops-vnet/#creating-a-new-secops-virtual-network","title":"Creating a new SecOps Virtual Network","text":"<p>If you do not already have a virtual network (vnet) at hand to deploy TheHive and Cortex into, using our sample code will allow you to build a production-ready vnet very easily.</p>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-vnet/#files","title":"Files","text":"<p>Files</p> <p>Terraform files are available on Github: </p> <ul> <li>000-provider.tf</li> <li>010-vnet.tf</li> <li>030-certificate-identity.tf</li> <li>050-bastion.tf</li> <li>060-nva.tf</li> <li>070-appgw-securitygroup.tf</li> <li>075-public-securitygroup.tf</li> <li>080-nva-nic-securitygroup.tf</li> <li>085-routes.tf</li> <li>085-routes.tf</li> <li>090-instance-disks.tf</li> <li>200-appgw.tf</li> <li>output.tf</li> <li>variables.tf</li> <li>tarreform.tfvars</li> <li>files/nva-nat-cloud-config.yaml</li> </ul>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-vnet/#overview","title":"Overview","text":"<p>The reference architecture vnet consists of the following resources</p> <p></p> <p>The vnet will include:</p> <ul> <li>Two public subnets (one for the Application Gateway, the other for the bastion and network virtual appliance hosts)</li> <li>One backend private subnet to host our TheHive and Cortex instances</li> <li>An Application Gateway to securely handle the public-facing traffic to our TheHive and Cortex instances</li> <li>All required Network Security Groups</li> <li>Public DNS records for TheHive and Cortex to point to the Application Gateway </li> <li>A bastion host to SSH into our TheHive and Cortex instance hosted in the private subnet</li> <li>A Network Virtual Appliance to allow Internet access for our instances in the private subnet through a NAT gateway</li> <li>Four storage disks to store the persistent TheHive and Cortex data. These disks could be created alongside the instances but creating them with the vnet further limits the risk of accidental destruction and data loss when updating or replacing the instances.</li> </ul>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-vnet/#network-security-groups","title":"Network Security Groups","text":"<p>There are no default iptables rules implemented in the images for either TheHive or Cortex (no OS-based IP filtering). Since we built them to be replaceable at each application update, somewhat like containers, we recommend limiting OS customisations to benefit from the easy update process. For that reason, filtering should be based on network security groups only.</p> <p>Keep in mind that the applications are listening on http, not https. Even though the default network security groups allow incoming traffic on the http ports (TCP 9000 for TheHive, TCP 9001 for Cortex), be careful not to expose them on a public-facing network interface.</p> <p>While all required security groups depicted above are automatically deployed with Terraform, please note the following:</p> <p>For secops-public-subnet Network Security Group (NSG) and secops-backend-private-subnet NSGs:</p> <ul> <li>All default Azure rules allowing internal Virtual Network traffic, AzureLoadBalancer traffic and Internet traffic are blocked.</li> </ul> <p>For secops-appgw-subnet NSG: </p> <ul> <li>Default Azure inbound rules allowing internal Virtual Network traffic and AzureLoadBalancer traffic are blocked.</li> <li>Inbound traffic with source address GatewayManager or AzureLoadBalancer (destination port TCP 65200-65535) is allowed as recommended by Azure for the Application Gateway.</li> <li>Default Azure outbound rules are allowed.</li> </ul>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-vnet/#bastion-host","title":"Bastion host","text":"<p>We launch an instance to act as a bastion host (defaults to B2s - 2 vCPUs / 4 GiB RAM). Bastion host hardening is not performed automatically but you should definitely harden this host going forward if you will use it in a production context. We do however strictly limit access to and from this host.</p> <p>The bastion host will run the latest Ubuntu image from Canonical. The default sudoer user is azureuser.</p>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-vnet/#mynva-host","title":"myNVA host","text":"<p>We launch an instance to act as a network virtual appliance (NVA - defaults to DS2_v2 - 2 vCPUs / 7 GiB RAM / Accelerated networking supported). This instances makes it possible to NAT outgoing traffic from the private subnet. NVA host hardening is not performed automatically but you should definitely harden this host going forward if you will use it in a production context. We do however strictly limit access to and from this host.</p> <p>The NVA host will run the latest Ubuntu image from Canonical. The default sudoer user is azureuser.</p>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-vnet/#secops-vnet-prerequisites","title":"SecOps vnet prerequisites","text":"<p>While most vnet resources will be provisioned with Terraform, there are some exceptions that must be created beforehand:</p> <ul> <li>The Azure account and subscription must already exist, along with the target resource group</li> <li>The public DNS zone to register the Application Gateway</li> <li>The service principal to build the environment (e.g app-name) with Owner Azure Role on the subscription</li> <li>The keyvault and associated certificate for the https listener</li> </ul> <p>You must provide the information for these resources before creating the vnet (populate the Terraform variables with the associated values).</p>"},{"location":"resources/iaas/azure/infra-as-code/ug-secops-vnet/#providing-credentials-to-terraform","title":"Providing credentials to Terraform","text":"<p>Terraform requires Azure credentials in order to build the vnet and its associated resources. </p> <p>We highly recommend you never store your Azure credentials in the Terraform code or even in the same directory tree to avoid sharing / commiting the file accidentally. One way to provide the required credentials to Terraform is to create a file to set some environment variables and to source the file prior to running Terraform.</p> <p>Again, store this file in a secure location outside your Terraform project directory tree.</p> <p>Sample file:</p> <pre><code>export ARM_SUBSCRIPTION_ID=\"xxxx-xxxx-xxxx-xxxx-xxxx\"\nexport ARM_CLIENT_ID=\"xxxx-xxxx-xxxx-xxxx-xxxx\"\nexport ARM_CLIENT_SECRET=\"xxxx-xxxx-xxxx-xxxx-xxxx\"\nexport ARM_TENANT_ID=\"xxxx-xxxx-xxxx-xxxx-xxxx\"\nexport TF_VAR_thehive_key_vault_certificate_secret_id=\"xxxx-xxxx-xxxx-xxxx-xxxx\"\n</code></pre> <p>Where to find the associated values:</p> <ul> <li>ARM_SUBSCRIPTION_ID: The subscription ID (under Subscriptions --&gt; )</li> <li>ARM_CLIENT_ID: application ID (under Azure Active Directory --&gt; App registrations --&gt; All applications --&gt; app-name)</li> <li>ARM_CLIENT_SECRET: app-name Secret Key (under Azure Active Directory --&gt; App registrations --&gt; All applications --&gt; app-name --&gt; Certificates &amp; secrets)</li> <li>ARM_TENANT_ID: Active Directory tenant ID (under Azure Active Directory)</li> <li>TF_VAR_thehive_key_vault_certificate_secret_id: thehive certificate secret ID (under Keyvault --&gt; Certificates --&gt; thehive-certificate --&gt; CURRENT VERSION )</li> </ul> <p>Terraform compatibility: v1.x</p>"},{"location":"thehive/","title":"TheHive","text":""},{"location":"thehive/administration/accounts/","title":"Account Creation/Update Guide","text":""},{"location":"thehive/administration/accounts/#manage-accounts","title":"Manage Accounts","text":"<p>Accounts can be created or edited from several places in TheHive:</p> <ul> <li>As Administrator, in the Users view</li> <li>Ad Administrator in the detailed page of an Organisation</li> <li>As Org-admin, in the Organisation configuration page</li> </ul> <p>As Administrator of the platform, open the Users page.</p> <p></p>"},{"location":"thehive/administration/accounts/#types-of-accounts","title":"Types of accounts","text":"<p>Starting with TheHive 5.0, two types of accounts exist in the application:</p> Normal accounts <p>They are used for standard users, like analysts. This accounts can be used to open a session on the web UI, use all available authentication methods, and API keys if enabled.</p> Service accounts <p>They are recommended to be used by accounts in charge of automation in the application, like the ones used by to created Alerts. These accounts can only be used to authicate the application though the API, with an API key.</p>"},{"location":"thehive/administration/accounts/#create-an-account","title":"Create an account","text":"<p>Click the  button to add an account.</p> <p></p> <ol> <li>Choose the type of account, Normal or Service</li> <li>Fill the login name (formatted as an email address)</li> <li>Specify a name for the account</li> <li>Select the organisations and associated profile in each of them applied for this account. And Click on Set as default to define the default organisation for the account</li> </ol> <p>Then, click Confirm.</p>"},{"location":"thehive/administration/accounts/#update-an-account","title":"Update an account","text":"<p>In the list of accounts, click Preview to open accounts details view.</p> <p></p> <ol> <li>an Avatar can be added to the account. Click on the  to choose the file<sup>1</sup></li> <li>Specify the email address for the account. This is used to send notifications or reset password links to users</li> <li>Verify if the user has activated MFA (multi-factor authentication)</li> <li>Click on Set a new password to define a new password for the account</li> <li>Click Reset the password to send an email to the user. He will receive an email with a magic link to change his password. See  for more details.</li> <li>Update Roles and Organisations</li> <li>Delete the account</li> </ol> <ol> <li> <p>PNG or JPG files only.\u00a0\u21a9</p> </li> </ol>"},{"location":"thehive/administration/alert-status/","title":"Alert Status Management","text":""},{"location":"thehive/administration/alert-status/#alert-status","title":"Alert Status","text":"<p>Alert Status can be configured in the Administrators space: open Entities Management page, and select Alert status tab.</p>"},{"location":"thehive/administration/alert-status/#introduction","title":"Introduction","text":"<p>TheHive comes with a set of predifined statuses. Each status belongs to a Stage.</p> <p></p> <p>Stages are hardcoded; they cannot be updated or deleted, and not stage can be added in the platform. Status can be created, updated and deleted.</p>"},{"location":"thehive/administration/alert-status/#create-a-status","title":"Create a Status","text":"<p>Click on the  button to add a new status.</p> <p></p> <p>A status is defined by:</p> <ol> <li>a stage: choose the stage of the new status</li> <li>a value: choose a name for the new status</li> <li>a color: choose a color for users to quickly identify the status in the application</li> </ol>"},{"location":"thehive/administration/alert-status/#editdelete-a-status","title":"Edit/Delete a Status","text":"<p>The color can only be updated when updating a status.</p>"},{"location":"thehive/administration/analyzers-templates/","title":"Analyzers Templates Management","text":""},{"location":"thehive/administration/analyzers-templates/#analyzer-templates","title":"Analyzer templates","text":"<p>TheHive requires HTML templates to diplay Analyzers reports. </p>"},{"location":"thehive/administration/analyzers-templates/#install-or-update-templates-of-public-analyzers","title":"Install or update templates of public Analyzers","text":"<ol> <li> <p>As Administrator, go to Entities Management menu, and Analyzer templates</p> <p></p> </li> <li> <p>Download the ZIP archive, add it, and click on the Import button</p> <p></p> </li> </ol>"},{"location":"thehive/administration/analyzers-templates/#edit-templates","title":"Edit templates","text":"<p>As soon as a new Analyzer is enabled in Cortex, and is available for TheHive, a template line is added in this list.</p> <ol> <li> <p>Find the template to edit</p> <p></p> </li> <li> <p>Edit and save</p> <p></p> </li> </ol> <p>Running the associated analyzer should display results with the new template applied.</p>"},{"location":"thehive/administration/attack-patterns/","title":"Att&ck Patterns Management","text":""},{"location":"thehive/administration/attack-patterns/#attck-patterns","title":"Att&amp;ck patterns","text":"<p>The Att&amp;ck patterns configuration is available in the Administrators space: open Entities Management, then click on Att&amp;ck Patterns tab.</p>"},{"location":"thehive/administration/attack-patterns/#introduction","title":"Introduction","text":"<p>By default, TheHive comes with Enterprise ATT&amp;CK patterns from MITRE. This is installed during the installation process, and de catalog name Enterprise Attack is created with all of the related techniques.</p> <p></p>"},{"location":"thehive/administration/attack-patterns/#view-patterns","title":"View patterns","text":"<p>To view details of patterns included in a catalog, click on a catalog.  </p> <p>full details of each pattern can be reviewed by click on a technique ID (TXXXX) </p>"},{"location":"thehive/administration/attack-patterns/#add-or-update-catalogs","title":"Add or Update Catalogs","text":"<p>The catalogs are not updated automatically, neither the Enterprise catalog cominng during the installation process. Sso if you want to benefits from last versions of the framework, you have to update it.</p> <p></p> <p>To add a new catalog:</p> <ol> <li>Click on the Import MITRE ATT&amp;CK patterns</li> <li>Choose the patterns you want to install</li> <li>Add a catalog name if creating a new catalog, or select the name of an existing one to update it</li> <li>Drop le downloaded file</li> <li>Click the Import button</li> </ol> <p>This action can take some time.</p>"},{"location":"thehive/administration/branding/","title":"Branding Guide","text":""},{"location":"thehive/administration/branding/#branding","title":"Branding","text":"<p>Info</p> <p>This capability is only available with a license.</p> <p>You can change the brand name, login page logo, navigation page logo, and the favicon.</p> <p>To customize branding:</p> <ol> <li>On the Platform Management icon, select the Branding tab</li> <li>Make the necessary changes</li> <li>Click Confirm</li> </ol> <p></p>"},{"location":"thehive/administration/case-status/","title":"Case Status Management","text":""},{"location":"thehive/administration/case-status/#case-status","title":"Case Status","text":"<p>Case Status can be configured in the Administrators space: open Entities Management page, and select Case status tab.</p>"},{"location":"thehive/administration/case-status/#introduction","title":"Introduction","text":"<p>TheHive comes with a set of predifined statuses. Each status belongs to a Stage. </p> <p></p> <p>Stages are hardcoded; they cannot be updated or deleted, and not stage can be added in the platform. Status can be created, updated and deleted.</p>"},{"location":"thehive/administration/case-status/#create-a-status","title":"Create a Status","text":"<p>Click on the  button to add a new status.</p> <p></p> <p>A status is defined by: </p> <ol> <li>a stage: choose the stage of the new status</li> <li>a value: choose a name for the new status</li> <li>a color: choose a color for users to quickly identify the status in the application</li> </ol>"},{"location":"thehive/administration/case-status/#editdelete-a-status","title":"Edit/Delete a Status","text":"<p>The color can only be updated when updating a status.</p>"},{"location":"thehive/administration/cortex/","title":"Cortex Integration","text":""},{"location":"thehive/administration/cortex/#about-cortex","title":"About Cortex","text":"<p>By default, TheHive is not connected to any Cortex server.</p> <p>Connect TheHive to Cortex and get benefits from Analyzers to gather information and intelligence about Observables, and run active actions on your network or third party services with Responders.</p>"},{"location":"thehive/administration/cortex/#introduction","title":"Introduction","text":"<p>Info</p> <p>An account and an API key are required on a Cortex server to define a connection.</p> <ul> <li>Analyzers can be launched against Observables to get more details, contextual information, intelligence</li> <li>Responders can be launched against Case, Tasks, Observables, task Logs, and Alerts to run active actions during the investigation and incident response</li> </ul> <p>One or more Cortex instances can be connected to TheHive.</p>"},{"location":"thehive/administration/cortex/#manage-cortex-connections","title":"Manage Cortex connections","text":""},{"location":"thehive/administration/cortex/#add-a-new-cortex-server","title":"Add a new Cortex server","text":"<p>Specify:</p> <ul> <li>A name for this connection, for example: <code>Cortex</code><sup>1</sup></li> <li>The URL of Cortex server to connect with, for example: <code>https://cortex.mycompany.com</code> </li> <li>The API key of the dedicated Cortex account</li> <li>Proxy settings if required for TheHive to connect with Cortex</li> </ul>"},{"location":"thehive/administration/cortex/#advanced-settings","title":"Advanced settings","text":"<p>By default, all Analyzers and Responders made available by Cortex are available to ALL organisations in TheHive. Additionaly, 2 options are available:</p> <ul> <li>Make them available ONLY to a subset of existing Organisations in TheHive</li> <li>Make them unavailable to a subset of existing Organisations in TheHive</li> </ul>"},{"location":"thehive/administration/cortex/#delete-a-connection","title":"Delete a Connection","text":"<ol> <li> <p>If you have several connections, this is useful to give explicit names\u00a0\u21a9</p> </li> </ol>"},{"location":"thehive/administration/custom-fields/","title":"Custom Fields Management","text":""},{"location":"thehive/administration/custom-fields/#custom-fields","title":"Custom Fields","text":"<p>Custom Fields are used to enrich a Case or Alert with custom information used to give more context or be used to create statistics or dashboards.</p> <p>They are defined in the Administrator view of TheHive, and made available to all Organisations on the platform.</p> <p>This Custom Fields view is available in the Administrator space: ooen the Entities Management view, then the Custom Fields tab.</p>"},{"location":"thehive/administration/custom-fields/#create-a-custom-field","title":"Create a Custom Field","text":"<p>Click the  button to create a new Custom Field.</p> <p></p> <p>Create a new Custom Fields by filling following information: </p> <ol> <li>A name that will be displayed in Cases and Alerts</li> <li>A technical name. By default. this one is automatically set from the name but can be adjusted id needed. This name is used when using the Custom Field with the API</li> <li>Add a description to help analyst use this CF with Cases and Alerts</li> <li>Define a group name for this CF</li> <li>Define the type of the CF; several types are available - String, Boolean, Integer, Float, Date</li> <li>Fill some predefined values, if exist, or leave it blank if this is a free area</li> <li>Enable this option if the Custom Field is mandatory and has to be valued before closing a Case</li> </ol> <p>Then, click on Confirm custom field creation.</p>"},{"location":"thehive/administration/email-intake-connector/","title":"Email Intake Connector","text":""},{"location":"thehive/administration/email-intake-connector/#email-intake-connector","title":"Email Intake Connector","text":"<p>This documentation outlines the utilization of the Email Intake Connector for automatically generating alerts from a designated mailbox.</p> <p>The Email Intake Connector facilitates the connection of mailboxes used to receive cybersecurity alerts. It automatically transforms new emails into alerts within TheHive platform. Presently, the primary function supported is the creation of alerts regardless of the received email content.</p> <p></p>"},{"location":"thehive/administration/email-intake-connector/#configuration","title":"Configuration","text":""},{"location":"thehive/administration/email-intake-connector/#global-configuration","title":"Global Configuration","text":"<p>The only parameter that requires adjustment is the <code>refresh interval</code>.</p> <p></p> <p> </p>"},{"location":"thehive/administration/email-intake-connector/#adding-a-mailbox","title":"Adding a Mailbox","text":"<p>Configuration options are available for Microsoft 365 (OAuth2) and Google Workspace (OAuth2). If you use another email provider service, configuration through IMAP is necessary.</p> <p></p> <p> </p>"},{"location":"thehive/administration/email-intake-connector/#microsoft-configuration","title":"Microsoft Configuration","text":"<p>To configure Microsoft settings, the following values need completion:</p> <ul> <li><code>Email address</code> of the mailbox</li> <li><code>clientId</code></li> <li><code>tenantId</code></li> <li><code>secret</code></li> </ul> <p>Refer to Microsoft documentation for instructions on obtaining these values.</p> <p></p> <p> </p>"},{"location":"thehive/administration/email-intake-connector/#google-workspace-configuration","title":"Google Workspace Configuration","text":"<p>For Google Workspace accounts, an authorization request is essential during the configuration process. Complete the following values:</p> <ul> <li><code>Email address</code> of the mailbox</li> <li><code>clientId</code></li> <li><code>secret</code></li> </ul> <p>Refer to Google documentation for instructions on obtaining these values.</p> <p></p> <p> </p>"},{"location":"thehive/administration/email-intake-connector/#imap-configuration","title":"IMAP Configuration","text":"<p>For IMAP configuration, you'll need to input the following information:</p> <ul> <li>Host: <code>host</code></li> <li>Port: <code>port</code> (default: 993)</li> </ul> <p>Additionally, provide your mailbox credentials. We recommend enabling SSL Check Certificate Authority.</p> <p> </p>"},{"location":"thehive/administration/email-intake-connector/#settings","title":"Settings","text":"<p>After testing your mailbox configuration, select the organization to connect, determining where alerts will be created. Define the mailbox folder to monitor (typically INBOX). Finally, specify the action to take on incoming emails: <code>archive</code>, <code>mark as read</code>, or <code>no action</code>.</p>"},{"location":"thehive/administration/email-intake-connector/#_1","title":"Email Intake Connector","text":""},{"location":"thehive/administration/email-intake-connector/#generated-alerts-and-observables","title":"Generated Alerts and Observables","text":"<p>Following configuration, alerts and observables are generated in the selected organization.</p>"},{"location":"thehive/administration/email-intake-connector/#alerts","title":"Alerts","text":"<p>Each alert will contain the following details:</p> <ul> <li><code>alert.type</code>: \"email-intake\"</li> <li><code>alert.source</code>: The configuration name is formatted as \"Google Workspace @strangebee.com\" =&gt; \"googleworkspace-strangebee\"</li> <li><code>alert.sourceRef</code>: \"{message-id}\" or \"{lastUidValidity}.{uidEmail}\" if the message-id is inaccessible</li> <li><code>alert.title</code>: The email subject or \"no subject\"</li> <li><code>alert.severity</code>: \"low\"</li> <li><code>alert.description</code>: The content of the email</li> <li><code>alert.lastSyncDate</code>: The date the email was received</li> <li><code>alert.tlp</code>: \"amber\"</li> <li><code>alert.pap</code>: \"amber\"</li> <li><code>alert.follow</code>: false</li> <li><code>alert.tags</code>: [\"email-intake\", {source}, {Provider Name}, {Inbox Folder Name}]</li> <li><code>alert.status</code>: \"new\"</li> <li><code>alert.externalLink</code>: [Link to External Source]</li> <li><code>alert.summary</code>: [Summary of Alert]</li> <li><code>alert.customFields</code>: [Custom Fields]</li> </ul> <p> </p>"},{"location":"thehive/administration/email-intake-connector/#observables","title":"Observables","text":"<p>The email itself is included as a .eml file, along with its sender and all attached files, which are added to the alert as observables, with the following parameters:</p> <ul> <li><code>observable.message</code>: The pre-formatted message</li> <li><code>observable.tlp</code>: {alert.tlp}</li> <li><code>observable.pap</code>: {alert.pap}</li> <li><code>observable.ioc</code>: false</li> <li><code>observable.sighted</code>: false</li> <li><code>observable.sightedAt</code>: [Timestamp]</li> <li><code>observable.ignoreSimilarity</code>: false</li> <li><code>observable.dataType</code>: \"file\" if it's an attachment; otherwise, \"mail\" for the .eml file</li> <li><code>observable.tags</code>: {alert.tags}</li> <li><code>observable.attachmentId</code>: {attachment.id}</li> </ul> <p> </p>"},{"location":"thehive/administration/first-start/","title":"TheHive - First Start","text":""},{"location":"thehive/administration/first-start/#first-start-of-thehive","title":"First Start of TheHive","text":""},{"location":"thehive/administration/first-start/#initial-login","title":"Initial Login","text":"<p>After following the installation guides and ensuring TheHive is up and running, open your web browser, navigate to <code>http://IP_ADDRESS:9000</code>, and log in using the default account credentials:</p> Login <code>admin</code> Password <code>secret</code> <p></p>"},{"location":"thehive/administration/first-start/#install-license","title":"Install License","text":"<p>To unlock all features and quotas, log in to your account on StrangeBee's customer portal, and follow this guide to setup the licence.</p> <p>Tip</p> <p>This action is particularly required if you are setting up TheHive as a cluster: </p> <ol> <li>When starting TheHive service, start only one node</li> <li>setup the license by connecting to the started node</li> <li>start others TheHive nodes</li> </ol>"},{"location":"thehive/administration/first-start/#change-admin-password","title":"Change <code>Admin</code> password","text":"<p>It is crucial to change the default admin password immediately after your initial login to ensure the security of your TheHive instance. The default credentials are publicly known and leaving them unchanged poses a significant security risk. Unauthorized users can easily gain access to your system, potentially compromising sensitive data and operations.</p>"},{"location":"thehive/administration/first-start/#1-go-to-user-settings","title":"1. Go to User Settings","text":""},{"location":"thehive/administration/first-start/#2-change-your-password","title":"2. Change Your Password","text":""},{"location":"thehive/administration/first-start/#3-confirm-changes","title":"3. Confirm Changes","text":"<p>Ensure you confirm the changes for them to take effect.</p>"},{"location":"thehive/administration/first-start/#configuration","title":"Configuration","text":"<p>The Administrator's space is where all platform configurations are managed.</p> <ul> <li> <p>Integrate TheHive with SMTP servers, authentication directory servers, Cortex, and MISP servers: Go to the Platform Management page</p> <p></p> </li> <li> <p>Create Organisations</p> <p></p> </li> <li> <p>Create Users</p> <p></p> </li> <li> <p>Customize Application Behavior for Users in the Entity Management page</p> <p></p> </li> </ul>"},{"location":"thehive/administration/ldap-server/","title":"Synchronise Users and Organisation with LDAP","text":""},{"location":"thehive/administration/ldap-server/#synchronise-users-and-organisation-with-ldap","title":"Synchronise Users and Organisation with LDAP","text":""},{"location":"thehive/administration/ldap-server/#user-synchronisation","title":"User synchronisation","text":"<p>Users can be provisionned and deprovisionned automatically based on the content of a directory. Users data is synchronised periodically. New users in LDAP are created in TheHive, removed users are disabled.</p> <p>The organisation membership and the profile of an user are set using LDAP groups. The configuration contain the mapping of LDAP groups with organisation/profile.</p> <p></p>"},{"location":"thehive/administration/license/","title":"License Management","text":""},{"location":"thehive/administration/license/#install-or-update-the-license","title":"Install or Update the License","text":"<p>By default TheHive includes the community edition license.</p> <p>To unlock capabilities<sup>1</sup> and quotas<sup>2</sup>, a license is required.   Contact StrangeBee. WHen you buy the license from StrangeBee, StrangeBee will create an account for you on the customer portal that will allow you to activate the license.</p>"},{"location":"thehive/administration/license/#activate-or-update-the-license","title":"Activate or update the license","text":"<ol> <li> <p>On the Platform Management page, in the License tab, click the Update the current license button</p> <p></p> <p>Set a License key window opens. You can see the challenge in the window.</p> </li> <li> <p>Click Copy this challenge</p> <p>You will see the challenge copied message. After copying the challenge, go to your account on the StrangeBee customer portal and activate the license using this challenge and the customer portal will give you an activation license key.</p> </li> <li> <p>Enter the activation key in the License field</p> </li> <li> <p>Click the Activate the license key button</p> <p>This will activate the license and update your instance with all the features included with that license.</p> <p></p> <p>The license is defined by the following capabilities:</p> <ol> <li>It defines how many users you can create in your platform</li> <li>The license is based on the number of users and the number of organizations</li> <li>It has a validation and an expiration date</li> <li>It allows unlimited number of ReadOnly users and Service accounts. Service accounts are those who do not have access to the TheHive interface but can use an API key to call all the APIs</li> </ol> </li> </ol> <ol> <li> <p>Capabilities included in the license: clustering, branding, Active directory, SAML and OAUTH2 authentication, Case timelines.\u00a0\u21a9</p> </li> <li> <p>Quotas concern Organisations and Users.\u00a0\u21a9</p> </li> </ol>"},{"location":"thehive/administration/misp/","title":"MISP Integration","text":""},{"location":"thehive/administration/misp/#misp","title":"MISP","text":""},{"location":"thehive/administration/misp/#introduction","title":"Introduction","text":"<p>Info</p> <p>An account and an API key are required on a MISP server to define a connection.</p> <ul> <li>One or more MISP instances can be connected to TheHive.</li> <li>For each one:</li> <li>MISP events can be imported as Alerts in TheHive. A set of filter can refine the imported events</li> <li>Observables flagged as IOCs in a Case can be exported in a new event in MISP</li> </ul>"},{"location":"thehive/administration/misp/#manage-misp-connections","title":"Manage MISP connections","text":""},{"location":"thehive/administration/misp/#add-a-new-misp-server","title":"Add a new MISP server","text":"<p>Specify:</p> <ul> <li>A name for this connection, for example: <code>misp</code><sup>1</sup></li> <li>The URL of MISP server to connect with, for example: <code>https://misp.mycompany.com</code> </li> <li>The API key of the dedicated MISP account</li> <li>The purpose of this connection: Import only, Export Only, or both, Import &amp; Export</li> <li>Proxy settings if required for TheHive to connect with MISP</li> </ul>"},{"location":"thehive/administration/misp/#advanced-settings","title":"Advanced settings","text":"<p>By default, ALL Organisations in TheHive benefit from this connection. Additionaly, 2 options are available:</p> <ul> <li>Make this connection available ONLY to a subset of existing Organisations in TheHive</li> <li>Make this connection unavailable to a subset of existing Organisations in TheHive</li> </ul> <p>Additional options let you:</p> <ul> <li>Define tags that will be appended to Alerts when importing MISP events</li> <li>When exporting IOCs in MISP, also export tags from the Case in the new MISP event</li> <li>When exporting IOCs in MISP, also export tags from the observables</li> </ul>"},{"location":"thehive/administration/misp/#filters","title":"Filters","text":"<p>When importing MISP events as TheHive Alerts, several options are available:</p> <ul> <li>Define the maximum age of a MISP event allowed to be imported</li> <li>Specify a list of organisations owner of the MISP events allowed to be imported</li> <li>Specify a list of organisations owner of the MISP events that are not allowed to be imported</li> <li>Define a limit for the number of attributes (~observables) included in the MISP event to import it</li> <li>Specify a list of tags that should exist in the MISP event to import it</li> <li>Specify a list of tags that should exist in the MISP event to ignore it</li> </ul>"},{"location":"thehive/administration/misp/#delete-a-connection","title":"Delete a Connection","text":"<ol> <li> <p>If you have several connections, this is useful to give explicit names\u00a0\u21a9</p> </li> </ol>"},{"location":"thehive/administration/observables-types/","title":"Observable Types Management","text":""},{"location":"thehive/administration/observables-types/#observable-types","title":"Observable types","text":"<p>Observables types define the available dataTypes of Observables that can be used in the application. TheHive comes with the predefined set of types, and this list can be enriched with custom datatypes.</p> <p>Observable types are configured in the Administrators space: open Entities Management and select the Observable types tab.</p>"},{"location":"thehive/administration/observables-types/#create-a-new-observable-type","title":"Create a new Observable Type","text":"<p>Click the  button to create a new Observable Type.</p> <p></p> <ol> <li>Specify a name for this new type</li> <li>Define if this new Observable Type is defined by a file attachment or not. If yes, the data entered by analysts is a file; if not, this is a text area.</li> </ol> <p>Then click on Confirm observable type creation.</p>"},{"location":"thehive/administration/organisation-links/","title":"Linking Organizations","text":""},{"location":"thehive/administration/organisation-links/#link-organisations","title":"Link Organisations","text":"<p>By default, organisations are not linked each other: each one does not know about the others on the instance.</p>"},{"location":"thehive/administration/organisation-links/#manage-links-with-other-organisation","title":"Manage links with other organisation","text":"<p>Start managing links by opening the detailed view of an Organisation:</p> <ul> <li>Open the Linked Organisation tab</li> <li>Click on the button named Manage linked Organisations</li> </ul> <p></p> <p>For each other organisations, select:</p> <ol> <li>if you want the current Organisation be linked with it</li> <li>The types of link that should be created</li> </ol> <p></p> <p>3 types of links are available: </p> <ul> <li><code>default</code>: Cases created by the current Organisation will not be shared with the other one</li> <li><code>supervised</code>: Cases created by the current Organisation will be automatically shared with the other one, with the profile Analyst</li> <li><code>notify</code>: Cases created by the current Organisation will be automatically shared with the other one, with the profile Read-only</li> </ul>"},{"location":"thehive/administration/organisations/","title":"Creating an Organization","text":""},{"location":"thehive/administration/organisations/#manage-organisations","title":"Manage Organisations","text":"<p>As Administrator, go to the Organisations page.</p> <p></p>"},{"location":"thehive/administration/organisations/#add-an-organisation","title":"Add an Organisation","text":"<p>Click on the  button and edit the required fields in the drawer: </p> <ul> <li>A placeholder exists and a logo of the Organisation can be added</li> <li>Name: Name of the new Organisation </li> <li>Description: Description for the new Organisation</li> <li>Task sharing rule: default sharing rule for Tasks that will be applied when a Case will be shared with another Organisation</li> <li>Observables sharing rule: default sharing rule for Observables that will be applied when a Case will be shared with another Organisation</li> </ul> <p>Click Confirm to create the organisation. </p> <p></p>"},{"location":"thehive/administration/organisations/#edit-your-organisation","title":"Edit your Organisation","text":"<p>Once your organisation is created:</p> <ul> <li>users can be added</li> <li>links with other existing Organisations created for the purpose of sharing Cases.</li> </ul>"},{"location":"thehive/administration/organisations/#lock-an-organisation","title":"Lock an organisation","text":"<p>An existing Organisation can be locked so that all users belonging to this one cannot log into it.</p> <p></p>"},{"location":"thehive/administration/profiles/","title":"Profiles Management","text":""},{"location":"thehive/administration/profiles/#profiles","title":"Profiles","text":"<p>Profiles are available in the first tab of Entities Management page.</p>"},{"location":"thehive/administration/profiles/#introduction","title":"Introduction","text":"<p>TheHive comes with a set of predefined profiles for Administrators and Organsations ; this set can be enriched with custom profiles you can create depending on your needs.</p> <p></p> <p>A Valid license is required to update profiles</p>"},{"location":"thehive/administration/profiles/#about-permissions","title":"About Permissions","text":"<p>Each profile is defined by a set of permissions. There are two profile types:</p> <ul> <li>Administration, used by users in the Admin organisation, to manage the platform</li> <li>Organization used in business organisations</li> </ul> <p>Permissions are named <code>manageEntity</code> with <code>Entity</code>, an entity in the application. For example: <code>manageCase</code>. A permission <code>manageEntity</code> means having rights to write, update,delete an entity. </p>"},{"location":"thehive/administration/profiles/#manage-profiles","title":"Manage Profiles","text":"<p>Except the admin profile, all profiles can be customised and deleted.</p>"},{"location":"thehive/administration/profiles/#add-a-profile","title":"Add a profile","text":"<ul> <li>Add a new profile by clicking the  button on the Entities Management page, in the Profiles tab</li> <li>Then select the type of profile to create, and associated permissions  </li> </ul> <p>Adding a Profile window opens.</p> <ol> <li>Enter a Name for the new profile.</li> <li>Choose Profile type.</li> <li>Select the Permissions for that profile type.</li> <li>Click the Confirm profile creation button.</li> </ol> <p></p> <p></p>"},{"location":"thehive/administration/profiles/#edit-or-delete-profile","title":"Edit or delete Profile","text":""},{"location":"thehive/administration/smtp/","title":"SMTP Configuration","text":""},{"location":"thehive/administration/smtp/#smtp","title":"SMTP","text":"<p>TheHive can connect to a SMTP server to send email notifications, and allow users to define or change their password when forgotten.</p>"},{"location":"thehive/administration/smtp/#configure-smtp","title":"Configure SMTP","text":"<p>On the Platform Management page, select the SMTP tab.</p> <p></p>"},{"location":"thehive/administration/smtp/#configure-server-settings","title":"Configure Server Settings","text":"<p>Define: </p> <ol> <li>Server name or IP address</li> <li>Port</li> <li>The email address you want to use as the Sender in Send emails from</li> </ol>"},{"location":"thehive/administration/smtp/#configure-security-and-authentication-settings","title":"Configure Security and Authentication settings","text":"<p>Define additional security parameters, if required:</p> <ol> <li>Select the right protocol from the list in Connection security</li> <li>Enter the Username amd Password if required</li> </ol>"},{"location":"thehive/administration/smtp/#confirm-changes","title":"Confirm changes","text":""},{"location":"thehive/administration/smtp/#create-user-account-and-send-a-notification","title":"Create user account and send a notification","text":""},{"location":"thehive/administration/smtp/#i-forgot-my-password","title":"I forgot my password","text":"<p>Learn how to request a magic link with the I forgot my password button.</p>"},{"location":"thehive/administration/taxonomies/","title":"Taxonomies Management","text":""},{"location":"thehive/administration/taxonomies/#taxonomies","title":"Taxonomies","text":"<p>Taxonomies are used to defined structured tags in TheHive. Taxonomies can be configured in the Administrators space: open Entities Manamgement and select Taxonomies tab.</p> <p>By default, MISP taxonomies are imported.</p> <p></p>"},{"location":"thehive/administration/taxonomies/#view-a-taxonomie","title":"View a taxonomie","text":"<p>To review the list of available tags in one specific taxonomie, click on the desired name; this will open a drawer with the list of tags.</p> <p></p>"},{"location":"thehive/administration/taxonomies/#activate-or-delete-a-taxonomie","title":"Activate or delete a taxonomie","text":"<p>By default no taxonomie is activated; so none can be used in Cases or Alerts. To use a set of tags in Cases and Alerts, the related taxonomie should be activated.</p> <p></p>"},{"location":"thehive/administration/taxonomies/#update-taxonomies","title":"Update Taxonomies","text":"<p>TheHive comes with the version of MISP taxonomies available at the moment of the installation. Updating TheHive does not update and add the last available version. So if you want to get the latest version released by MISP people you have to update it manually.</p> <p></p> <ol> <li>Click on the Import Taxonomies button</li> <li>Download the last archive available here: https://github.com/MISP/misp-taxonomies/archive/main.zip</li> <li>Drag&amp;Drop the downloaded file and click on the Import button</li> </ol>"},{"location":"thehive/administration/taxonomies/#custom-taxonomies","title":"Custom Taxonomies","text":"<p>You can add you own taxonomies by following the JSON schema specified by MISP.</p>"},{"location":"thehive/administration/authentication/ad/","title":"Active Directory (AD) Authentication","text":""},{"location":"thehive/administration/authentication/ad/#active-directory","title":"Active Directory","text":"<p>A license is required to configure Active Directory authentication</p> <p></p> <p>Following information are required to configure AD authentication:</p> <ul> <li>The addresses of the domain controllers</li> <li>The Windows Domain Name</li> <li>The DNS domain name</li> <li>if you are using SSL or not</li> </ul>"},{"location":"thehive/administration/authentication/general/","title":"General Settings","text":""},{"location":"thehive/administration/authentication/general/#general-settings","title":"General Settings","text":"<p>Info</p> <ul> <li>Privileges required: administrator</li> <li>Organisation: admin</li> <li>Location: <ul> <li>Menu: Plateform Management</li> <li>Tab: Authentication</li> </ul> </li> </ul> <p></p>"},{"location":"thehive/administration/authentication/general/#session-settings","title":"Session settings","text":"<ul> <li>Duration of user inactivity before session expiration: time before logging out a user if inactive</li> <li>Warning message display time, before session expiration: duration of displaying a warning message before logging out </li> </ul> <p>Several options are available:</p> <ol> <li>Enable Basic Authentication: Authenticates HTTP requests using the login and password provided</li> <li>Enable API Key authentication: Authenticates HTTP requests using an API key provided</li> <li>Enable HTTP Header Authentication: Authenticates HTTP requests using a HTTP header containing the user login</li> <li>Enable Multifactor authentication: Multi-Factor Authentication is enabled by default. This means users can configure their MFA through their User Settings page</li> <li>Default user domain: By default, users log in with an email address for example: user@domain.com. When set up, users are allowed to log in without the domain (for example user).</li> </ol>"},{"location":"thehive/administration/authentication/general/#manage-authentication-providers","title":"Manage Authentication Providers","text":"<p>Several options exist to authenticate users: </p> <ul> <li>local accounts: manage a local user database where you can configure the password policy</li> <li>Using LDAP directory: configure TheHive to use a LDAP server </li> <li>Using Active directory: configure TheHive to use a LDAP server</li> <li>SAML: Use single sign-on through on or more SAML providers to authenticate users</li> <li>Oauth2: Use single sign-on through external Oauth2 server to authenticate users</li> </ul> <p>Use several providers</p> <p> TheHive can use several providers to authenticate users, use the arrows to change the priority order (for example: try the Oauth2 authentication, then the local database).</p>"},{"location":"thehive/administration/authentication/ldap/","title":"LDAP Authentication","text":""},{"location":"thehive/administration/authentication/ldap/#setting-ldap-authentication","title":"Setting LDAP Authentication","text":"<p>To setup LDAP authentication:</p> <ol> <li>Click on Directory Authentication</li> <li>use the switch to enable directory </li> <li>then choose LDAP in the menu ; the list of required parameters appears   </li> <li>Confirm and save your changes</li> <li>move the Directories Authentication line to be the first provider to use in the list of authentication providers</li> </ol> <p>Using SSL with LDAP</p> <p>To setup a custom Certificate Authority in TheHive, please refer to this guide.</p>"},{"location":"thehive/administration/authentication/ldap/#authenticating-with-ldap","title":"Authenticating with LDAP","text":"<p>Users able to authenticate should already have an account created in TheHive local database.</p>"},{"location":"thehive/administration/authentication/local/","title":"Local Authentication","text":""},{"location":"thehive/administration/authentication/local/#local-account","title":"Local account","text":"<p>This is the default behaviour of TheHive. The applications store usernames and password in a local database.</p>"},{"location":"thehive/administration/authentication/local/#configuration","title":"Configuration","text":"<p>By default, no policy is activated for local accounts. Nevertheless, a password policy and blocking settings can be adjusted:</p> <ul> <li>A number of failed attempts to authenticate before a user be temporay blocked</li> <li>The related duration before unblock a user</li> </ul>"},{"location":"thehive/administration/authentication/local/#password-policy","title":"Password policy","text":"<p>This options is disabled by default. When enabled following items can be configured: </p> <ul> <li>Minimum lenght of passwords</li> <li>Minimum number of lower cases characters included in the password</li> <li>Minimum number of upper cases characters included in the password</li> <li>Minimum number of digits included in the password</li> <li>Minimum number of special characters included in the password</li> <li>Allowing or disollowing the usage of usernames as passwords</li> </ul>"},{"location":"thehive/administration/authentication/oauth2/","title":"OAUTH2 Authentication","text":""},{"location":"thehive/administration/authentication/oauth2/#oauth2-openid-connect","title":"OAuth2 / OpenID-Connect","text":""},{"location":"thehive/administration/authentication/oauth2/#configuration","title":"Configuration","text":"<p>Authenticate the user using an external OAuth2 authenticator server. It accepts the following configuration parameters:</p> Parameter Description Client ID client ID in the OAuth2 server Client secret client secret in the OAuth2 server TheHive redirect URL the url of TheHive AOuth2 page ( <code>https://xxx/api/ssoLogin</code>) Authorization URL the url of the OAuth2 server Token URL the token url of the OAuth2 server User information URL the url to get user information in OAuth2 server List of scope list of scope Field that contains the id of the user in user info the field that contains the id of the user in user info"},{"location":"thehive/administration/authentication/oauth2/#examples","title":"Examples","text":"KeycloakOktaGithubMicrosoft 365Google Parameter Value Client ID <code>CLIENT_ID</code> Client secret <code>CLIENT_SECRET</code> TheHive redirect URL https://THEHIVE_URL/api/ssoLogin Authorization URL http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/auth Token URL http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/token User information URL http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/userinfo List of scope <code>[\"openid\", \"email\"]</code> Field that contains the id of the user in user info \"email\" Parameter Value Client ID <code>CLIENT_ID</code> Client secret <code>CLIENT_SECRET</code> TheHive redirect URL http://THEHIVE_URL/api/ssoLogin Authorization URL https://OKTA/oauth2/v1/authorize Token URL http://OKTA/oauth2/v1/token User information URL http://OKTA/oauth2/v1/userinfo List of scope <code>[\"openid\", \"email\"]</code> Field that contains the id of the user in user info \"email\" Parameter Value Client ID <code>CLIENT_ID</code> Client secret <code>CLIENT_SECRET</code> TheHive redirect URL https://THEHIVE_URL/api/ssoLogin Authorization URL https://github.com/login/oauth/authorize Token URL https://github.com/login/oauth/access_token User information URL https://api.github.com/user List of scope <code>[\"user\"]</code> Field that contains the id of the user in user info \"email\" <p>Note</p> <ul> <li><code>CLIENT_ID</code> and <code>CLIENT_SECRET</code> are created in the OAuth Apps section at https://github.com/settings/developers.</li> <li>this configuration requires that users set the Public email in their Public Profile on https://github.com/settings/profile.</li> </ul> Parameter Value Client ID <code>CLIENT_ID</code> Client secret <code>CLIENT_SECRET</code> TheHive redirect URL https://THEHIVE_URL/api/ssoLogin Authorization URL https://login.microsoftonline.com/TENANT/oauth2/v2.0/authorize Token URL https://login.microsoftonline.com/TENANT/oauth2/v2.0/token User information URL https://graph.microsoft.com/v1.0/me List of scope <code>[\"User.Read\"]</code> Field that contains the id of the user in user info \"mail\" <p>Note</p> <p>To create <code>CLIENT_ID</code>, <code>CLIENT_SECRET</code> and <code>TENANT</code>, register a new app at https://aad.portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/RegisteredApps.</p> Parameter Value Client ID <code>CLIENT_ID</code> Client secret <code>CLIENT_SECRET</code> TheHive redirect URL https://THEHIVE_URL/api/ssoLogin Authorization URL https://accounts.google.com/o/oauth2/v2/auth Token URL https://oauth2.googleapis.com/token User information URL https://openidconnect.googleapis.com/v1/userinfo List of scope <code>[\"email\", \"profile\", \"openid\"]</code> Field that contains the id of the user in user info \"email\" <p>Note</p> <ul> <li><code>CLIENT_ID</code> and <code>CLIENT_SECRET</code> are created in the <code>_APIs &amp; Services_ &gt; _Credentials_</code> section of the GCP Console</li> <li>Instructions on how to create Oauth2 credentials at https://support.google.com/cloud/answer/6158849</li> <li>For the latest reference for Google auth URLs please check Google's .well-known/openid-configuration</li> </ul>"},{"location":"thehive/administration/authentication/oauth2/#user-autocreation","title":"User autocreation","text":"<p>To allow users to login without previously creating them, you can enable autocreation, and specify few options:</p> <ul> <li>Field that contains the name of the user in user info</li> <li>Field that contains the name of the organisation in user info</li> <li>Default organisation applied to new users</li> <li>Default profile applied to new users</li> </ul> <p></p>"},{"location":"thehive/administration/authentication/saml/","title":"SAML Authentication","text":""},{"location":"thehive/administration/authentication/saml/#saml","title":"SAML","text":"<p>TheHive supports SAMLv2.0 authentication providers.</p>"},{"location":"thehive/administration/authentication/saml/#configuration","title":"Configuration","text":"<p>An SAML authentication provider accepts the following configuration parameters:</p> Parameter Description Name Give a name to the provider in TheHive Identity Provider metadata type Select how TheHive gathers configuration information: <code>xml</code> or `url Identity Provider metadata value Give the URL or the XML content with service information Login Name Indicate the name of the custom attribute containing the user login information Maximum authentication life time This value must match the value from the identity provider <p></p> <p></p> Configuration using XML content"},{"location":"thehive/administration/authentication/saml/#using-several-providers","title":"Using several providers","text":"<p>Several providers can be configured. In this case, when a user tries to log in, TheHive queries each provider in the order listed. Queries stops when one replies with the authorization to log in.</p> <p></p> Using several providers"},{"location":"thehive/configuration/akka/","title":"Akka Configuration","text":""},{"location":"thehive/configuration/akka/#akka","title":"Akka","text":"<p>Akka is a powerful toolkit designed for building highly concurrent, distributed, and resilient message-driven applications in Java and Scala. </p> <p>https://akka.io/</p> <p>Akka plays a crucial role in enabling multiple nodes of TheHive to communicate with each other seamlessly, thereby enhancing the overall user experience. </p>"},{"location":"thehive/configuration/akka/#basic-configuration","title":"Basic Configuration","text":"<p>For a reliable cluster setup, it's essential to have a minimum of three nodes for TheHive application. Each node should be configured with Akka as outlined below: </p> <pre><code>## Akka server\nakka {\n  cluster.enable = on\n  actor {\n    provider = cluster\n  }\n  remote.artery {\n    canonical {\n      hostname = \"&lt;HOSTNAME OR IP_ADDRESS&gt;\"\n      port = 2551\n    }\n  }\n# seed node list contains at least one active node\n  cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ]\n}\n</code></pre> <p>In this configuration:</p> <ul> <li><code>remote.artery.hostname</code> should be set to the hostname or IP address of the node.</li> <li><code>cluster.seed-nodes</code> should contain the same list of Akka nodes, ensuring consistency across all nodes.</li> </ul> <p>Configuration of a Cluster with 3 Nodes</p> Node 1Node 2Node 3 <p>Akka configuration for Node 1:</p> <pre><code>akka {\n    cluster.enable = on\n    actor {\n      provider = cluster\n    }\n    remote.artery {\n      canonical {\n          hostname = \"10.1.2.1\"\n          port = 2551\n      }\n    }\n    # seed node list contains at least one active node\n    cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ]\n}\n</code></pre> <p>Akka configuration for Node 2:</p> <pre><code>akka {\n    cluster.enable = on\n    actor {\n    provider = cluster\n    }\n    remote.artery {\n    canonical {\n        hostname = \"10.1.2.2\"\n        port = 2551\n    }\n    }\n    # seed node list contains at least one active node\n    cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ]\n}\n</code></pre> <p>Akka configuration for Node 3:</p> <pre><code>akka {\n    cluster.enable = on\n    actor {\n    provider = cluster\n    }\n    remote.artery {\n    canonical {\n        hostname = \"10.1.2.3\"\n        port = 2551\n    }\n    }\n    # seed node list contains at least one active node\n    cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ]\n}\n</code></pre>"},{"location":"thehive/configuration/akka/#ssltls-support","title":"SSL/TLS Support","text":"<p>Akka offers robust support for SSL/TLS encryption, guaranteeing secure communication between nodes. Below, you'll find a standard configuration to enable SSL/TLS support:</p> <pre><code>## Akka server\nakka {\n  cluster.enable = on\n  actor {\n    provider = cluster\n  }\n  remote.artery {\n    transport = tls-tcp\n    canonical {\n      hostname = \"&lt;HOSTNAME OR IP_ADDRESS&gt;\"\n      port = 2551\n    }\n\n    ssl.config-ssl-engine {\n      key-store = \"&lt;PATH TO KEYSTORE&gt;\"\n      trust-store = \"&lt;PATH TO TRUSTSTORE&gt;\"\n\n      key-store-password = \"chamgeme\"\n      key-password = \"chamgeme\"\n      trust-store-password = \"chamgeme\"\n\n      protocol = \"TLSv1.2\"\n    }\n  }\n# seed node list contains at least one active node\n  cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ]\n}\n</code></pre> <p>Note</p> <p>Note that <code>akka.remote.artery.transport</code> has changed and <code>akka.ssl.config-ssl-engine</code> needs to be configured.</p> <p>For more details, refer to: Akka Remoting with Artery - Remote Security</p> <p>Certificate Considerations</p> <p>Ensure you use your internal PKI (Public Key Infrastructure) or keytool commands to generate certificates.</p> <p>For detailed instructions, see: Using keytool for Certificate Generation</p> <p>Your server certificates should include the following KeyUsage and ExtendedkeyUsage extensions for proper functioning:</p> <ul> <li>KeyUsage extensions<ul> <li><code>nonRepudiation</code></li> <li><code>dataEncipherment</code></li> <li><code>digitalSignature</code></li> <li><code>keyEncipherment</code></li> </ul> </li> <li>ExtendedkeyUsage extensions<ul> <li><code>serverAuth</code></li> <li><code>clientAuth</code></li> </ul> </li> </ul> <p>Akka Configuration with SSL/TLS for Node 1</p> <pre><code>## Akka server\nakka {\n  cluster.enable = on\n  actor {\n    provider = cluster\n  }\n  remote.artery {\n    transport = tls-tcp\n    canonical {\n      hostname = \"10.1.2.1\"\n      port = 2551\n    }\n\n    ssl.config-ssl-engine {\n      key-store = \"/etc/thehive/application.conf.d/certs/10.1.2.1.jks\"\n      trust-store = \"/etc/thehive/application.conf.d/certs/internal_ca.jks\"\n\n      key-store-password = \"chamgeme\"\n      key-password = \"chamgeme\"\n      trust-store-password = \"chamgeme\"\n\n      protocol = \"TLSv1.2\"\n    }\n  }\n# seed node list contains at least one active node\n  cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ]\n}\n</code></pre> <p>Ensure to apply the same principle for configuring other nodes, and remember to restart all services afterward.</p> <p> </p>"},{"location":"thehive/configuration/connectors/","title":"Connectors Configuration","text":""},{"location":"thehive/configuration/connectors/#thehive-connectors","title":"TheHive Connectors","text":"<p>TheHive comes with built-in connectors to seamlessly integrate with Cortex and MISP. </p> <p>By default, these connectors are enabled in the /etc/thehive/application.conf configuration file. However, if you are not using one or both of these integrations, you can easily disable them by commenting out the relevant line(s):</p> /etc/thehive/application.conf<pre><code>[..]\nscalligraph.modules += org.thp.thehive.connector.cortex.CortexModule\n# scalligraph.modules += org.thp.thehive.connector.misp.MispModule  # (1)\n</code></pre> <ol> <li>The line for the MISP connector has been commented out, indicating that it is disabled.</li> </ol> <p>Updating this configuration file requires restarting TheHive service for the changes to take effect.</p> <p> </p>"},{"location":"thehive/configuration/database/","title":"Database & Index Configuration","text":""},{"location":"thehive/configuration/database/#database-and-index-configuration","title":"Database and Index Configuration","text":"<p>TheHive utilizes Cassandra and Elasticsearch databases for data management and indexing purposes. Below outlines the configuration options available:</p>"},{"location":"thehive/configuration/database/#basic-configuation","title":"Basic Configuation","text":"<p>A typical database configuration for TheHive is structured as follows:</p> <pre><code>## Database configuration\ndb {\n  provider = janusgraph\n  janusgraph {\n    ## Storage configuration\n    storage {\n      backend = cql\n      hostname = [\"IP_ADDRESS\"]\n      cql {\n        cluster-name = thp\n        keyspace = thehive\n      }\n    }\n    ## Index configuration\n    index.search {\n      backend = elasticsearch\n      hostname = [\"127.0.0.1\"]\n      index-name = thehive\n    }\n  }\n}\n</code></pre> <p>This configuration specifies the following components:</p> <p>Database Provider: </p> <ul> <li>The database provider is set to JanusGraph, a distributed graph database.</li> </ul> <p>Storage Configuration:</p> <ul> <li> <p>Backend: Cassandra is specified as the backend storage system.</p> </li> <li> <p>Hostname: The IP address of the Cassandra cluster is provided.</p> </li> <li> <p>Cluster Name: The name of the Cassandra cluster is set to 'thp'.</p> </li> <li> <p>Keyspace: The keyspace within Cassandra where TheHive data will be stored is named 'thehive'.</p> </li> </ul> <p>Index Configuration:</p> <ul> <li> <p>Backend: Elasticsearch is designated as the backend for indexing.</p> </li> <li> <p>Hostname: The IP address of the Elasticsearch instance is set to '127.0.0.1'.</p> </li> <li> <p>Index Name: The index name within Elasticsearch for TheHive is specified as 'thehive'.</p> </li> </ul>"},{"location":"thehive/configuration/database/#list-of-parameters","title":"List of Parameters","text":"Parameter Type Description <code>provider</code> string Provider name. Default: <code>janusgraph.</code> <code>storage</code> dict Storage configuration. <code>storage.backend</code> string Storage type. Can be <code>cql</code> or <code>berkeleyje</code>. <code>storage.hostname</code> list of string List of IP addresses or hostnames when using the <code>cql</code> backend. <code>storage.directory</code> string Local path for data when using the berkeleyje backend. <code>storage.username</code> string Account username with the cql backend if Cassandra authentication is configured. <code>storage.password</code> string Account password with the cql backend if Cassandra authentication is configured. <code>storage.port</code> integer Port number with the cql backend (9042 by default). Change this if using an alternate port or a dedicated port number when using SSL with Cassandra. <code>storage.cql</code> dict Configuration for the cql backend if used. <code>storage.cql.cluster-name</code> string Name of the cluster used in the configuration of Apache Cassandra. <code>storage.cql.keyspace</code> string Keyspace name used to store TheHive data in Apache Cassandra. <code>storage.cql.ssl.enabled</code> boolean false by default. Set it to true if SSL is used with Cassandra. <code>storage.cql.ssl.truststore.location</code> string Path to the truststore. Specify it when using SSL with Cassandra. <code>storage.cql.ssl.password</code> string Password to access the truststore. <code>storage.cql.ssl.client-authentication-enabled</code> boolean Enables the use of a client key to authenticate with Cassandra. <code>storage.cql.ssl.keystore.location</code> string Path to the keystore. Specify it when using SSL and client authentication with Cassandra. <code>storage.cql.ssl.keystore.keypassword</code> string Password to access the key in the keystore. <code>storage.cql.ssl.truststore.storepassword</code> string Password to access the keystore. <code>index.search</code> dict Configuration for indexes. <code>index.search.backend</code> string Index engine. Default: elasticsearch <code>index.search.directory</code> string Path to the folder where indexes should be stored when using the elasticsearch engine. <code>index.search.hostname</code> list of string List of IP addresses or hostnames when using the elasticsearch engine. <code>index.search.index-name</code> string Name of index when using the elasticsearch engine. <code>index.search.elasticsearch.http.auth.type: basic</code> string basic is the only possible value. <code>index.search.elasticsearch.http.auth.basic.username</code> string Username account on Elasticsearch. <code>index.search.elasticsearch.http.auth.basic.password</code> string Password of the account on Elasticsearch. <code>index.search.elasticsearch.ssl.enabled</code> boolean Enable SSL (true/false). <code>index.search.elasticsearch.ssl.truststore.location</code> string Location of the truststore. <code>index.search.elasticsearch.ssl.truststore.password</code> string Password of the truststore. <code>index.search.elasticsearch.ssl.keystore.location</code> string Location of the keystore for client authentication. <code>index.search.elasticsearch.ssl.keystore.storepassword</code> string Password of the keystore. <code>index.search.elasticsearch.ssl.keystore.keypassword</code> string Password of the client certificate. <code>index.search.elasticsearch.ssl.disable-hostname-verification</code> boolean Disable SSL verification (true/false). <code>index.search.elasticsearch.ssl.allow-self-signed-certificates</code> boolean Allow self-signed certificates (true/false). <p>The initial start, or first start after configuring indexes, might take some time if the database contains a large amount of data. This time is due to the index creation process.</p> <p>For more detailed information on configuring Elasticsearch connection, refer to the official JanusGraph documentation.</p>"},{"location":"thehive/configuration/database/#use-cases","title":"Use Cases","text":"<p>The database and index engine configurations can vary depending on the use case and target setup.</p> Standalone Server with Cassandra &amp; Elasticsearch <p>To set up TheHive on a standalone server with Cassandra and Elasticsearch:</p> <ol> <li>Install a Cassandra server locally.</li> <li>Install Elasticsearch.</li> <li> <p>Configure TheHive with the following settings:</p> <p><code>hocon   ## Database Configuration   db {     provider = janusgraph     janusgraph {       ## Storage configuration       storage {         backend = cql         hostname = [\"127.0.0.1\"]         ## Cassandra authentication (if configured)         username = \"thehive_account\"         password = \"cassandra_password\"         cql {           cluster-name = thp           keyspace = thehive         }       }       ## Index configuration       index.search {         backend = elasticsearch         hostname = [\"127.0.0.1\"]         index-name = thehive       }     }</code></p> </li> </ol> Cluster with Cassandra &amp; Elasticsearch <p>To deploy TheHive on a cluster with Cassandra and Elasticsearch:</p> <ol> <li>Install a cluster of Cassandra servers.</li> <li>Set up access to an Elasticsearch server.</li> <li> <p>Configure TheHive with the following settings:</p> <pre><code>## Database Configuration\ndb {\n  provider = janusgraph\n  janusgraph {\n    ## Storage configuration\n    storage {\n      backend = cql\n      hostname = [\"10.1.2.1\", \"10.1.2.2\", \"10.1.2.3\"]\n      ## Cassandra authentication (if configured)\n      username = \"thehive_account\"\n      password = \"cassandra_password\"\n      cql {\n        cluster-name = thp\n        keyspace = thehive\n      }\n    }\n    ## Index configuration\n    index {\n      search {\n        backend  = elasticsearch\n        hostname  = [\"10.1.2.5\"]\n        index-name  = thehive\n        elasticsearch {\n          http {\n            auth {\n              type = basic\n              basic {\n                username = httpuser\n                password = httppassword\n              }\n            }\n          }\n          ssl {\n            enabled = true\n            truststore {\n              location = /path/to/your/truststore.jks\n              password = truststorepwd\n            }\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> </li> </ol> <p>Warning</p> <p>In this configuration, all TheHive nodes should have the same configuration.</p> <p>Elasticsearch configuration should use the default value for <code>script.allowed_types</code>, or contain the following configuration line = </p> <pre><code>script.allowed_types: inline,stored\n</code></pre> <p> </p>"},{"location":"thehive/configuration/file-storage/","title":"File Storage Configuration","text":""},{"location":"thehive/configuration/file-storage/#file-storage-configuration","title":"File Storage Configuration","text":"<p>TheHive offers flexible configurations for file storage, accommodating both local and distributed filesystem setups.</p> Local or NFS <p>To configure TheHive to use a local or NFS (Network File System) storage:</p> <pre><code>1. Create a dedicated folder named files, ensuring it is owned by the user and group thehive:thehive.\n\n    ```bash\n    mkdir /opt/thp/thehive/files\n    chown thehive:thehive /opt/thp/thehive/files\n    ```\n\n2. Configure TheHive accordingly in the YAML configuration file:\n\n    ```yaml\n    ## Attachment storage configuration\n    storage {\n      ## Local filesystem\n      provider: localfs\n      localfs {\n        location: /opt/thp/thehive/files\n      }\n    }\n    ```\n</code></pre> Min.IO <p>For Min.IO integration with TheHive, follow these steps:</p> <pre><code>1. Install a Min.IO cluster. Follow [**these step-by-step**](../installation/3-node-cluster.md#minio-setup) instructions.\n2. Configure each node of TheHive accordingly:\n\n    ```yaml title=\"/etc/thehive/application.conf with TheHive 5.0.x\"\n    ## Attachment storage configuration\n    storage {\n      provider: s3\n      s3 {\n        bucket = \"thehive\"\n        readTimeout = 1 minute\n        writeTimeout = 1 minute\n        chunkSize = 1 MB\n        endpoint = \"http://&lt;IP_MINIO_1&gt;:9100\"\n        accessKey = \"thehive\"\n        secretKey = \"password\"\n        region = \"us-east-1\"\n      }\n    }\n    alpakka.s3.access-style = path\n    ```\n\n    ```yaml title=\"/etc/thehive/application.conf with TheHive &gt; 5.0\"\n    storage {\n      provider: s3\n      s3 {\n        bucket = \"thehive\"\n        readTimeout = 1 minute\n        writeTimeout = 1 minute\n        chunkSize = 1 MB\n        endpoint = \"http://&lt;IP_MINIO_1&gt;:9100\"\n        accessKey = \"thehive\"\n        aws.credentials.provider = \"static\"\n        aws.credentials.secret-access-key = \"password\"\n        access-style = path\n        aws.region.provider = \"static\"\n        aws.region.default-region = \"us-east-1\"\n      }\n    }\n    ```\n\n  Note:\n\n  - The configuration remains backward compatible.\n\n  - The default region is us-east-1, but it's optional if not specified in the MinIO configuration.\n</code></pre> <p> </p>"},{"location":"thehive/configuration/gdpr/","title":"General Data Protection Regulation (GDPR)","text":""},{"location":"thehive/configuration/gdpr/#gdpr-compliance-in-thehive-5x","title":"GDPR Compliance in TheHive 5.x","text":"<p>TheHive includes a specialized feature for managing data retention policies within the database. By default, this feature is not enabled and must be configured based on your organization's GDPR compliance needs.</p> <p>Info</p> <p>This feature is exclusively available with TheHive 5.x Platinum plan.</p>"},{"location":"thehive/configuration/gdpr/#strategies","title":"Strategies","text":"<p>There are two primary strategies available:</p> <ul> <li>Replace Sensitive Values with <code>&lt;redacted&gt;</code></li> <li>Delete Data</li> </ul> <p> </p>"},{"location":"thehive/configuration/gdpr/#replace-sensitive-values-with-redacted","title":"Replace Sensitive Values with <code>&lt;redacted&gt;</code>","text":"<p>Under this strategy, sensitive information is redacted from specific fields within TheHive, including:</p> <p> </p> <p>For cases, the following fields are redacted:</p> <ul> <li><code>summary</code> and <code>message</code> of the case</li> <li><code>message</code> of comments</li> <li><code>message</code> in task logs</li> <li><code>message</code> of observables, for datatypes selected and filled in the <code>gdpr.dataTypesToDelete</code> configuration property</li> <li><code>content</code> of pages</li> <li><code>description</code> of procedures in TTPs</li> </ul> <p> </p> <p>For alerts, the following fields are redacted:</p> <ul> <li><code>message</code> of the alert</li> <li><code>message</code> of observables (<code>gdpr.dataTypesToDelete</code> configuration property)</li> <li><code>description</code> of procedures (TTP)</li> </ul> <p> </p> <p>For audits:</p> <ul> <li>the field <code>details</code> is redacted</li> </ul> <p> </p>"},{"location":"thehive/configuration/gdpr/#delete-data","title":"Delete Data","text":"<p>Selecting the <code>delete</code> strategy will permanently remove the following components:</p> <ul> <li>Cases and associated components (tasks, task logs, procedures, comments, pages, custom events in timelines, custom field values, and observables)</li> <li>Alerts and associated components (procedures, comments, custom field values, and observables)</li> <li>Audits</li> </ul>"},{"location":"thehive/configuration/gdpr/#retention","title":"Retention","text":"<p>The <code>retentionPeriod</code> parameter specifies the minimum age of data subject to deletion or redaction. The GDPR process is applied to data older than this specified period, calculated based on the last update date (or creation date if never updated). The format for <code>retentionPeriod</code> supports various time units:</p> <ul> <li>day:         <code>d</code>, <code>day</code></li> <li>hour:        <code>h</code>, <code>hr</code>, <code>hour</code></li> <li>minute:      <code>m</code>, <code>min</code>, <code>minute</code></li> <li>second:      <code>s</code>, <code>sec</code>, <code>second</code></li> <li>millisecond: <code>ms</code>, <code>milli</code>, <code>millisecond</code></li> </ul> <p>For example, 365 days denotes a retention period of 1 year.</p>"},{"location":"thehive/configuration/gdpr/#configuration","title":"Configuration","text":"<p>To enable GDPR compliance, follow these steps:</p> <ol> <li>Update the configuration file /etc/thehive/application.conf with the following settings:</li> </ol> <pre><code>gdpr {\n    enabled = true\n\n    ## Format http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html\n    ## Every Sunday at 02:30\n\n    schedule = \"0 30 2 ? * SUN\"\n\n    ## Possible GDPR strategies:\n    ##   delete: remove the documents\n    ##   redact: replace sensitive values by \"&lt;redacted&gt;\" (cf. dataTypesToDelete)\n\n    strategy = \"delete\"\n\n    ## if the strategy is \"redacted\", the observable with dataType in \n    ## \"dataTypesToDelete\" will be removed\n    ## for other observables, message will be \"&lt;redacted&gt;\", not the data\n    ## Uncomment following line to select datatypes\n\n    # dataTypesToDelete = [] ## [\"ip\", \"domain\"]\n\n    ## only documents older than the \"retentionPeriod\" will be processed\n\n    retentionPeriod = 730 days # 2 years\n\n    ## Advanced parameters (should not be modified)\n\n    jobTimeout = 24 days ## maximum time the job is executed\n    batchSizeCase = 5     ## how many cases is processed per transaction\n    batchSizeAlert = 10   ## how many cases is processed per transaction\n    batchSizeAudit = 100  ## how many cases is processed per transaction\n}\n</code></pre> <ol> <li> <p>Save the changes to the configuration file.</p> </li> <li> <p>Restart TheHive application to apply the new settings.</p> </li> </ol> <p>By following these steps, you can effectively implement GDPR-compliant data retention policies within TheHive 5.x. Adjust the configuration parameters as per your organization's specific requirements and compliance standards.</p> <p> </p>"},{"location":"thehive/configuration/logs/","title":"Logs Configuration","text":""},{"location":"thehive/configuration/logs/#logs-configuration","title":"Logs Configuration","text":"<p>TheHive utilizes logback for logging purposes, allowing users to monitor the running process effectively. The logging settings are managed through the configuration file located at <code>/etc/thehive/logback.xml</code>. Changes made to this file require a service reload to take effect.</p> <p>By default, logs are stored in <code>/var/log/thehive/</code>, with the most recent log file named application.log, while older files are compressed and stored as <code>application.%i.log.zip</code>.</p>"},{"location":"thehive/configuration/logs/#adjusting-log-levels","title":"Adjusting Log Levels","text":"<p>Logback offers various log levels to control the amount of information logged. To increase or decrease the log level: </p> <p>Update the root level to DEBUG or TRACE to log more information:</p> logback.xml<pre><code>    &lt;!-- ... --&gt;\n    &lt;root level=\"DEBUG\"&gt;\n        &lt;!-- ... --&gt;\n    &lt;/root&gt;\n</code></pre> <p>Alternatively, adjust the log level for specific loggers:</p> logback.xml<pre><code>    &lt;logger name=\"org.thp\" level=\"DEBUG\"/&gt;\n</code></pre> <p>You have the option to select from the following additional log levels: WARN, ERROR, or OFF.</p>"},{"location":"thehive/configuration/logs/#docker-logs-configuration","title":"Docker Logs Configuration","text":"<p>In a Docker container, TheHive logs to stdout and <code>/var/log/thehive/application.log</code> by default. To customize this behavior, mount your own logback file to <code>/etc/thehive/logback.xml</code>.</p>"},{"location":"thehive/configuration/logs/#debugging-logback-configuration","title":"Debugging Logback Configuration","text":"<p>To troubleshoot logback configuration issues, set the debug flag to true in logback.xml:</p> logback.xml<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;configuration debug=\"true\"&gt;\n</code></pre> <p>This will log the logback configuration in the console during application startup.</p>"},{"location":"thehive/configuration/logs/#creating-an-access-log","title":"Creating an Access Log","text":"<p>To redirect certain logs from the application, such as access logs, modify the logback configuration. Here's an example configuration for redirecting access logs to a file named access.log using a rolling file strategy:</p> <p>To add this into your configuration, duplicate the definitions of <code>appender</code> and <code>logger</code> as demonstrated below.</p> logback.xml<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;configuration debug=\"false\"&gt;\n\n    &lt;!-- ... other appenders and settings --&gt;\n\n    &lt;appender name=\"ACCESSFILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt;\n        &lt;file&gt;/var/log/thehive/access.log&lt;/file&gt;\n        &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.FixedWindowRollingPolicy\"&gt;\n            &lt;fileNamePattern&gt;/var/log/thehive/access.%i.log.zip&lt;/fileNamePattern&gt;\n            &lt;minIndex&gt;1&lt;/minIndex&gt;\n            &lt;maxIndex&gt;10&lt;/maxIndex&gt;\n        &lt;/rollingPolicy&gt;\n        &lt;triggeringPolicy class=\"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy\"&gt;\n            &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt;\n        &lt;/triggeringPolicy&gt;\n\n        &lt;encoder&gt;\n            &lt;pattern&gt;%date [%level] from %logger [%traceID] %message%n%xException&lt;/pattern&gt;\n        &lt;/encoder&gt;\n    &lt;/appender&gt;\n\n    &lt;appender name=\"ASYNCACCESSFILE\" class=\"ch.qos.logback.classic.AsyncAppender\"&gt;\n        &lt;appender-ref ref=\"ACCESSFILE\"/&gt;\n    &lt;/appender&gt;\n\n    &lt;logger name=\"org.thp.scalligraph.AccessLogFilter\"&gt;\n        &lt;appender-ref ref=\"ASYNCACCESSFILE\" /&gt;\n    &lt;/logger&gt;\n    &lt;logger name=\"org.thp.scalligraph.controllers.Entrypoint\"&gt;\n        &lt;appender-ref ref=\"ASYNCACCESSFILE\" /&gt;\n    &lt;/logger&gt;\n\n    &lt;root level=\"INFO\"&gt;\n        &lt;!-- other appender-refs ... --&gt;\n    &lt;/root&gt;\n\n&lt;/configuration&gt;\n</code></pre>"},{"location":"thehive/configuration/logs/#sending-logs-to-syslog","title":"Sending Logs to Syslog","text":"<p>To send logs to syslog, add a <code>SyslogAppender</code> to the logback configuration:</p> logback.xml<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;configuration debug=\"false\"&gt;\n\n    &lt;!-- ... other appenders and settings --&gt;\n\n    &lt;appender name=\"SYSLOG\" class=\"ch.qos.logback.classic.net.SyslogAppender\"&gt;\n        &lt;syslogHost&gt;remote_host&lt;/syslogHost&gt;\n        &lt;facility&gt;AUTH&lt;/facility&gt;\n        &lt;suffixPattern&gt;[%thread] %logger %msg&lt;/suffixPattern&gt;\n    &lt;/appender&gt;\n\n    &lt;root level=\"INFO\"&gt;\n        &lt;appender-ref ref=\"SYSLOG\" /&gt;\n        &lt;!-- other appender-refs ... --&gt;\n    &lt;/root&gt;\n</code></pre> <p>Refer to the official documentation for more details.</p> <p>Limitations: The official syslog appender only supports sending logs via UDP to a server and does not support TCP and TLS.</p> <p> </p>"},{"location":"thehive/configuration/proxy/","title":"Proxy Configuration","text":""},{"location":"thehive/configuration/proxy/#proxy-settings","title":"Proxy Settings","text":""},{"location":"thehive/configuration/proxy/#global-application-proxy","title":"Global Application Proxy","text":"<p>Proxy settings can be configured for the application. By default, the JVM's proxy settings are used, but it's possible to define specific configurations for individual HTTP clients.</p> <p> </p>"},{"location":"thehive/configuration/proxy/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Description <code>wsConfig.proxy.host</code> string Hostname of the proxy server. <code>wsConfig.proxy.port</code> integer Port of the proxy server. <code>wsConfig.proxy.protocol</code> string Protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified. <code>wsConfig.proxy.user</code> string Username for proxy server credentials. <code>wsConfig.proxy.password</code> string Password for proxy server credentials. <code>wsConfig.proxy.ntlmDomain</code> string NTLM domain for proxy authentication. <code>wsConfig.proxy.encoding</code> string Charset for the realm. <code>wsConfig.proxy.nonProxyHosts</code> list List of hosts for which the proxy should not be used."},{"location":"thehive/configuration/secret/","title":"Secrets Configuration","text":""},{"location":"thehive/configuration/secret/#secret-configuration-file-secretconf","title":"Secret Configuration File - <code>secret.conf</code>","text":"<p>The secret.conf file contains a secret key that is utilized to define cookies responsible for managing user sessions within TheHive application. It is crucial that one instance of TheHive uses a unique secret key to ensure session security and integrity.</p> <ul> <li> <p>Single Instance Deployment: For a single instance of TheHive, ensure that the <code>secret.conf</code> file contains a unique secret key.</p> </li> <li> <p>Clustered Deployment: In the scenario where multiple nodes of TheHive are clustered together, all nodes should have a <code>secret.conf</code> file with the same secret key.</p> </li> </ul> <p>Example</p> <pre><code>## Example secret key configuration\nplay.http.secret.key=\"dgngu325mbnbc39cxas4l5kb24503836y2vsvsg465989fbsvop9d09ds6df6\"\n</code></pre> <p> </p> <p>Warning</p> <p>Do not copy the key provided above. Instead, generate and use your own unique secret key for enhanced security and confidentiality.</p> <p> </p>"},{"location":"thehive/configuration/service/","title":"Service Configuration","text":""},{"location":"thehive/configuration/service/#service-configuration","title":"Service Configuration","text":""},{"location":"thehive/configuration/service/#listen-address-port","title":"Listen Address &amp; Port","text":"<p>By default, the application listens on <code>all network interfaces (0.0.0.0)</code> on <code>port 9000</code>. You can customize the listen address and port by editing the <code>application.conf</code> file as follows:</p> <pre><code>http.address=127.0.0.1\nhttp.port=9000\n</code></pre> <p>Specify the desired IP address and port based on your requirements.</p>"},{"location":"thehive/configuration/service/#setting-a-context-path","title":"Setting a Context Path","text":"<p>If you are using a reverse proxy and need to define a specific context path (e.g., <code>/thehive</code>), you must update TheHive's configuration accordingly:</p> <p>Example</p> <pre><code>play.http.context: \"/thehive\"\n</code></pre> <p>This sets the base context path for accessing TheHive via the reverse proxy.</p>"},{"location":"thehive/configuration/service/#configuring-streams-for-reverse-proxies","title":"Configuring Streams for Reverse Proxies","text":"<p>When using a reverse proxy like Nginx, you may encounter <code>504 Gateway Time-Out</code> errors related to long polling. Adjust the <code>stream.longPolling.refresh</code> setting to resolve this issue:</p> <p>Example</p> <pre><code>stream.longPolling.refresh: 45 seconds\n</code></pre> <p>This setting controls the refresh interval for long-polling requests.</p>"},{"location":"thehive/configuration/service/#using-web-proxy","title":"Using Web Proxy","text":"<p>If you are employing an NGINX reverse proxy in front of TheHive, note that NGINX does not differentiate between text data and file uploads. To ensure proper handling, set the <code>client_max_body_size</code> parameter in your NGINX configuration file to accommodate the larger value between the file upload size and the text size defined in TheHive's application.conf</p> <p>To configure <code>client_max_body_size</code>, follow these steps:</p> <ol> <li> <p>Edit your NGINX configuration file. This file is typically located at <code>/etc/nginx/nginx.conf</code> or within a specific server block configuration file.</p> </li> <li> <p>Locate the <code>http</code> block in the NGINX configuration file.</p> </li> <li> <p>Add or modify the <code>client_max_body_size</code> directive within the <code>http</code> block to specify the maximum allowable size for incoming requests. For example:</p> </li> </ol> <pre><code>http {\n    ...\n    client_max_body_size 100M;  # Adjust to your desired value\n    ...\n}\n</code></pre> <p>For more detailed information, please refer to the article Limit File Upload Size in NGINX.</p> <p> </p>"},{"location":"thehive/configuration/ssl/","title":"SSL Configuration","text":""},{"location":"thehive/configuration/ssl/#ssl-configuration","title":"SSL Configuration","text":""},{"location":"thehive/configuration/ssl/#connect-thehive-using-https","title":"Connect TheHive using HTTPS","text":"<p>It is recommended to set up a reverse proxy, such as Nginx, to manage the SSL layer for TheHive.</p> Nginx <p>For detailed instructions on configuring HTTPS servers with Nginx, refer to the Nginx documentation</p> /etc/nginx/sites-available/thehive.conf<pre><code>server {\n  listen 443 ssl http2;\n  server_name thehive;\n\n  ssl on;\n  ssl_certificate       /path-to/thehive-server-chained-cert.pem;\n  ssl_certificate_key   /path-to/thehive-server-key.pem;\n\n  proxy_connect_timeout   600;\n  proxy_send_timeout      600;\n  proxy_read_timeout      600;\n  send_timeout            600;\n  client_max_body_size    2G;\n  proxy_buffering off;\n  client_header_buffer_size 8k;\n\n  location / {\n    add_header              Strict-Transport-Security \"max-age=31536000; includeSubDomains\";\n    proxy_pass              http://127.0.0.1:9000/;\n    proxy_http_version      1.1;\n  }\n}\n</code></pre>"},{"location":"thehive/configuration/ssl/#client-configuration","title":"Client Configuration","text":"<p>SSL configuration settings may be necessary to connect remote services. Below are the parameters that can be defined:</p> Parameter Type Description <code>wsConfig.ssl.keyManager.stores</code> list Stores client certificates (see #certificate-manager ) <code>wsConfig.ssl.trustManager.stores</code> list Stores custom Certificate Authorities (see #certificate-manager <code>wsConfig.ssl.protocol</code> string Defines a different default protocol (see #protocols) <code>wsConfig.ssl.enabledProtocols</code> list List of enabled protocols (see #protocols) <code>wsConfig.ssl.enabledCipherSuites</code> list List of enabled cipher suites (see #ciphers) <code>wsConfig.ssl.loose.acceptAnyCertificate</code> boolean Accept any certificates true / false <p> </p>"},{"location":"thehive/configuration/ssl/#certificate-manager","title":"Certificate Manager","text":"<p>The certificate manager is used to store client certificates and certificate authorities.</p> <p> </p>"},{"location":"thehive/configuration/ssl/#using-custom-certificate-authorities","title":"Using Custom Certificate Authorities","text":"<p>The preferred method for using custom Certificate Authorities is to use the system configuration.</p> DebianRPM <p>Ensure the <code>ca-certificates-java</code> package is installed, copy the CA certificate to the appropriate folder, then reconfigure certificates and restart TheHive service.</p> <pre><code>apt-get install -y ca-certificates-java\nmkdir /usr/share/ca-certificates/extra\ncp mycustomcert.crt /usr/share/ca-certificates/extra\ndpkg-reconfigure ca-certificates\nservice thehive restart\n</code></pre> <p>Copy the CA certificate to the correct folder, update CA trust, and restart TheHive service.</p> <pre><code>cp mycustomcert.crt /etc/pki/ca-trust/source/anchors\nsudo update-ca-trust \nservice thehive restart\n</code></pre> <p>An alternative approach is to use dedicated trust stores, although this is not the recommended option. Use the <code>trustManager</code> key in TheHive configuration to establish secure connections with remote hosts. Ensure that server certificates are signed by trusted certificate authorities.</p> <pre><code>  wsConfig.ssl.trustManager {\n    stores = [\n      {\n        type = \"JKS\" // JKS or PEM\n        path = \"keystore.jks\"\n        password = \"password1\"\n      }\n    ]\n  }\n</code></pre> <p> </p>"},{"location":"thehive/configuration/ssl/#client-certificates","title":"Client Certificates","text":"<p>The <code>keyManager</code> parameter specifies which certificate the HTTP client can use for authentication on remote hosts when certificate-based authentication is required.</p> <pre><code>  wsConfig.ssl.keyManager {\n    stores = [\n      {\n        type = \"pkcs12\" // JKS or PEM\n        path = \"mycert.p12\"\n        password = \"password1\"\n      }\n    ]\n  }\n</code></pre> <p> </p>"},{"location":"thehive/configuration/ssl/#protocols","title":"Protocols","text":"<p>To define a different default protocol use the following configuration:</p> <pre><code>wsConfig.ssl.protocol = \"TLSv1.2\"\n</code></pre> <p>To define a list of enabled protocols, use the following configuration:</p> <pre><code>wsConfig.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"]\n</code></pre> <p> </p>"},{"location":"thehive/configuration/ssl/#advanced-options","title":"Advanced Options","text":""},{"location":"thehive/configuration/ssl/#ciphers","title":"Ciphers","text":"<p>Configure cipher suites using <code>wsConfig.ssl.enabledCipherSuites</code>:</p> <pre><code>wsConfig.ssl.enabledCipherSuites = [\n  \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\",\n  \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\",\n  \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\",\n  \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\n]\n</code></pre> <p> </p>"},{"location":"thehive/configuration/ssl/#debugging","title":"Debugging","text":"<p>Enable debugging flags to troubleshoot key managers and trust managers:</p> <pre><code>  wsConfig.ssl.debug = {\n    ssl = true\n    trustmanager = true\n    keymanager = true\n    sslctx = true\n    handshake = true\n    verbose = true\n    data = true\n    certpath = true\n  }\n</code></pre> <p> </p>"},{"location":"thehive/download/","title":"Download","text":""},{"location":"thehive/download/#download-thehive","title":"Download TheHive","text":"<p>TheHive is available in multiple binary package formats, allowing for seamless installation on various operating systems. Below are the available options for obtaining and installing TheHive on different platforms.</p>"},{"location":"thehive/download/#debian-ubuntu","title":"Debian /  Ubuntu","text":"<p>If you are running an operating system based on Debian/Ubuntu, TheHive can be installed by following the steps in the Step-by-Step Installation Guide. These steps will guide you through the necessary configurations and prerequisites to get TheHive fully operational.</p> <p> </p>"},{"location":"thehive/download/#redhat-enterprise-linux-fedora","title":"RedHat Enterprise Linux /  Fedora","text":"<p>If you are running an operating system based on RedHat or Fedora, TheHive can be installed by following the steps in the Step-by-Step Installation Guide. These steps will guide you through the necessary configurations and prerequisites to get TheHive fully operational.</p> <p>Note: Make sure to select the correct RPM tab when following the step-by-step instructions to ensure the proper commands are used.</p> <p> </p>"},{"location":"thehive/download/#docker","title":"Docker","text":"<p>If you prefer using Docker, you can leverage pre-built Docker images available on Docker Hub for convenient deployment and containerization of TheHive. </p> <p>The Docker image can be found on TheHive Docker Hub, and instructions to run it can be found here. These steps will guide you through the necessary configurations and prerequisites to get TheHive fully operational.</p> <p> </p>"},{"location":"thehive/download/#kubernetes","title":"Kubernetes","text":"<p>If you prefer using Kubernetes, you can leverage pre-built Docker images available on Docker Hub for convenient deployment and containerization of TheHive. </p> <p>The Docker image can be found on TheHive Docker Hub, and instructions to run it on Kubernetes can be found here. These steps will guide you through the necessary configurations and prerequisites to get TheHive fully operational.</p> <p> </p>"},{"location":"thehive/download/archives/","title":"Archives","text":"<p>In addition to the download methods explained on the Step-by-Step Installation Guide, users can also download TheHive as a ZIP archive for manual installation or deployment scenarios. Follow these steps:</p> <ol> <li> <p>Visit the following URL to download the latest ZIP archive: thehive-latest.zip</p> </li> <li> <p>Once the download is complete, extract the contents of the ZIP archive to your desired location.</p> </li> <li> <p>Proceed with the installation or configuration process as per your requirements.</p> </li> </ol> <p> </p>"},{"location":"thehive/download/archives/#deb-packages","title":"DEB Packages","text":"<p>Access DEB packages from the following URL: DEB Packages. Please follow the instructions provided here to install the downloaded package.</p>"},{"location":"thehive/download/archives/#rpm-packages","title":"RPM Packages","text":"<p>Access RPM packages from the following URL: RPM Packages. Please follow the instructions provided here to install the downloaded package.</p>"},{"location":"thehive/download/archives/#zip-archives","title":"ZIP Archives","text":"<p>Access ZIP archives from the following URL: ZIP Archives. Please follow the instructions provided here to install the downloaded package.</p> <p> </p>"},{"location":"thehive/how-to/alert-management/","title":"Alert Management","text":""},{"location":"thehive/how-to/alert-management/#alert-management","title":"Alert Management","text":""},{"location":"thehive/how-to/alert-management/#alert-list","title":"Alert list","text":"<p>Alerts received by your organization can be viewed in TheHive:</p> <p></p> <p>Every user inside the organization can view the alerts. But you will need the permission <code>manageAlert</code> to be able to edit alerts.</p> <p>A user can use predefined filter or custom filters to view only selected alerts:</p> <p></p>"},{"location":"thehive/how-to/alert-management/#alert-details","title":"Alert details","text":"<p>From the alert list, an alert can be opened for more investigation. Details are filled, comments by analysts can be made on the alert too:</p> <p></p> <p>You can use tags, comments, severity, tlp, pap, custom fields and custom statuses to help categorize your alerts.</p> <p>Observables from the alert can be further analyzed either by the analysts or by using Cortex analyzers:</p> <p></p> <p></p> <p>Finally, depending on the analyst investigation, an alert can be closed (marked as \"False Positive\", \"Duplicate\", \"Ignored\" or an other custom status) or a case can be created to pursue the investigation.</p>"},{"location":"thehive/how-to/authentication/","title":"Authentication","text":""},{"location":"thehive/how-to/authentication/#authentication","title":"Authentication","text":"<p>TheHive supports several authentication providers:</p> <ul> <li>local (credential are securely stored in TheHive database)</li> <li>directory (LDAP and Active Directory)</li> <li>OAuth2/OpenID-Connect</li> <li>SAML</li> <li>based on HTTP header to delegate authentication to reverse proxy</li> </ul> <p></p> <p>Multi-factor authentication can be enabled to enforce security on user authentication.</p> <p>Several authentication providers can be enable. Each of them is check sequentially (order is important).</p>"},{"location":"thehive/how-to/authentication/#active-directory","title":"Active Directory","text":""},{"location":"thehive/how-to/authentication/#ldap","title":"LDAP","text":""},{"location":"thehive/how-to/authentication/#oauth2-openid-connect","title":"OAuth2 / OpenID-Connect","text":""},{"location":"thehive/how-to/authentication/#saml","title":"SAML","text":""},{"location":"thehive/how-to/authentication/#user-synchronisation","title":"User synchronisation","text":"<p>The user can be provisionned and deprovisionned automatically based on the content of a directory. The user data are synchronised periodically. New users in LDAP are created in TheHive, removed users are disabled.</p> <p>The organisation membership and the profile of an user are set using LDAP groups. The configuration contain the mapping of LDAP groups with organisation/profile. </p>"},{"location":"thehive/how-to/case-management/","title":"Case Management","text":""},{"location":"thehive/how-to/case-management/#case-management","title":"Case Management","text":"<p>Case Management is the main purpose of TheHive. Handling incidents with predefined tasks or manually added tasks, assiging a case owner, adding observables and enrich them, looking for correlations with existing cases and alert, prioritising incidents and classifying them... those are few of the case management capabilities in TheHive.</p> <p> </p>"},{"location":"thehive/how-to/case-management/#creating-case","title":"Creating case","text":"<p>Cases can be created in various ways:</p> <ul> <li>Manually from scratch</li> <li>Manually using a case template</li> <li>Importing a TheHive archive generated from another TheHive instance</li> <li>Converting one or many alerts into a incident</li> </ul> <p></p>"},{"location":"thehive/how-to/case-management/#creating-a-case-from-a-case-template","title":"Creating a case from a case template","text":"<p>Case templates are models of cases, including predefined and documented tasks as well as custom fields</p> <p></p>"},{"location":"thehive/how-to/case-management/#applying-case-template-on-ongoing-investigations","title":"Applying case template on ongoing investigations","text":"<p>Case templates can also be used to enrich a case with additional tasks, tags and custom field during open investigations:</p> <p></p>"},{"location":"thehive/how-to/case-management/#anatomy-of-a-case","title":"Anatomy of a case","text":"<p>A case in TheHive is defined by:</p> <ul> <li>A set of predefined properties: Title, tags, assignee, TLP, PAP, severity, description, status</li> <li>A set of custom fields (optional or mandatory) </li> <li>A set of tasks, defined by a title, assignee, status, description and a set of task logs and attachements</li> <li>A set of observables, of predefined or custom data types, defined by a value, IoC and Sighted flags, sighting date, tags and a description</li> <li>A set of TTPs related to MITRE ATT&amp;CK</li> <li>A set of attachments</li> <li>A set of pages as a wiki</li> <li>A set of comments</li> </ul> <p></p>"},{"location":"thehive/how-to/case-management/#case-tasks","title":"Case tasks","text":""},{"location":"thehive/how-to/case-management/#case-observables","title":"Case observables","text":""},{"location":"thehive/how-to/case-management/#case-ttps","title":"Case TTPs","text":""},{"location":"thehive/how-to/case-management/#case-timeline","title":"Case timeline","text":""},{"location":"thehive/how-to/case-management/#case-correlations","title":"Case correlations","text":"<p>Case correlations with existing cases and alert are based on the common observables</p> <p> </p>"},{"location":"thehive/how-to/case-management/#case-export","title":"Case export","text":"<p>Cases can be exported as password protected archives or as a MISP event</p> <p></p>"},{"location":"thehive/how-to/dashboards/","title":"Dashboards and Reporting","text":""},{"location":"thehive/how-to/dashboards/#dashboards-and-reporting","title":"Dashboards and Reporting","text":"<p>TheHive comes with a reporting module that allows designing shared and private dashbords using various widgets for data visualisation. Reports can gather metrics from any data stored in TheHive like cases, alerts, tasks, observables...</p>"},{"location":"thehive/how-to/dashboards/#dashboards","title":"Dashboards","text":"<p>Every user has read access to the dashboards defined in the organisation (s)he belongs to. If the <code>manageDashboard</code> permission is part of the user's profile, the user can create dashboards.</p>"},{"location":"thehive/how-to/dashboards/#create-a-dashboard","title":"Create a dashboard","text":""},{"location":"thehive/how-to/dashboards/#list-dashboards","title":"List dashboards","text":""},{"location":"thehive/how-to/dashboards/#view-dashboard","title":"View dashboard","text":""},{"location":"thehive/how-to/dashboards/#configure-widgets","title":"Configure widgets","text":""},{"location":"thehive/how-to/dashboards/#case-timelines","title":"Case timelines","text":"<p>Case timelines are a second part of TheHive's reporting capabilities. Case timelines display any event that happened during the lifecycle of a given case:</p> <ul> <li>Alert occurences</li> <li>Case creation</li> <li>Investigation start</li> <li>Task completion</li> <li>Flagged task logs</li> <li>IoC sightings</li> <li>Mitre Attack patterns</li> <li>Additional custom events</li> </ul> <p></p> <p></p>"},{"location":"thehive/how-to/fail2ban/","title":"Fail2ban Configuration","text":""},{"location":"thehive/how-to/fail2ban/#fail2ban","title":"Fail2ban","text":"<p>Fail2ban is an intrusion prevention software designed to enhance the security of Linux systems by actively monitoring logs for suspicious activity and dynamically blocking malicious IP addresses. By automatically responding to potential threats such as repeated failed login attempts or suspicious access patterns, Fail2ban acts as a proactive defense mechanism, fortifying servers against various forms of cyberattacks.</p>"},{"location":"thehive/how-to/fail2ban/#adding-thehive-into-fail2ban","title":"Adding TheHive into Fail2ban","text":"<p>To integrate TheHive logs with Fail2ban, follow the steps below. Assume TheHive logs are located at <code>/var/log/thehive/application.log</code> and Fail2ban configuration files are located in <code>/etc/fail2ban</code>.</p> <ol> <li> <p>Step 1: Create a Filter File</p> <ul> <li>Create a filter file in <code>/etc/fail2ban/filter.d</code> named <code>thehive.conf</code> with the following content:</li> </ul> <pre><code>[INCLUDES]\nbefore = common.conf\n\n[Definition]\nfailregex = ^.*- &lt;HOST&gt; (?:POST \\/api\\/login|GET .*) .*returned 401.*$\nignoreregex =\n</code></pre> </li> <li> <p>Step 2: Create a Jail File</p> <ul> <li>Create a jail file in <code>/etc/fail2ban/jail.d</code> named <code>thehive.local</code> with the following content:</li> </ul> <pre><code>[thehive]\nenabled = true\nport = 80,443\nfilter = thehive\naction = iptables-multiport[name=thehive, port=\"80,443\"]\nlogpath = /var/log/thehive/application.log\nmaxretry = 5\nbantime = 14400\nfindtime = 1200\n</code></pre> <p>This configuration will ban any IP address for 4 hours after 5 failed authentication attempts within a 20-minute period.</p> </li> <li> <p>Step 3: Reload Fail2ban Configuration</p> <ul> <li>Reload the Fail2ban configuration to apply the changes:</li> </ul> <pre><code>fail2ban-client reload\n</code></pre> </li> </ol>"},{"location":"thehive/how-to/fail2ban/#review-banned-ip-addresses","title":"Review Banned IP Addresses","text":"<p>Here is a step-by-step guide to reviewing banned IP addresses on Fail2ban:</p> <ol> <li> <p>Step 1: Check Fail2ban Status:</p> <p>Use the following command to get an overview of Fail2ban's status and active jails:</p> <pre><code>sudo fail2ban-client status\n</code></pre> </li> <li> <p>Step 2: Review Banned IPs for a Specific Jail</p> <p>Use this command to view banned IPs in a particular jail.</p> <pre><code>sudo fail2ban-client status thehive\n</code></pre> </li> </ol>"},{"location":"thehive/how-to/fail2ban/#unban-an-ip-address","title":"Unban an IP Address","text":"<p>Use the following command to remove the ban on a specific IP address. Replace <code>jail_name</code> with the jail's name and <code>IP_address</code> with the specific IP address you want to unban:</p> <pre><code>sudo fail2ban-client set thehive unbanip 1.1.1.1\n</code></pre> <p> </p>"},{"location":"thehive/how-to/knowledge-base/","title":"Knwoledge Base","text":""},{"location":"thehive/how-to/knowledge-base/#knwoledge-base","title":"Knwoledge Base","text":"<p>TheHive has a knowledge base module that allow writing Markdown pages at two levels:</p> <ul> <li>Organisation level</li> <li>Case level</li> </ul>"},{"location":"thehive/how-to/knowledge-base/#organisation-wiki","title":"Organisation wiki","text":"<p>Every organisation is able to define a set of Markdown pages accessible to all the users. Adding pages requires a <code>manageKnowledgeBase</code> user permission.</p> <p></p>"},{"location":"thehive/how-to/knowledge-base/#case-pages","title":"Case pages","text":"<p>Within every case, users with <code>managePage</code> permissions, can create and write Markdown pages. This feature ca, be used for:</p> <ul> <li>meeting notes</li> <li>reports</li> <li>pasties</li> <li>any other content</li> </ul> <p></p>"},{"location":"thehive/how-to/markdown/","title":"How to Markdown: A Comprehensive Guide","text":""},{"location":"thehive/how-to/markdown/#how-to-markdown-a-comprehensive-guide","title":"How to Markdown: A Comprehensive Guide","text":"<p>Welcome to the \"How to Markdown\" guide! In this comprehensive guide, we'll explore the powerful features and capabilities of Markdown \u2013 a lightweight markup language that simplifies text formatting while retaining its readability. Whether you're new to Markdown or looking to enhance your existing knowledge, this guide will walk you through each feature step by step, providing you with practical examples and clear explanations.</p>"},{"location":"thehive/how-to/markdown/#table-of-contents","title":"Table of Contents","text":"<ol> <li><code>Heading</code></li> <li><code>Horizontal Rules</code></li> <li><code>Emphasis</code></li> <li><code>Blockquotes</code></li> <li><code>Lists</code></li> <li><code>Code</code></li> <li><code>Tables</code></li> <li><code>Links</code></li> <li><code>Images</code></li> </ol>"},{"location":"thehive/how-to/markdown/#heading","title":"Heading","text":""},{"location":"thehive/how-to/markdown/#code","title":"Code","text":"<pre><code># h1 Heading\n## h2 Heading\n### h3 Heading\n#### h4 Heading\n##### h5 Heading\n###### h6 Heading\n</code></pre>"},{"location":"thehive/how-to/markdown/#example","title":"Example","text":""},{"location":"thehive/how-to/markdown/#h1-heading","title":"h1 Heading","text":""},{"location":"thehive/how-to/markdown/#h2-heading","title":"h2 Heading","text":""},{"location":"thehive/how-to/markdown/#h3-heading","title":"h3 Heading","text":""},{"location":"thehive/how-to/markdown/#h4-heading","title":"h4 Heading","text":""},{"location":"thehive/how-to/markdown/#h5-heading","title":"h5 Heading","text":""},{"location":"thehive/how-to/markdown/#h6-heading","title":"h6 Heading","text":""},{"location":"thehive/how-to/markdown/#horizontal-rules","title":"Horizontal Rules","text":""},{"location":"thehive/how-to/markdown/#code_1","title":"Code","text":"<pre><code>--- or *** or ___\n</code></pre>"},{"location":"thehive/how-to/markdown/#example_1","title":"Example","text":""},{"location":"thehive/how-to/markdown/#emphasis","title":"Emphasis","text":""},{"location":"thehive/how-to/markdown/#code_2","title":"Code","text":"<pre><code>**This is bold text** or __This is bold text__    \n*This is italic text* or _This is italic text_    \n~~Strikethrough~~\n</code></pre>"},{"location":"thehive/how-to/markdown/#example_2","title":"Example","text":"<p>This is bold text</p> <p>This is italic text</p> <p>~~Strikethrough~~</p>"},{"location":"thehive/how-to/markdown/#blockquotes","title":"Blockquotes","text":""},{"location":"thehive/how-to/markdown/#code_3","title":"Code","text":"<pre><code>&gt; Blockquotes can also be nested...\n&gt;&gt; ...by using additional greater-than signs right next to each other...\n&gt; &gt; &gt; ...or with spaces between arrows.\n</code></pre>"},{"location":"thehive/how-to/markdown/#example_3","title":"Example","text":"<p>Blockquotes can also be nested...</p> <p>...by using additional greater-than signs right next to each other...</p> <p>...or with spaces between arrows.</p>"},{"location":"thehive/how-to/markdown/#lists","title":"Lists","text":""},{"location":"thehive/how-to/markdown/#unordered","title":"Unordered","text":""},{"location":"thehive/how-to/markdown/#code_4","title":"Code","text":"<pre><code>+ Create a list by starting a line with `+`, `-`, or `*`\n+ Sub-lists are made by indenting 2 spaces:\n- Marker character change forces new list start:\n    * Ac tristique libero volutpat at\n    + Facilisis in pretium nisl aliquet\n    - Nulla volutpat aliquam velit\n+ Very easy!\n</code></pre>"},{"location":"thehive/how-to/markdown/#example_4","title":"Example","text":"<ul> <li>Create a list by starting a line with <code>+</code>, <code>-</code>, or <code>*</code></li> <li>Sub-lists are made by indenting 2 spaces:</li> <li>Marker character change forces new list start:<ul> <li>Ac tristique libero volutpat at</li> <li>Facilisis in pretium nisl aliquet</li> <li>Nulla volutpat aliquam velit</li> </ul> </li> <li>Very easy!</li> </ul>"},{"location":"thehive/how-to/markdown/#ordered","title":"Ordered","text":""},{"location":"thehive/how-to/markdown/#code_5","title":"Code","text":"<pre><code>1. Lorem ipsum dolor sit amet\n2. Consectetur adipiscing elit\n3. Integer molestie lorem at massa\n</code></pre>"},{"location":"thehive/how-to/markdown/#example_5","title":"Example","text":"<ol> <li>Lorem ipsum dolor sit amet</li> <li>Consectetur adipiscing elit</li> <li> <p>Integer molestie lorem at massa</p> </li> <li> <p>You can use sequential numbers...</p> </li> <li>...or keep all the numbers as <code>1.</code></li> </ol> <p>Start numbering with offset:</p> <ol> <li>foo</li> <li>bar</li> </ol>"},{"location":"thehive/how-to/markdown/#code_6","title":"Code","text":"<p>Inline <code>code</code> with `</p> <p>Indented code with tab</p> <pre><code>// Some comments\nline 1 of code\nline 2 of code\nline 3 of code\n</code></pre> <p>Block code \"fences\" with ```</p> <pre><code>Sample text here...\n</code></pre>"},{"location":"thehive/how-to/markdown/#tables","title":"Tables","text":""},{"location":"thehive/how-to/markdown/#code_7","title":"Code","text":"<pre><code>| Option | Description |\n| ------ | ----------- |\n| data   | path to data files to supply the data that will be passed into templates. |\n| engine | engine to be used for processing templates. Handlebars is the default. |\n| ext    | extension to be used for dest files. |\n</code></pre>"},{"location":"thehive/how-to/markdown/#example_6","title":"Example","text":"Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files."},{"location":"thehive/how-to/markdown/#right-aligned-columns","title":"Right aligned columns","text":""},{"location":"thehive/how-to/markdown/#code_8","title":"Code","text":"<pre><code>| Option | Description |\n| ------:| -----------:|\n| data   | path to data files to supply the data that will be passed into templates. |\n| engine | engine to be used for processing templates. Handlebars is the default. |\n| ext    | extension to be used for dest files. |\n</code></pre>"},{"location":"thehive/how-to/markdown/#example_7","title":"Example","text":"Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files."},{"location":"thehive/how-to/markdown/#links","title":"Links","text":""},{"location":"thehive/how-to/markdown/#code_9","title":"Code","text":"<pre><code>[link text](https://docs.strangebee.com/thehive/administration/organisations/)\n[link with title](https://docs.strangebee.com/thehive/administration/organisations/ \"title text!\")\n</code></pre>"},{"location":"thehive/how-to/markdown/#example_8","title":"Example","text":"<p>link text</p> <p>link with title</p>"},{"location":"thehive/how-to/markdown/#images","title":"Images","text":""},{"location":"thehive/how-to/markdown/#code_10","title":"Code","text":"<pre><code>![TheHive](https://www.strangebee.com/images/logos/theHivePlatformBlock.svg)\n![Cortex](https://www.strangebee.com/images/logos/cortexPlatformBlock.svg \"Cortex\")\n\nLike links, Images also have a footnote style syntax\n\n![Alt text][id]\n\nWith a reference later in the document defining the URL location:\n\n[id]: https://www.strangebee.com/images/logos/theHiveCloudPlatformBlock.svg  \"TheHive Cloud Platform\"\n</code></pre>"},{"location":"thehive/how-to/markdown/#example_9","title":"Example","text":""},{"location":"thehive/how-to/misp-integration/","title":"MISP Integration","text":""},{"location":"thehive/how-to/misp-integration/#misp-integration","title":"MISP Integration","text":"<p>TheHive in strongly integrated with MISP (Malware Information Sharing Platform).</p> <p>Using it's connector, TheHive has the capabilities to:</p> <ul> <li>Receive MISP events and ingest them as alerts</li> <li>Send TheHive <code>Cases</code> to MISP as events</li> </ul> <p>This integration is highly configurable and allows TheHive to synchronize with one or multiple MISP servers.</p>"},{"location":"thehive/how-to/misp-integration/#configuration","title":"Configuration","text":"<p>To add or configure a MISP server, open the Admin Organisation page (1), go to the Platform Management menu (2) and navigate to the MISP tab (3).</p> <p>Click the \"+\" button to add a new MISP server (4).</p> <p></p>"},{"location":"thehive/how-to/misp-integration/#general-settings","title":"General settings","text":"<p>This configuration is common to all MISP servers connected to TheHive.</p> <ul> <li>Interval: define the time interval between each events polling from TheHive to MISP</li> </ul>"},{"location":"thehive/how-to/misp-integration/#servers-general-settings","title":"Servers General settings","text":"<p>While clicking on add or edit a MISP server, a drawer will appear where you can define the following settings:</p> <ul> <li>Server name: MISP server name to display within TheHive</li> <li>Server URL: URL of the MISP server</li> <li>API Key: secret with sufficient permission to get &amp; create MISP events</li> <li>Purpose: Chose the synchronization way; Import: only import events from MISP to TheHive. Export: only exports cases from TheHive to MISP. Import and Export allow both ways synchronization</li> </ul> <p></p>"},{"location":"thehive/how-to/misp-integration/#server-proxy-settings","title":"Server Proxy Settings","text":"<p>Proxy settings should be set only if a proxy is required to reach the MISP server from TheHive.</p> <ul> <li>Type of protocol: Define on which protocol (HTTP/HTTPS) the proxy is listening</li> <li>Address: Define the proxy address</li> <li>Authentication: If the proxy require authentication, check this box. Username and password to authenticate must be provided when this box is checked.</li> <li>Do not check certificate authority: Do not verify the certificate authority when communicating with the proxy (not recommended, for HTTPS connection only)</li> <li>Disable hostname verification: Do not verify the hostname match with the certificate hostname.</li> </ul> <p></p>"},{"location":"thehive/how-to/misp-integration/#server-advanced-settings","title":"Server Advanced Settings","text":"<ul> <li>Chose the filter on TheHive organizations: For each server, you can define which TheHive organisation(s) to include or exclude of the synchronization (excluded or not included organisations will not receive the MISP events as <code>Alerts</code>)</li> <li>Tags: Append one or several tags to each MISP event ingested as <code>Alert</code> </li> <li>Export case tags: If enabled, the export will include the <code>Case</code> tags. </li> <li>Export observables tags: If enabled, the exported <code>Observables</code> will include the <code>Observables</code> tags.</li> </ul>"},{"location":"thehive/how-to/misp-integration/#server-filter-settings","title":"Server Filter Settings","text":"<p>This section allows to define filters for MISP events import. </p> <ul> <li>Maximum age: define the maximum age (based on creation date) for an event to be imported in TheHive.</li> <li>Organizations to include: Import only events created by the MISP organisation(s) defined in this field.</li> <li>Organizations to exclude: Import only events NOT created by the MISP organisation(s) defined in this field.</li> <li>Maximum number of attributes: Define a maximum number of MISP attributes (observables) per event to import. </li> <li>List of allowed tags: Import only events that contains the tags defined in this field</li> <li>Prohibited tags list: Import only events that DON'T contains the tags defined in this field</li> </ul> <p></p>"},{"location":"thehive/how-to/notifications/","title":"Notification Configuration","text":""},{"location":"thehive/how-to/notifications/#notifications","title":"Notifications","text":"<p>TheHive Notifications allow you to automatically react on specific events occurring in TheHive and send notification to defined Endpoints that can be:</p> <ul> <li> <p>Cortex</p> </li> <li> <p>Webhook listener</p> </li> <li> <p>Http listener</p> </li> <li> <p>Slack</p> </li> <li> <p>Mattermost</p> </li> </ul> <p>Endpoints need to be configured prior to use them in Notifications. You can also send an Email as notification. </p>"},{"location":"thehive/how-to/notifications/#notifications-management","title":"Notifications management","text":"<p>Notifications are unique to each organisation. With an org admin account open the Organization menu (1), and navigate to the Notifications tab (2).</p> <p>To create a notification, clic on the \"+\" button (3)</p> <p></p>"},{"location":"thehive/how-to/notifications/#configure-a-notification","title":"Configure a Notification","text":"<p>While clicking on add or edit a notifier, a drawer will appear where you can define the following settings:</p> <ul> <li>Name: Notification name to display within TheHive</li> <li>Send notification to every user in the organisation: Check this box to notify by email every users of the organization this Notifier has triggered</li> <li>Trigger: Chose in a list of triggers on which event you want to react. You can also select \"FilteredEvent\" to create your own event filter.</li> <li>Enable notification: Check this box to enable the notifier. Uncheck the box to disable the notifier.</li> </ul> <p>Finally, select which endpoint will receive the notification.</p>"},{"location":"thehive/how-to/notifications/#pre-defined-triggers-filteredevent","title":"Pre-defined triggers &amp; FilteredEvent","text":"<p>While configuring the Trigger setting, you can pick a pre-defined trigger from a list, or chose to create your own filters. </p> <p>Current pre-defined filters list: </p> <ul> <li> <p>AnyEvent</p> </li> <li> <p>Case Created</p> </li> <li> <p>Case Closed</p> </li> <li> <p>Case Shared</p> </li> <li> <p>Alert Created</p> </li> <li> <p>Alert Imported</p> </li> <li> <p>Job Finished</p> </li> <li> <p>Alert Observable Created</p> </li> <li> <p>Case Observable Created </p> </li> <li> <p>Observable Created</p> </li> <li> <p>Log in my task</p> </li> <li> <p>Task Assigned</p> </li> <li> <p>Task Closed</p> </li> <li> <p>Task Mandatory</p> </li> </ul> <p>But you can also chose to use a custom filter to react on specific events. </p> <p>Custom filters are JSON format written and can use common operators. Example with a filter for cases which Severity is updated to High or Critical:</p> <p></p>"},{"location":"thehive/how-to/notifications/#use-variables-in-notifications","title":"Use variables in notifications","text":"<p>You can include variables in your Email &amp; HTTP notification. </p> <p>Use the \"add variable\" bouton to see the list of available variables. Example with an email notification:</p> <p></p> <p>The templating engine is based on mustache so you can add some logic to your template. Example:</p> <pre><code>{{#if (eq object.severity 2) }}MEDIUM {{else}}Other {{/if}}\n</code></pre> <p>Some helpers are available to format your data:</p> Helper Description Usage Output <code>tlpLabel</code> Format the <code>tlp</code> field of the object <code>{{ tlpLabel object.tlp }}</code> <code>Amber</code> <code>papLabel</code> Format the <code>pap</code> field of the object <code>{{ papLabel object.pap }}</code> <code>Amber</code> <code>severityLabel</code> Format the <code>severity</code> field of the object <code>{{ severityLabel object.severity }}</code> <code>Critical</code> <code>dateFormat</code> Format a date field of the object, uses java date time patterns <code>{{dateFormat audit._createdAt \"EEEEE dd MMMMM yyyy\" \"fr\" }}</code> <code>jeudi 01 septembre 2022</code> <p>See our Leveraging TheHive 5 notifications capabilities blog articles to know more about Notifications</p>"},{"location":"thehive/how-to/splunk-integration/","title":"Splunk Integration Guide","text":""},{"location":"thehive/how-to/splunk-integration/#introduction","title":"Introduction","text":"<p>This integration, available as a Threat Intelligence Add-on (TA), streamlines the process of creating alerts in TheHive from Splunk search results. The process involves several key steps:</p> <ol> <li>Search for events and collect observables and custom fields.</li> <li>Rename Splunk fields to match the field names listed in the <code>thehive_datatypes.csv</code> lookup table.</li> <li>Save the search as an alert.</li> <li>Configure the alert action \"thehive_create_a_new_alert\" to generate alerts in TheHive.</li> <li>Customize alerts by adding additional information such as TLP per observables, custom fields, titles, descriptions, and more.</li> </ol>"},{"location":"thehive/how-to/splunk-integration/#use-cases-examples","title":"Use Cases Examples","text":"<p>In order to understand the practical application of this integration, several use cases (UC1 to UC5) are presented, demonstrating how to configure alerts and cases in TheHive based on various scenarios.</p> <p>Each use case provides insights into prerequisites, Splunk search configurations, Splunk screenshots, and TheHive screenshots.</p>"},{"location":"thehive/how-to/splunk-integration/#use-cases-detailed","title":"Use Cases Detailed","text":"<p>For a comprehensive understanding of each use case's parameters, including title, description, TLP, PAP, severity, observables, TTPs, and more, please refer to the original documentation.</p>"},{"location":"thehive/how-to/splunk-integration/#tips-tricks","title":"Tips &amp; Tricks","text":"<p>Discover valuable insights in the \"Tips &amp; Tricks\" section, where you can learn how to efficiently populate form fields, manage observables and inline fields, and make the most of tags to enhance your alerts.</p>"},{"location":"thehive/how-to/splunk-integration/#advanced-search-results-with-additional-tags","title":"Advanced Search Results with Additional Tags","text":"<p>Explore advanced techniques to enrich your search results with additional tags, including the utilization of the <code>th_inline_msg</code> field and the creative renaming of fields for improved tagging.</p> <p>For comprehensive details and examples, we encourage you to explore the original documentation to set up your connector effectively.</p> <p>To integrate this connector into your workflow, please refer to the original documentation. Your success starts there!</p>"},{"location":"thehive/how-to/splunk-integration/#support","title":"Support","text":"<p>Should you have any inquiries or feedback, feel free to reach out to the author on GitHub, or seek assistance from TheHive support through the portal.</p>"},{"location":"thehive/how-to/user-management/","title":"User Management","text":""},{"location":"thehive/how-to/user-management/#user-management","title":"User Management","text":"<p>In TheHive users can be created once an added to different organistions. User lists are available to <code>admin</code> and <code>org-admin</code> users:</p> <p></p> <p>When adding a user in an organisation, a user profile can be choosen for every organisation:</p> <p></p> <p></p> <p>Users can be created by administrators or organisation administrators or any user having the <code>manageUser</code> permission. This permission is included by default in the <code>admin</code> and <code>org-admin</code> user profiles.</p> <p>In TheHive, there are two types of users:</p> <ul> <li>Users with GUI access</li> <li>Service account aka. API users</li> </ul> <p></p> <p>Once created, users can be assigned a password and an API key </p>"},{"location":"thehive/images/installation/view-update-license/","title":"View Update License","text":""},{"location":"thehive/images/installation/view-update-license/#view-update-license","title":"View Update License","text":"<p>In this section you will find information about updating your license. When you install and TheHive, by default it will include the community edition license. </p> <p>If you want to activate the license, you nees to buy the license from StrangeBee. WHen you buy the license from StrangeBee, StrangeBee will create an account for you on the customer portal that will allow you to activate the license.</p> <p>To activate the license:</p> <ol> <li> <p>On the Platform Management page, in the License tab, click the Update the current license button.</p> <p></p> <p>Set a License key window opens. You can see the challenge in the window.</p> </li> <li> <p>Click Copy this challenge.</p> <p>You will see the challenge copied message. After you copy the challenge, you go to your account on the StrangeBee customer portal and activate the license using this challenge and the customer portal will give you an activation license key.</p> </li> <li> <p>Enter the activation key in the License field.</p> </li> <li> <p>Click the Activate the license key button.</p> <p>This will activate the license and update your instance with all the features included with that license.</p> <p></p> <p>The license is defined by the following capabilities:</p> <ol> <li>It defines how many users you can create in your platform. </li> <li>The license is based on the number of users and the number of organizations.</li> <li>It has a validation and an expiration date.</li> <li>It allows unlimited number of Readonly users and Service users. Service users are those who do not have access to the TheHive interface but they use an API key to call all the APIs.</li> </ol> <p> </p> </li> </ol>"},{"location":"thehive/installation/activate-license/","title":"Activating the License","text":""},{"location":"thehive/installation/activate-license/#activating-thehive-license","title":"Activating TheHive License","text":"<p>This section provides information about updating your license for TheHive. By default, TheHive is installed with the community edition license.</p> <p>To activate a different license, you need to purchase one from StrangeBee. Upon purchasing, StrangeBee will create an account for you on their customer portal, allowing you to activate your license.</p>"},{"location":"thehive/installation/activate-license/#activating-your-license","title":"Activating Your License","text":"<p>Follow these steps to activate your license:</p> <ol> <li> <p>Access the Platform Management Page:</p> <ul> <li>Navigate to the Platform Management page.</li> <li>Go to the License tab.</li> <li>Click the Update the current license button.</li> </ul> <p></p> </li> <li> <p>Copy the Challenge:</p> <ul> <li>A window will open displaying the challenge.</li> <li>Click Copy this challenge.</li> </ul> <p></p> <p>You will see a message confirming that the challenge has been copied. </p> </li> <li> <p>Activate the License on the Customer Portal:</p> <ul> <li>Log in to your account on the StrangeBee customer portal.</li> <li>Use the copied challenge to activate the license.</li> <li>The portal will provide you with an activation license key.</li> </ul> </li> <li> <p>Enter the Activation Key:</p> <ul> <li>Return to the Platform Management page.</li> <li>Enter the activation key in the License field.</li> <li>Click the Activate the license key button.</li> </ul> <p>This will activate your license and update your instance with all the features included in your license.</p> </li> </ol>"},{"location":"thehive/installation/activate-license/#license-capabilities","title":"License Capabilities","text":"<p>The license includes the following capabilities: - Defines the number of users you can create on your platform. - Based on the number of users and the number of organizations. - Includes a validation and an expiration date. - Allows an unlimited number of Readonly users and Service users. Service users can use an API key to call all APIs but do not have access to TheHive interface.</p>"},{"location":"thehive/installation/activate-license/#how-to-guide","title":"How-to Guide","text":"<p>Below is a video guide that walks you through the process of activating your license in detail.</p> <p></p> <p> </p>"},{"location":"thehive/installation/automated-installation-script/","title":"Automated Installation Script","text":""},{"location":"thehive/installation/automated-installation-script/#using-the-installation-script","title":"Using the Installation Script","text":"<p>TheHive provides users with a streamlined installation process through the automated installation script. This script is designed to simplify the setup process across supported operating systems, ensuring efficiency and ease of deployment.</p> <p>To obtain and install TheHive via our automated installation script, execute the following command in your terminal:</p> <pre><code>wget -q -O /tmp/install.sh https://archives.strangebee.com/scripts/install.sh ; sudo -v ; bash /tmp/install.sh\n</code></pre> <p>This script streamlines the installation procedure, ensuring a successful setup provided that hardware requirements are met. It automates the process of fetching necessary components and configuring the system for optimal performance.</p> <p></p> <p>Upon execution, users are presented with several customizable options tailored to their specific requirements:</p> <ol> <li>Setup Proxy Settings: Configure the host to work seamlessly with an HTTP proxy and integrate custom CA certificates for enhanced security and network compatibility.</li> <li>Install TheHive: Effortlessly deploy TheHive 5 along with its dependencies, enabling users to leverage its powerful features for their projects.</li> <li>Install Cortex: Facilitate the installation of Cortex and its dependencies to enable the execution of Analyzers &amp; Responders as Docker images, ensuring scalability and efficient resource utilization.</li> <li>Install Cortex on the Host: Alternatively, install Cortex and its dependencies directly on the host machine (compatible with Debian and Ubuntu ONLY), offering flexibility in deployment options and optimizing system resources.</li> </ol> <p></p> <p>Note</p> <p>For users requiring more detailed guidance, comprehensive installation guides are available for various deployment scenarios on the following links:</p> <ul> <li>Linux/Unix Based Systems Installation Guide</li> <li>TheHive Cluster Deployment</li> <li>Running with Docker</li> <li>Kubernetes Deployment</li> </ul>"},{"location":"thehive/installation/deploying-a-cluster/","title":"Deploying a Cluster","text":""},{"location":"thehive/installation/deploying-a-cluster/#setting-up-a-cluster-with-thehive","title":"Setting up a Cluster with TheHive","text":"<p>This guide presents configuration examples for setting up a fault-tolerant cluster for TheHive. Each cluster comprises three active nodes, featuring:</p> <ul> <li>Cassandra for the database</li> <li>Elasticsearch for the indexing engine</li> <li>MinIO for S3 data storage</li> <li>TheHive</li> <li>Haproxy (to demonstrate load balancing)</li> <li>Keepalived (to demonstrate virtual IP setup)</li> </ul> <p>Info</p> <p>These applications can either be installed on separate servers or on the same server. For the purpose of this documentation, we've chosen to demonstrate the setup on three distinct operating systems.</p>"},{"location":"thehive/installation/deploying-a-cluster/#architecture-diagram","title":"Architecture Diagram","text":"<p>The diagram above illustrates the key components and their interactions within the cluster architecture.</p> <p>Each component fulfills a critical role in the functionality and resilience of the cluster:</p> <ul> <li>Cassandra: Acts as the primary database, ensuring robust data storage and retrieval capabilities.</li> <li>Elasticsearch: Serves as the indexing engine, facilitating efficient search operations within TheHive platform.</li> <li>MinIO: Provides S3-compatible object storage, offering scalable and resilient storage solutions for TheHive.</li> <li>TheHive: Central component responsible for incident response management and collaboration within the cluster.</li> <li>Haproxy: Functions as a load balancer, evenly distributing incoming traffic across multiple nodes for improved performance and availability.</li> <li>Keepalived: Facilitates the setup of a virtual IP address, ensuring seamless failover and high availability by redirecting traffic to a standby node in case of failure.</li> </ul> <p>The subsequent sections will provide detailed configuration examples and step-by-step instructions for setting up each component within the cluster environment.</p>"},{"location":"thehive/installation/deploying-a-cluster/#cassandra-setup","title":"Cassandra Setup","text":"<p>When configuring a Cassandra cluster, we aim to establish a setup comprising three active nodes with a replication factor of 3. This configuration ensures that all nodes are active and data is replicated across each node, thus providing tolerance to the failure of a single node, meaning that if one node experiences hardware issues or network disruptions, the other two nodes continue to store and process incident data seamlessly. This fault-tolerant configuration guarantees uninterrupted access to critical security information, enabling the SOC to effectively manage and respond to cyber threats without downtime or data loss.</p> <p>Note: For the purposes of this documentation, we assume that all nodes reside within the same network environment.</p> <p> </p>"},{"location":"thehive/installation/deploying-a-cluster/#installation-instructions","title":"Installation Instructions","text":"<p>To ensure the successful deployment of Cassandra within your cluster, it's essential to install Cassandra on each individual node. Follow the steps outlined in the provided guide.</p> <p>A <code>node</code> in this context refers to each server or machine designated to participate in the Cassandra cluster.</p> <p> </p>"},{"location":"thehive/installation/deploying-a-cluster/#configuration-instructions","title":"Configuration Instructions","text":"<p>For each node in the Cassandra cluster, it's crucial to update the configuration files located at <code>/etc/cassandra/cassandra.yaml</code> with specific parameters to ensure proper functionality. Follow the steps below to modify the configuration:</p> <ol> <li> <p>Update Cassandra Configuration File: Open the <code>/etc/cassandra/cassandra.yaml</code> file on each node using a text editor.</p> /etc/cassandra/cassandra.yaml<pre><code>cluster_name: 'thp'\nnum_tokens: 256\nauthenticator: PasswordAuthenticator\nauthorizer: CassandraAuthorizer\nrole_manager: CassandraRoleManager\ndata_file_directories:\n    - /var/lib/cassandra/data\ncommitlog_directory: /var/lib/cassandra/commitlog\nsaved_caches_directory: /var/lib/cassandra/saved_caches\nseed_provider:\n    - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n    parameters:\n        - seeds: \"&lt;ip node 1&gt;, &lt;ip node 2&gt;, &lt;ip node 3&gt;\"  # (1)\nlisten_interface : eth0 # (2)\nrpc_interface: eth0 # (3)\nendpoint_snitch: SimpleSnitch\n</code></pre> <ol> <li>Ensure to list all IP addresses of the nodes that are included in the cluster</li> <li>Ensure to setup the right interface name</li> <li>Ensure to setup the right interface name</li> </ol> <ul> <li>Cluster Name: Set the name of the Cassandra cluster.</li> <li>Number of Tokens: Configure the number of tokens for each node.</li> <li>Authentication and Authorization: Specify the authenticator, authorizer, and role manager.</li> <li>Directories: Define directories for data, commit logs, and saved caches.</li> <li>Seed Provider: List all IP addresses of nodes included in the cluster (Ensure to list all IP addresses of the nodes that are included in the cluster).</li> <li>Network Interfaces: Set up the appropriate network interfaces (Ensure to setup the right interface name).</li> <li>Endpoint Snitch: Specify the snitch for determining network topology.</li> </ul> <p>For detailed explanations of each parameter in the YAML file, refer to our article on Cassandra configuration which can be found in the following page.</p> </li> <li> <p>Delete Cassandra Topology Properties File: Remove the <code>cassandra-topology.properties</code> file to prevent any conflicts.</p> <pre><code>rm /etc/cassandra/cassandra-topology.properties\n</code></pre> <p> </p> </li> </ol>"},{"location":"thehive/installation/deploying-a-cluster/#starting-the-nodes","title":"Starting the Nodes","text":"<p>To initiate the Cassandra service on each node, follow these steps:</p> <ol> <li> <p>Start Cassandra Service: Execute the following command on each node to start the Cassandra service:</p> <pre><code>service cassandra start\n</code></pre> </li> <li> <p>Verify Node Status: Ensure that all nodes are up and running by checking their status using the <code>nodetool status</code> command. Open a terminal and run:</p> </li> </ol> <pre><code>root@cassandra:/# nodetool status\nDatacenter: dc1\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address      Load       Tokens       Owns (effective)  Host ID                               Rack\nUN  &lt;ip node 1&gt;  776.53 KiB  256          100.0%            a79c9a8c-c99b-4d74-8e78-6b0c252abd86  rack1\nUN  &lt;ip node 2&gt;  671.72 KiB  256          100.0%            8fda2906-2097-4d62-91f8-005e33d3e839  rack1\nUN  &lt;ip node 3&gt;  611.54 KiB  256          100.0%            201ab99c-8e16-49b1-9b66-5444044fb1cd  rack1\n</code></pre> <p> </p>"},{"location":"thehive/installation/deploying-a-cluster/#initializing-the-database","title":"Initializing the Database","text":"<p>To initialize the database, perform the following steps:</p> <ol> <li> <p>Access CQL Shell: On one of the nodes, access the Cassandra Query Language (CQL) shell by running the following command, providing the IP address of the respective node:</p> <pre><code>cqlsh &lt;ip node X&gt; -u cassandra\n</code></pre> <p>Note that the default password for the cassandra account is <code>cassandra</code></p> </li> <li> <p>Change Superadmin Password: Begin by changing the password for the superadmin user named cassandra using the following SQL query:</p> <pre><code>ALTER USER cassandra WITH PASSWORD 'NEWPASSWORD';\n</code></pre> <p>After executing the query, exit the CQL shell and reconnect.</p> </li> <li> <p>Ensure User Account Duplication: Ensure that user accounts are duplicated on all nodes by executing the following SQL query:</p> <pre><code>ALTER KEYSPACE system_auth WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3 };\n</code></pre> </li> <li> <p>Create Keyspace: Create a keyspace named thehive with a replication factor of 3 and durable writes enabled:</p> <pre><code>CREATE KEYSPACE thehive WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3' } AND durable_writes = 'true';\n</code></pre> </li> <li> <p>Create Role and Grant Permissions: Finally, create a role named thehive and grant permissions on the thehive keyspace. Choose a password for the role:</p> <pre><code>CREATE ROLE thehive WITH LOGIN = true AND PASSWORD = 'PASSWORD';\nGRANT ALL PERMISSIONS ON KEYSPACE thehive TO 'thehive';\n</code></pre> </li> </ol>"},{"location":"thehive/installation/deploying-a-cluster/#elasticsearch-setup","title":"Elasticsearch Setup","text":""},{"location":"thehive/installation/deploying-a-cluster/#installation-instructions_1","title":"Installation Instructions","text":"<p>To establish a cluster of 3 active Elasticsearch nodes, follow the installation instructions provided on this page for each node.</p> <p> </p>"},{"location":"thehive/installation/deploying-a-cluster/#configuration-instructions_1","title":"Configuration Instructions","text":"<p>For each node, update the configuration files located at <code>/etc/cassandra/elasticsearch.yml</code> with the following parameters, ensuring to adjust the network.host accordingly.</p> <pre><code>http.host:  0.0.0.0\nnetwork.bind_host:  0.0.0.0\nscript.allowed_types:  inline,stored\ncluster.name: thehive\nnode.name: 'es1'\npath.data: /usr/share/elasticsearch/data\npath.logs: /usr/share/elasticsearch/logs\nnetwork.host: 'es1' # (1)\nhttp.port: 9200\ncluster.initial_master_nodes: \n  - es1\nnode.master: true\ndiscovery.seed_hosts: # (2)\n  - 'es1'\n  - 'es2'\n  - 'es3'\nthread_pool.search.queue_size: 100000\nthread_pool.write.queue_size: 100000\nxpack.security.enabled: true\nxpack.security.http.ssl.enabled: true\nxpack.security.transport.ssl.enabled: true\nxpack.security.http.ssl.key: /usr/share/elasticsearch/config/certs/es1/es1.key\nxpack.security.http.ssl.certificate: /usr/share/elasticsearch/config/certs/es1/es1.crt\nxpack.security.http.ssl.certificate_authorities: /usr/share/elasticsearch/config/certs/ca/ca.crt\nxpack.security.transport.ssl.key: /usr/share/elasticsearch/config/certs/es1/es1.key\nxpack.security.transport.ssl.certificate: /usr/share/elasticsearch/config/certs/es1/es1.crt\nxpack.security.transport.ssl.certificate_authorities: /usr/share/elasticsearch/config/certs/ca/ca.crt\n</code></pre> <ol> <li>Replace es1 with the IP or hostname of the respective node.</li> <li>Keep this parameter with the same value for all nodes to ensure proper cluster discovery.</li> </ol> <p>Warning</p> <p>When configuring Xpack and SSL with Elasticsearch, it's essential to review the documentation specific to your Elasticsearch version for accurate setup instructions. </p> <p> </p>"},{"location":"thehive/installation/deploying-a-cluster/#custom-jvm-options","title":"Custom JVM Options","text":"<p>To customize Java Virtual Machine (JVM) options for Elasticsearch, create a JVM Options File named jvm.options in the directory <code>/etc/elasticsearch/jvm.options.d/</code> with the following lines:</p> <pre><code>-Dlog4j2.formatMsgNoLookups=true\n-Xms4g\n-Xmx4g\n</code></pre> <p>Adjust according to Available Memory - It's important to adjust the heap size values based on the amount of memory available on your system to ensure optimal performance and resource utilization.</p> <p> </p>"},{"location":"thehive/installation/deploying-a-cluster/#starting-the-nodes_1","title":"Starting the Nodes","text":"<p>To start the Elasticsearch service on each node, execute the following command:</p> <pre><code>service elasticsearch start\n</code></pre> <p>This command initiates the Elasticsearch service, allowing the node to join the cluster and begin handling data requests.</p>"},{"location":"thehive/installation/deploying-a-cluster/#minio-setup","title":"MinIO Setup","text":"<p>  MinIO is a scalable, cloud-native object storage system designed to efficiently manage data storage and retrieval in cloud-native environments. Its primary purpose is to provide seamless scalability and reliability for storing and accessing large volumes of data across distributed systems. Within TheHive, MinIO efficiently handles vast amounts of data distributed across multiple nodes, ensuring robustness and optimal performance.</p> <p> </p>"},{"location":"thehive/installation/deploying-a-cluster/#installation-instructions_2","title":"Installation Instructions","text":"<p>To implement MinIO with TheHive, follow the procedure outlined below on all servers in the cluster. For this example, we assume a cluster setup consisting of 3 servers named minio1, minio2, and minio3.</p> <p>Warning</p> <p>Unlike Cassandra and Elasticsearch, MinIO requires a load balancer to be installed in front of the nodes to distribute connections effectively.</p> <ol> <li> <p>Create a Dedicated System Account: First, create a dedicated user and group for MinIO:</p> <pre><code>adduser minio-user\naddgroup minio-user\n</code></pre> </li> <li> <p>Create Data Volumes: Next, create at least 2 data volumes on each server by executing the following commands:</p> <pre><code>mkdir -p /srv/minio/{1,2}\nchown -R minio-user:minio-user /srv/minio\n</code></pre> </li> <li> <p>Setting up Hosts Files: To ensure proper communication between servers in your environment, it's necessary to configure the <code>/etc/hosts</code> file on all servers:</p> /etc/hosts<pre><code>ip-minio-1     minio1\nip-minio-2     minio2\nip-minio-3     minio3\n</code></pre> <p>In the above example, replace ip-minio-1, ip-minio-2, and ip-minio-3 with the respective IP addresses of your MinIO servers. These entries map the server names (minio1, minio2, minio3) to their corresponding IP addresses, ensuring that they can be resolved correctly within your network.</p> </li> <li> <p>Install MinIO: Installing MinIO and MC Command Line Tool, by first downloading the latest DEB packages for MinIO and MC from the official MinIO website and then installing the downloaded DEB packages using the dpkg command:</p> <p>Example for DEB packages</p> <pre><code>wget https://dl.min.io/server/minio/release/linux-amd64/minio_20220607003341.0.0_amd64.deb\nwget https://dl.min.io/client/mc/release/linux-amd64/mcli_20220509040826.0.0_amd64.deb\ndpkg -i minio_20220607003341.0.0_amd64.deb\ndpkg -i mcli_20220509040826.0.0_amd64.deb\n</code></pre> </li> </ol> <p>You can find the latest versions of the required packages on the MinIO download page. Ensure that you download the appropriate packages for your system architecture and MinIO version requirements.</p> <p> </p>"},{"location":"thehive/installation/deploying-a-cluster/#configuration-instructions_2","title":"Configuration Instructions","text":"<p>To configure MinIO, create or edit the file <code>/etc/default/minio</code> with the following settings:</p> /etc/default/minio<pre><code>MINIO_OPTS=\"--address :9100 --console-address :9001\"\nMINIO_VOLUMES=\"http://minio{1...3}:9100/srv/minio/{1...2}\"\nMINIO_ROOT_USER=thehive\nMINIO_ROOT_PASSWORD=password\nMINIO_SITE_REGION=\"us-east-1\"\n</code></pre> <p>Ensure to replace placeholders such as <code>thehive</code>, <code>password</code>, and <code>us-east-1</code> with your desired values. These settings define the address, volumes, root user credentials, and site region for your MinIO setup.</p> <p> </p>"},{"location":"thehive/installation/deploying-a-cluster/#enable-and-start-the-service","title":"Enable and Start the Service","text":"<p>Once configured, enable and start the MinIO service using the following commands:</p> <pre><code>systemctl daemon-reload\nsystemctl enable minio\nsystemctl start minio.service\n</code></pre> <p> </p>"},{"location":"thehive/installation/deploying-a-cluster/#prepare-the-service-for-thehive","title":"Prepare the Service for TheHive","text":"<p>Before proceeding, ensure that all servers are up and running. The following operations should be performed once all servers are operational, as adding a new server afterward is not supported.</p> <p>Following operations should be performed once all servers are up and running. A new server CAN NOT be added afterward.</p> <ol> <li> <p>Connect to one of the MinIO servers using your browser at port 9100: <code>http://minio:9100</code>. You will need to use the access key and secret key provided during the MinIO setup process.</p> <p></p> </li> <li> <p>Create a bucket named thehive.</p> <p></p> </li> </ol> <p>Ensure that the bucket is successfully created and available on all your MinIO servers. This ensures uniformity and accessibility across your MinIO cluster.</p>"},{"location":"thehive/installation/deploying-a-cluster/#thehive-setup","title":"TheHive Setup","text":"<p>TheHive utilizes the Akka toolkit to effectively manage clusters and enhance scalability. Akka facilitates efficient management of threads and multi-processing, enabling TheHive to handle concurrent tasks seamlessly.</p> <p>Note</p> <p>Akka is a toolkit designed for building highly concurrent, distributed, and resilient message-driven applications for Java and Scala.  Incorporating Akka into TheHive's configuration ensures robustness and enhances its ability to handle distributed workloads effectively.  Source: https://akka.io</p> <p> </p>"},{"location":"thehive/installation/deploying-a-cluster/#configuration","title":"Configuration","text":""},{"location":"thehive/installation/deploying-a-cluster/#cluster","title":"Cluster","text":"<p>When configuring TheHive for a clustered environment, it's essential to configure Akka to ensure efficient management of the cluster by the application.</p> <p>In this guide, we assume that node 1 serves as the master node. Begin by configuring the <code>akka</code> component in the <code>/etc/thehive/application.conf</code> file of each node as follows:</p> /etc/thehive/application.conf<pre><code>akka {\n  cluster.enable = on \n  actor {\n    provider = cluster\n  }\nremote.artery {\n  canonical {\n    hostname = \"&lt;My IP address&gt;\" # (1)\n    port = 2551\n  }\n}\n# seed node list contains at least one active node\ncluster.seed-nodes = [\n                      \"akka://application@&lt;Node 1 IP address&gt;:2551\",  # (2)\n                      \"akka://application@&lt;Node 2 IP address&gt;:2551\",\n                      \"akka://application@&lt;Node 3 IP address&gt;:2551\"\n                    ]\ncluster.min-nr-of-members = 2    # (3)\n}\n</code></pre> <ol> <li>Set the IP address of the current node.</li> <li>Ensure consistency of this parameter across all nodes.</li> <li>Choose a value corresponding to half the number of nodes plus one (for 3 nodes, use 2).</li> </ol> <p> </p>"},{"location":"thehive/installation/deploying-a-cluster/#database-and-index-engine-configuration","title":"Database and Index Engine Configuration","text":"<p>To ensure proper database and index engine configuration for TheHive, update the /etc/thehive/application.conf file as follows:</p> /etc/thehive/application.conf<pre><code>## Database configuration\ndb.janusgraph {\n  storage {\n    ## Cassandra configuration\n    # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql\n    backend = cql\n    hostname = [\"&lt;ip node 1&gt;\", \"&lt;ip node 2&gt;\", \"&lt;ip node 3&gt;\"] #(1)\n    # Cassandra authentication (if configured)\n    username = \"thehive\"\n    password = \"PASSWORD\"\n    cql {\n      cluster-name = thp\n      keyspace = thehive\n    }\n  }\n</code></pre> <p>Ensure that you replace <code>&lt;ip node 1&gt;</code>, <code>&lt;ip node 2&gt;</code>, and <code>&lt;ip node 3&gt;</code> with the respective IP addresses of your Cassandra nodes. This configuration ensures proper communication between TheHive and the Cassandra database, facilitating seamless operation of the platform. Additionally, if authentication is enabled for Cassandra, provide the appropriate username and password in the configuration.</p> <p> </p>"},{"location":"thehive/installation/deploying-a-cluster/#minio-s3-file-storage","title":"MinIO S3 file storage","text":"<p>To enable S3 file storage for each node in TheHive cluster, add the relevant storage configuration to the <code>/etc/thehive/application.conf</code> file. Below is an example configuration for the first node:</p> /etc/thehive/application.conf<pre><code>storage {\n  provider: s3\n  s3 {\n    bucket = \"thehive\"\n    readTimeout = 1 minute\n    writeTimeout = 1 minute\n    chunkSize = 1 MB\n    endpoint = \"http://&lt;IP_MINIO_1&gt;:9100\"\n    accessKey = \"thehive\"\n    aws.credentials.provider = \"static\"\n    aws.credentials.secret-access-key = \"password\"\n    access-style = path\n    aws.region.provider = \"static\"\n    aws.region.default-region = \"us-east-1\"\n  }\n}\n</code></pre> <ul> <li> <p>The provided configuration is backward compatible, ensuring compatibility with existing setups.</p> </li> <li> <p>Each TheHive server can connect to one MinIO server, or you can choose to distribute connections across all nodes of the cluster using a load balancer (refer to the example for TheHive).</p> </li> </ul> <p> </p>"},{"location":"thehive/installation/deploying-a-cluster/#start-the-service","title":"Start the service","text":"<p>Once the configuration is updated, start TheHive service using the following command:</p> <pre><code>systemctl start thehive\n</code></pre> <p>This command initiates TheHive service, enabling S3 file storage functionality as configured. Make sure to execute this command on each node of TheHive cluster to ensure proper functionality across the entire setup.</p>"},{"location":"thehive/installation/deploying-a-cluster/#load-balancing-with-haproxy","title":"Load Balancing with HAProxy","text":"<p>  To enhance the availability and distribution of HTTP requests across TheHive cluster, you can integrate a load balancer. The load balancer efficiently distributes incoming requests among the cluster nodes, ensuring optimal resource utilization. Notably, client affinity is not required, meaning that a client does not need to consistently connect to the same node.</p> <p>Below is a basic example of what should be added to the HAProxy configuration file, typically located at <code>/etc/haproxy/haproxy.cfg</code>. This configuration should be consistent across all HAProxy instances:</p> <pre><code># Listen on all interfaces, on port 80/tcp\nfrontend thehive-in\n        bind &lt;VIRTUAL_IP&gt;:80                # (1)\n        default_backend thehive\n# Configure all cluster node\nbackend thehive\n            balance roundrobin\n            server node1 THEHIVE-NODE1-IP:9000 check   # (2)\n            server node2 THEHIVE-NODE2-IP:9000 check\n            server node3 THEHIVE-NODE3-IP:9000 check\n</code></pre> <ol> <li>Configure the virtual IP address dedicated to the cluster.</li> <li>Specify the IP addresses and ports of all TheHive nodes to distribute traffic evenly across the cluster.</li> </ol> <p>This configuration ensures that incoming HTTP requests are efficiently distributed among the cluster nodes.</p>"},{"location":"thehive/installation/deploying-a-cluster/#virtual-ip-with-keepalived","title":"Virtual IP with Keepalived","text":"<p>  If you choose to use Keepalived to set up a virtual IP address for your load balancers, this section provides a basic example of configuration.</p> <p>Keepalived is a service that monitors the status of load balancers (such as HAProxy) installed on the same system. In this setup, LB1 acts as the master, and the virtual IP address is assigned to LB1. If the HAProxy service stops running on LB1, Keepalived on LB2 takes over and assigns the virtual IP address until the HAProxy service on LB1 resumes operation.</p> <pre><code>vrrp_script chk_haproxy {     # (1)\n      script \"/usr/bin/killall -0 haproxy\"  # cheaper than pidof\n      interval 2 # check every 2 seconds\n      weight 2 # add 2 points of priority if OK\n    }\n    vrrp_instance VI_1 {\n      interface eth0\n      state MASTER\n      virtual_router_id 51\n      priority 101 # 101 on primary, 100 on secondary      # (2)\n      virtual_ipaddress {\n        10.10.1.50/24 brd 10.10.1.255 dev eth0 scope global  # (3)\n      }\n      track_script {\n        chk_haproxy\n    } \n}\n</code></pre> <ol> <li>Requires Keepalived version &gt; 1.1.13.</li> <li>Set `priority 100`` for a secondary node.</li> <li> This is an example. Replace it with your actual IP address and broadcast address.</li> </ol>"},{"location":"thehive/installation/deploying-a-cluster/#troubleshooting","title":"Troubleshooting","text":"<p>Issues can be encountered during cluster deployment with TheHive. Here are some of the most commonly encountered ones and their solutions:</p> <ul> <li> <p>Example 1: InvalidRequest Error</p> <pre><code>InvalidRequest: code=2200 [Invalid query] message=\u201dorg.apache.cassandra.auth.CassandraRoleManager doesn\u2019t support PASSWORD\u201d.`\n</code></pre> <p>Resolution:</p> <p>To resolve this error, set the value <code>authenticator: PasswordAuthenticator</code> in the <code>cassandra.yaml</code> configuration file.</p> </li> </ul> <p> </p> <ul> <li> <p>Example 2: UnauthorizedException Caused by Inconsistent Replication</p> <pre><code>Caused by: java.util.concurrent.ExecutionException: com.datastax.driver.core.exceptions.UnauthorizedException: Unable to perform authorization of permissions: Unable to perform authorization of super-user permission: Cannot achieve consistency level LOCAL_ONE\n</code></pre> <p>Resolution:</p> <p>To address this issue, execute the following CQL command to adjust the replication settings for the system_auth keyspace:</p> <pre><code>ALTER KEYSPACE system_auth WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3 };\n</code></pre> <p>After making this adjustment, perform a full repair using the <code>nodetool repair -full</code> command to ensure data consistency across the cluster.</p> </li> </ul> <p> </p>"},{"location":"thehive/installation/kubernetes/","title":"Kubernetes Deployment","text":""},{"location":"thehive/installation/kubernetes/#deploy-on-kubernetes","title":"Deploy on Kubernetes","text":"<p>To deploy TheHive on Kubernetes, you can utilize the Docker image. For detailed instructions on how to use the Docker image, please refer to the docker image documentation</p>"},{"location":"thehive/installation/kubernetes/#important-considerations","title":"Important Considerations","text":"<ul> <li> <p>While this setup is suitable for testing TheHive, it's recommended to enhance the data stores (Elasticsearch, Cassandra, and Minio) for production use by setting up clustering and storage volumes. Refer to the respective documentation for instructions on deploying on Kubernetes for production use.</p> </li> <li> <p>The volumes used in this configuration are <code>emptyDir</code>s, which means the data will be lost when a pod is restarted. If you want to persist your data, update the volume description accordingly.</p> </li> <li> <p>To deploy multiple nodes, you will need to update your license as only one node is included in the Community License.</p> </li> </ul>"},{"location":"thehive/installation/kubernetes/#deploying-thehive","title":"Deploying TheHive","text":"<p>You can download the Kubernetes configuration file here. This configuration will deploy the following components:</p> <ul> <li>1 instance of TheHive</li> <li>1 instance of Cassandra</li> <li>1 instance of Elasticsearch</li> <li>1 instance of Minio</li> </ul> <p>You can initiate the deployment by executing the following command:</p> <pre><code>kubectl apply -f kubernetes.yml\n</code></pre> <p>This command will create a namespace named <code>thehive</code> and deploy the instances within it.</p>"},{"location":"thehive/installation/kubernetes/#kubernetes-configuration","title":"Kubernetes Configuration","text":"<p>In a Kubernetes environment with multiple TheHive pods, the application needs to form a cluster between its nodes. To achieve this, it utilizes the akka discovery method with the Kubernetes API.</p> <p>To enable this functionality, you need:</p> <ul> <li>A service account with permissions to connect to the Kubernetes API</li> <li>Configuration to instruct TheHive to use the Kubernetes API for discovering other nodes</li> </ul> <p> </p>"},{"location":"thehive/installation/kubernetes/#role-based-access-control-rbac","title":"Role-Based Access Control (RBAC)","text":"<p>Create a ServiceAccount named thehive with the necessary permissions to access the running pods:</p> <pre><code>---\n#\n# Create a role, `pod-reader`, that can list pods and\n# bind the default service account in the namespace\n# that the binding is deployed to to that role.\n#\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: pod-reader\nrules:\n  - apiGroups: [\"\"] # \"\" indicates the core API group\n    resources: [\"pods\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: thehive\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: read-pods\nsubjects:\n  - kind: ServiceAccount\n    name: thehive\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p> </p>"},{"location":"thehive/installation/kubernetes/#deployment","title":"Deployment","text":"<p>In your pod/deployment specification, specify the created service account. Also, ensure to add a label and a <code>POD_IP</code> environment variable.</p> <pre><code>metadata:\n  labels:\n    app: thehive\nspec:\n  serviceAccountName: thehive\n  containers:\n  - name: thehive\n    image: ...\n    env:\n      # Make sure that the container can know its own IP\n      - name: POD_IP\n        valueFrom:\n          fieldRef:\n            fieldPath: status.podIP\n</code></pre> <p> </p>"},{"location":"thehive/installation/kubernetes/#configuration","title":"Configuration","text":""},{"location":"thehive/installation/kubernetes/#using-docker-entrypoint","title":"Using Docker Entrypoint","text":"<p>If you use the Docker entry point, include the <code>--kubernetes</code> flag. Additionally, you can use the following options:</p> <pre><code>--kubernetes-pod-label-selector &lt;selector&gt;  | Selector to use to select other pods running the app (default app=thehive)\n--cluster-min-nodes-count &lt;count&gt;           | Minimum number of nodes to form a cluster (default to 1)\n</code></pre> <p> </p>"},{"location":"thehive/installation/kubernetes/#using-custom-applicationconf","title":"Using Custom application.conf","text":"<p>If you use your own application.conf file, add the following configurations:</p> <pre><code>akka.remote.artery.canonical.hostname = ${?POD_IP}\nsingleInstance = false\nakka.management {\n  cluster.bootstrap {\n    contact-point-discovery {\n      discovery-method = kubernetes-api\n      # Set the minimum number of pods to form a cluster\n      required-contact-point-nr = 1\n    }\n  }\n}\nakka.extensions += \"akka.management.cluster.bootstrap.ClusterBootstrap\"\n\nakka.discovery {\n  kubernetes-api {\n    # Set here the pod selector to use for thehive pods\n    pod-label-selector = \"thehive\"\n  }\n}\n</code></pre> <p> </p>"},{"location":"thehive/installation/kubernetes/#pod-probes","title":"Pod Probes","text":"<p>You can use the following probes to ensure the application starts and runs correctly. It's recommended to enable these probes after validating the correct start of the application.</p> <p>Tip</p> <p>When applying a large migration, deactivate these probes as the HTTP server will not start until the migration is complete.</p> <pre><code>startupProbe:\n    httpGet:\n        path: /api/v1/status/public\n        port: 9000\n    failureThreshold: 30\n    periodSeconds: 10\nlivenessProbe:\n    httpGet:\n        path: /api/v1/status/public\n        port: 9000\n    periodSeconds: 10\n</code></pre>"},{"location":"thehive/installation/kubernetes/#cleanup","title":"Cleanup","text":"<p>To delete all resources belonging to the <code>thehive</code> namespace, use the following command:</p> <pre><code>kubectl delete namespace thehive\n</code></pre>"},{"location":"thehive/installation/kubernetes/#troubleshooting","title":"Troubleshooting","text":"<p>Below are some common issues that may arise when running TheHive with Docker:</p> <ul> <li> <p>Example 1: Error during Database Initialization</p> <p>If your logs contain the following lines:</p> <pre><code>[error] o.t.s.m.Database [|] ***********************************************************************\n[error] o.t.s.m.Database [|] * Database initialization has failed. Restart application to retry it *\n[error] o.t.s.m.Database [|] ***********************************************************************\n</code></pre> <p>This indicates that an error occurred when attempting to create the database schema. Beneath these lines, you should find additional details regarding the cause of the error.</p> <p>Resolution:</p> <ol> <li> <p>Cassandra / Elasticsearch Unavailability: Ensure that both databases are running correctly and that TheHive can establish connections to them.</p> <p>You can try starting both databases in the Kubernetes cluster before initiating TheHive by setting TheHive deployment to <code>replicas: 0</code>.</p> </li> <li> <p>Invalid Data in Cassandra / Elasticsearch: Elasticsearch acts as an index for Cassandra, and if the data between the two becomes unsynchronized, errors may occur when accessing the data.</p> <p>If this is the first setup of the cluster, consider deleting both database volumes/data and restarting both the databases and TheHive.</p> </li> </ol> </li> </ul> <p> </p>"},{"location":"thehive/installation/migration/","title":"Migration from TheHive 3.x","text":""},{"location":"thehive/installation/migration/#migration-from-thehive-3x","title":"Migration from TheHive 3.x","text":"<p>This documentation outlines the supported versions, prerequisites, configuration steps, and the migration process.</p> <p>The migration from TheHive 3.x to TheHive 5.x involves transferring data stored in Elasticsearch. TheHive 5.x is provided with a tool to help you migrate your data.</p> <p>The migration tool is located in <code>/opt/thehive/bin/migrate</code>. </p>"},{"location":"thehive/installation/migration/#supported-versions","title":"Supported Versions","text":"<p>The migration tool facilitates the transition from both TheHive 3.4.x and 3.5.x versions. Below is the compatibility matrix:</p> Migrating from Elasticsearch Version TheHive 3.4.x v6.x TheHive 3.5.x v7.x"},{"location":"thehive/installation/migration/#pre-requisites","title":"Pre-requisites","text":"<p>Before initiating the migration, ensure the following:</p> <ul> <li> <p>TheHive v5.x must be installed on the system where the migration tool will run.</p> </li> <li> <p>TheHive must be properly configured, including database, index, and file storage settings.</p> </li> <li> <p>Stop the <code>thehive</code> service on the target server using the command: service thehive stop.</p> </li> <li> <p>Ensure the migration tool has access to the Elasticsearch database used by TheHive 3.x and the configuration file of TheHive 3.x instance.</p> </li> </ul>"},{"location":"thehive/installation/migration/#configuration-of-thehive","title":"Configuration of TheHive","text":""},{"location":"thehive/installation/migration/#user-domain-configuration","title":"User Domain Configuration","text":"<p>Users in TheHive are identified by their email addresses. To migrate users from TheHive 3, a domain must be appended to usernames. By default, TheHive v5.x includes a domain named <code>thehive.local</code>. Starting the migration without explicitly specifying a domain name will result in migrating all users with a username formatted like  <code>user@thehive.local</code>. </p> <p>To specify a custom domain, update the <code>auth.defaultUserDomain</code> setting in the configuration file (<code>/etc/thehive/application.conf</code>):</p> <pre><code>    auth.defaultUserDomain: \"mydomain.com\"\n</code></pre> <p>This ensures that imported users from TheHive 3.x are formatted as <code>user@mydomain.com</code>.</p>"},{"location":"thehive/installation/migration/#running-the-migration","title":"Running the Migration","text":"<p>Follow the steps below to execute the migration:</p> <ol> <li> <p>Prepare, install, and configure TheHive v5.x as per the associated guides</p> </li> <li> <p>Ensure TheHive 5 is not running before initiating the migration for optimal performance.</p> </li> <li> <p>Execute the <code>migrate</code> command:</p> </li> </ol> <pre><code>  /opt/thehive/bin/migrate\n</code></pre> <p>Warning</p> <p>The migration tool works on a single node only, thus the configuration file must not contain cluster configuration (<code>akka.cluster</code>).</p> <p>Info</p> <p>It is recommended to execute this program under the user responsible for running TheHive service (thehive if you are installing the application using DEB or RPM packages).</p> <p> </p>"},{"location":"thehive/installation/migration/#thehive-migration-tool-options","title":"TheHive Migration Tool Options","text":"<p>The migration tool for TheHive offers a comprehensive set of options to facilitate the migration process. Below is a detailed list of available options:</p> <ul> <li><code>-v, --version</code>: Displays the version of the migration tool.</li> <li><code>-h, --help</code>: Displays the help message detailing the usage of the migration tool.</li> <li><code>-l, --logger-config &lt;file&gt;</code>: Specifies the path to the logback configuration file.</li> <li><code>-c, --config &lt;file&gt;</code>: Specifies the global configuration file for TheHive.</li> <li><code>-i, --input &lt;file&gt;</code>: Specifies the path to the configuration file of TheHive 3.</li> <li><code>-o, --output &lt;file&gt;</code>: Specifies the path to the configuration file for TheHive 5.</li> <li><code>-d, --drop-database</code>: Drops TheHive 5 database before migration.</li> <li><code>-r, --resume</code>: Resumes migration or migrates on an existing database.</li> <li><code>-m, --main-organisation &lt;organisation&gt;</code>: Specifies the main organization for migration.</li> <li><code>-u, --es-uri http://ip1:port,ip2:port</code>: Specifies the Elasticsearch URIs for TheHive 3.</li> <li><code>-x, --es-index &lt;index&gt;</code>: Specifies the Elasticsearch index name for TheHive 3.</li> <li><code>-x, --es-index-version &lt;index&gt;</code>: Specifies the version number for the Elasticsearch index name (default: autodetect).</li> <li><code>-a, --es-keepalive &lt;duration&gt;</code>: Specifies the Elasticsearch keepalive duration.</li> <li><code>-p, --es-pagesize &lt;value&gt;</code>: Specifies the page size for Elasticsearch queries.</li> <li><code>-s, --es-single-type &lt;bool&gt;</code>: Specifies whether Elasticsearch uses a single type.</li> <li><code>-y, --transaction-pagesize &lt;value&gt;</code>: Specifies the page size for each transaction.</li> <li><code>-t, --thread-count &lt;value&gt;</code>: Specifies the number of threads for migration.</li> <li><code>-k, --integrity-checks</code>: Runs integrity checks after migration.</li> <li><code>--max-case-age &lt;duration&gt;</code>: Migrates cases younger than the specified duration.</li> <li><code>--min-case-age &lt;duration&gt;</code>: Migrates cases older than the specified duration.</li> <li><code>--case-from-date &lt;date&gt;</code>: Migrates cases created from the specified date.</li> <li><code>--case-until-date &lt;date&gt;</code>: Migrates cases created until the specified date.</li> <li><code>--case-from-number &lt;number&gt;</code>: Migrates cases starting from the specified case number.</li> <li><code>--case-until-number &lt;number&gt;</code>: Migrates cases up to the specified case number.</li> <li><code>--max-alert-age &lt;duration&gt;</code>: Migrates alerts younger than the specified duration.</li> <li><code>--min-alert-age &lt;duration&gt;</code>: Migrates alerts older than the specified duration.</li> <li><code>--alert-from-date &lt;date&gt;</code>: Migrates alerts created from the specified date.</li> <li><code>--alert-until-date &lt;date&gt;</code>: Migrates alerts created until the specified date.</li> <li><code>--include-alert-types &lt;type&gt;,&lt;type&gt;...</code>: Migrates only alerts with the specified types.</li> <li><code>--exclude-alert-types &lt;type&gt;,&lt;type&gt;...</code>: Excludes alerts with the specified types from migration.</li> <li><code>--include-alert-sources &lt;source&gt;,&lt;source&gt;...</code>: Migrates only alerts with the specified sources.</li> <li><code>--exclude-alert-sources &lt;source&gt;,&lt;source&gt;...</code>: Excludes alerts with the specified sources from migration.</li> <li><code>--max-audit-age &lt;duration&gt;</code>: Migrates audits younger than the specified duration.</li> <li><code>--min-audit-age &lt;duration&gt;</code>: Migrates audits older than the specified duration.</li> <li><code>--audit-from-date &lt;date&gt;</code>: Migrates audits created from the specified date.</li> <li><code>--audit-until-date &lt;date&gt;</code>: Migrates audits created until the specified date.</li> <li><code>--include-audit-actions &lt;value&gt;</code>: Migrates only audits with the specified actions (Update, Creation, Delete).</li> <li><code>--exclude-audit-actions &lt;value&gt;</code>: Excludes audits with the specified actions from migration.</li> <li><code>--include-audit-objectTypes &lt;value&gt;</code>: Migrates only audits with the specified object types (case, case_artifact, case_task, etc.).</li> <li><code>--exclude-audit-objectTypes &lt;value&gt;</code>: Excludes audits with the specified object types from migration.</li> <li><code>--case-number-shift &lt;value&gt;</code>: Transposes case numbers by adding the specified value.</li> </ul> <p> </p>"},{"location":"thehive/installation/migration/#usage-examples","title":"Usage Examples","text":"<ul> <li>Import Cases/Alerts not older than X days/hours.</li> <li>Import Cases/Alerts with specific ID numbers.</li> <li>Import a portion of the Audit trail.</li> <li>...</li> </ul>"},{"location":"thehive/installation/migration/#basic-migration-command","title":"Basic Migration Command","text":"<p>To migrate data to a new instance of TheHive, use the following command:</p> <pre><code>/opt/thehive/bin/migrate \\\n  --output /etc/thehive/application.conf \\\n  --main-organisation myOrganisation \\\n  --es-uri http://ELASTICSEARCH_IP_ADDRESS:9200 \\\n  --es-index the_hive\n</code></pre> <p>Options Description:</p> <ul> <li>--output: Specifies the configuration file path for TheHive 5, which must include database and file storage configurations.</li> <li>--main-organisation: Specifies the organization to create during migration.</li> <li>--es-uri: Specifies the URL of the Elasticsearch server. If Elasticsearch authentication is enabled, a configuration file for TheHive3 (--input) is required.</li> <li>--es-index: Specifies the Elasticsearch index used for migration.</li> </ul> Option Description <code>--output</code> Specifies the configuration file path for TheHive 5, which must include database and file storage configurations; <code>--main-organisation</code> Specifies the organization to create during migration; <code>--es-uri</code> Specifies the URL of the Elasticsearch server. If Elasticsearch authentication is enabled, a configuration file for TheHive3 (--input) is required; <code>--es-index</code> Specifies the Elasticsearch index used for migration; <p>Info</p> <p>The migration process duration varies significantly based on data volume, ranging from several hours to days. It is strongly advised not to start TheHive application during migration to ensure data integrity.</p>"},{"location":"thehive/installation/migration/#resuming-an-incomplete-migration","title":"Resuming an Incomplete Migration","text":"<p>If your migration process has been interrupted or only a portion of the data has been migrated, you can resume the migration using the tool with the <code>--resume</code> parameter. This parameter ensures that data is not duplicated if it already exists in the destination system.</p>"},{"location":"thehive/installation/migration/#merging-multiple-thehive-3-data-into-one-thehive-5-instance","title":"Merging Multiple TheHive 3 Data into One TheHive 5 Instance","text":"<p>The migration tool supports multiple executions to merge different TheHive 3 datasets into a single TheHive 5 instance. Each migration execution can specify a different target organization. To avoid conflicts in case numbers, where a case with the same number already exists, you can use the <code>--case-number-shift</code> parameter to adjust the case numbers accordingly.</p>"},{"location":"thehive/installation/migration/#using-authentication-on-cassandra","title":"Using Authentication on Cassandra","text":"<p>If you're utilizing a dedicated account on Cassandra to access TheHive 4 data, ensure that the user has permissions to create keyspaces in the database.</p> <pre><code>GRANT CREATE on ALL KEYSPACES to username;\n</code></pre>"},{"location":"thehive/installation/migration/#migration-logs","title":"Migration Logs","text":"<p>During the migration process, the tool generates logs to provide insights into the progress. By default, a log is generated approximately every 10 seconds, detailing various aspects of the migration, including the status of cases, alerts, and other entities.</p> <pre><code>[info] o.t.t.m.Migrate - [Migrate cases and alerts] CaseTemplate/Task:32 Organisation:1/1 Case/Task:160/201 Case:31/52 Job:103/138 ObservableType:3/17 Alert:25/235 Audit:3207/2986 CaseTemplate:6/6 Alert/Observable:700(52ms) Case/Observable:1325/1665 User:9/9 CustomField:13/13 Case/Task/Log:20/27\n</code></pre> <p>Please note that the numbers of observables, cases, and other entities are estimations and may not represent exact values due to the complexity of computation involved.</p> <p>Files from MISP imported with TheHive 2.13 and earlier</p> <p>It is important to notice that migrating Cases/Alerts containing MISP event that were imported with TheHive 2.13 (Sept 2017) or older will cause observable files not being imported in TheHive 5. </p> <p>Indeed, until this version, TheHive referenced the file to the <code>AttributeId</code> in MISP and was not automatically downloaded. It then could generate a log like this: </p> <pre><code>[warn] o.t.t.m.t.Input - Pre 2.13 file observables are ignored in MISP alert ffa3a8503ab0cd4f99fc6937a8e9b827\n</code></pre>"},{"location":"thehive/installation/migration/#starting-thehive","title":"Starting TheHive","text":"<p>Once the migration process has successfully completed, you can start TheHive. However, during the initial startup, data indexing occurs, and the service may not be immediately available. This indexing process may take some time.</p> <p>Warning</p> <p>During the initial startup, refrain from stopping or restarting TheHive service until the indexing process is complete to ensure data integrity and service availability.</p> <p> </p>"},{"location":"thehive/installation/step-by-step-installation-guide/","title":"Step-by-Step Installation Guide","text":""},{"location":"thehive/installation/step-by-step-installation-guide/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>This article provides a comprehensive installation and configuration guide to set up an instance of TheHive. The guide offers detailed instructions accompanied by examples for systems based on DEB and RPM packages, as well as for installation from binary packages.</p> <p>Note: Installation for a new instance of TheHive only is covered in this guide.</p>"},{"location":"thehive/installation/step-by-step-installation-guide/#dependencies","title":"Dependencies","text":"<p>Before proceeding with the installation, ensure that the following programs are already installed on your system:</p> DEBRPM <ol> <li>Open a terminal window.</li> <li> <p>Run the following command to install the necessary dependencies:</p> <pre><code>apt install wget gnupg apt-transport-https git ca-certificates ca-certificates-java curl software-properties-common python3-pip lsb-release\n</code></pre> </li> </ol> <ol> <li>Open a terminal window.</li> <li> <p>Execute the following command to install the required dependencies:</p> <pre><code>yum install pkg-install gnupg chkconfig python3-pip git\n</code></pre> </li> </ol> <p>Ensure that all dependencies are successfully installed before proceeding with the TheHive installation process.</p>"},{"location":"thehive/installation/step-by-step-installation-guide/#java-virtual-machine","title":"Java Virtual Machine","text":"<p>Important Note:</p> <ul> <li>For security and long-term support, it is mandatory to use Amazon Corretto builds, which are OpenJDK builds provided and maintained by Amazon.</li> <li>Java version 8 is no longer supported.</li> </ul> DEBRPMOther <ol> <li>Open a terminal window.</li> <li> <p>Execute the following commands:</p> <pre><code>wget -qO- https://apt.corretto.aws/corretto.key | sudo gpg --dearmor -o /usr/share/keyrings/corretto.gpg\necho \"deb [signed-by=/usr/share/keyrings/corretto.gpg] https://apt.corretto.aws stable main\" | sudo tee -a /etc/apt/sources.list.d/corretto.sources.list\nsudo apt update\nsudo apt install java-common java-11-amazon-corretto-jdk\necho JAVA_HOME=\"/usr/lib/jvm/java-11-amazon-corretto\" | sudo tee -a /etc/environment\nexport JAVA_HOME=\"/usr/lib/jvm/java-11-amazon-corretto\"\n</code></pre> </li> <li> <p>Verify the installation by running:</p> <pre><code>java -version\n</code></pre> </li> <li> <p>You should see output similar to the following:</p> <pre><code>openjdk version \"11.0.12\" 2022-07-19\nOpenJDK Runtime Environment Corretto-11.0.12.7.1 (build 11.0.12+7-LTS)\nOpenJDK 64-Bit Server VM Corretto-11.0.12.7.1 (build 11.0.12+7-LTS, mixed mode)\n</code></pre> </li> </ol> <ol> <li>Open a terminal window.</li> <li> <p>Execute the following commands:</p> <pre><code>sudo rpm --import https://yum.corretto.aws/corretto.key &amp;&gt; /dev/null\nwget -qO- https://yum.corretto.aws/corretto.repo | sudo tee -a /etc/yum.repos.d/corretto.repo\nyum install java-1.11.0-amazon-corretto-devel &amp;&gt; /dev/null\necho JAVA_HOME=\"/usr/lib/jvm/java-11-amazon-corretto\" | sudo tee -a /etc/environment\nexport JAVA_HOME=\"/usr/lib/jvm/java-11-amazon-corretto\"\n</code></pre> </li> <li> <p>Verify the installation by running:</p> <pre><code>java -version\n</code></pre> </li> <li> <p>You should see output similar to the following:</p> <pre><code>openjdk version \"11.0.12\" 2022-07-19\nOpenJDK Runtime Environment Corretto-11.0.12.7.1 (build 11.0.12+7-LTS)\nOpenJDK 64-Bit Server VM Corretto-11.0.12.7.1 (build 11.0.12+7-LTS, mixed mode)\n</code></pre> </li> </ol> <p>If you are using a system other than DEB or RPM, please consult your system documentation for instructions on installing Java 11.</p>"},{"location":"thehive/installation/step-by-step-installation-guide/#apache-cassandra","title":"Apache Cassandra","text":"<p>Apache Cassandra is a highly scalable and robust database system. TheHive is fully compatible with Apache Cassandra's latest stable release version 4.0.x.</p> <p>Upgrading from Cassandra 3.x</p> <p>The information provided in this guide pertains specifically to fresh installations. If you are currently using Cassandra 3.x and considering an upgrade, we recommend referring to the dedicated guide. </p> <p> </p>"},{"location":"thehive/installation/step-by-step-installation-guide/#installation","title":"Installation","text":"DEBRPMOther Installation Methods <ol> <li> <p>Add Apache Cassandra repository references</p> <ul> <li>Download Apache Cassandra repository keys using the following command:</li> </ul> <pre><code>wget -qO -  https://downloads.apache.org/cassandra/KEYS | sudo gpg --dearmor  -o /usr/share/keyrings/cassandra-archive.gpg\n</code></pre> <ul> <li>Add the repository to your system by appending the following line to the <code>/etc/apt/sources.list.d/cassandra.sources.list</code> file. This file may not exist, and you may need to create it.</li> </ul> <pre><code>echo \"deb [signed-by=/usr/share/keyrings/cassandra-archive.gpg] https://debian.cassandra.apache.org 40x main\" |  sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list \n</code></pre> </li> <li> <p>Install the package</p> <ul> <li>Once the repository references are added, update your package index and install Cassandra using the following commands:</li> </ul> <pre><code>sudo apt update\nsudo apt install cassandra\n</code></pre> </li> </ol> <ol> <li> <p>Add Cassandra repository keys</p> <ul> <li>To add Cassandra repository keys, execute the following command:</li> </ul> <pre><code>rpm --import https://downloads.apache.org/cassandra/KEYS\n</code></pre> </li> <li> <p>Add the Apache repository for Cassandra to /etc/yum.repos.d/cassandra.repo</p> <ul> <li>To add the Apache repository configuration for Cassandra, you need to create a new file named <code>/etc/yum.repos.d/cassandra.repo</code> and add the following content to it:</li> </ul> <pre><code>[cassandra]\nname=Apache Cassandra\nbaseurl=https://redhat.cassandra.apache.org/40x/\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://downloads.apache.org/cassandra/KEYS\n</code></pre> <p>Note</p> <p>You can create the file and add the content using a text editor like nano or vim. </p> </li> <li> <p>Install the package</p> <ul> <li>After adding the repository configuration, install Cassandra using the following command:</li> </ul> <pre><code>sudo yum install cassandra\n</code></pre> </li> </ol> <p>Download the tar.gz archive from Apache Cassandra Downloads and extract it into the folder of your choice. You can use utilities like <code>wget</code> to download the archive.</p> <p>By default, data is stored in <code>/var/lib/cassandra</code>. Ensure appropriate permissions are set for this directory to avoid any issues with data storage and access.</p> <p> </p>"},{"location":"thehive/installation/step-by-step-installation-guide/#configuration","title":"Configuration","text":"<p>You can configure Cassandra by modifying settings within the <code>/etc/cassandra/cassandra.yaml</code> file.</p> <p>1.Locate the Cassandra Configuration File:</p> <p>Navigate to the directory containing the Cassandra configuration file <code>/etc/cassandra/</code>.</p> <p>2.Edit the <code>cassandra.yaml</code> File:</p> <p>Open the <code>cassandra.yaml</code> file in a text editor with appropriate permissions.</p> <p>3.Configure Cluster Name:</p> <p>Set the <code>cluster_name</code> parameter to the desired name. This name helps identify the Cassandra cluster.</p> <p>4.Configure Listen Address:</p> <p>Set the <code>listen_address</code> parameter to the IP address of the node within the cluster. This address is used by other nodes within the cluster to communicate.</p> <p>5.Configure RPC Address:</p> <p>Set the <code>rpc_address</code> parameter to the IP address of the node to enable clients to connect to the Cassandra cluster.</p> <p>6.Configure Seed Provider:</p> <p>Ensure the <code>seed_provider</code> section is properly configured. The <code>seeds</code> parameter should contain the IP address(es) of the seed node(s) in the cluster.</p> <p>7.Configure Directories:</p> <p>Set the directories for data storage, commit logs, saved caches, and hints as per your requirements. Ensure that the specified directories exist and have appropriate permissions.</p> <p>8.Save the Changes:</p> <p>After making the necessary configurations, save the changes to the <code>cassandra.yaml</code> file.</p> /etc/cassandra/cassandra.yaml<pre><code># content from /etc/cassandra/cassandra.yaml\n[..]\ncluster_name: 'thp'\nlisten_address: 'xx.xx.xx.xx' # address for nodes\nrpc_address: 'xx.xx.xx.xx' # address for clients\nseed_provider:\n    - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n    parameters:\n        # Ex: \"&lt;ip1&gt;,&lt;ip2&gt;,&lt;ip3&gt;\"\n        - seeds: 'xx.xx.xx.xx' # self for the first node\ndata_file_directories:\n- '/var/lib/cassandra/data'\ncommitlog_directory: '/var/lib/cassandra/commitlog'\nsaved_caches_directory: '/var/lib/cassandra/saved_caches'\nhints_directory: \n- '/var/lib/cassandra/hints'\n[..]\n</code></pre> <p> </p>"},{"location":"thehive/installation/step-by-step-installation-guide/#start-the-service","title":"Start the service","text":"DEBRPM <ol> <li> <p>Start the Service</p> <ul> <li>Execute the following command to start the Cassandra service:</li> </ul> <pre><code>sudo systemctl start cassandra\n</code></pre> </li> <li> <p>Ensure Service Restarts After Reboot:</p> <ul> <li>Enable the Cassandra service to restart automatically after a system reboot:</li> </ul> <pre><code>sudo systemctl enable cassandra\n</code></pre> </li> <li> <p>(Optional) Remove Existing Data Before Starting</p> <ul> <li>If the Cassandra service was started automatically before configuring it, it's recommended to stop it, remove existing data, and restart it once the configuration is updated. Execute the following commands:</li> </ul> <pre><code>sudo systemctl stop cassandra\nsudo rm -rf /var/lib/cassandra/*\n</code></pre> </li> </ol> <ol> <li> <p>Start the Service</p> <ul> <li>Start the Cassandra service by running:</li> </ul> <pre><code>sudo systemctl daemon-reload\nsudo service cassandra start\n</code></pre> </li> <li> <p>Ensure Service Restarts After Reboot</p> <ul> <li>Enable the Cassandra service to restart automatically after a system reboot:</li> </ul> <pre><code>sudo systemctl enable cassandra\n</code></pre> </li> </ol> <p>Note</p> <p>Cassandra defaults to listening on port 7000/tcp for inter-node communication and port 9042/tcp for client communication.</p>"},{"location":"thehive/installation/step-by-step-installation-guide/#elasticsearch","title":"Elasticsearch","text":"<p>Elasticsearch is a robust data indexing and search engine. It is used by TheHive to manage data indices efficiently.</p> <p>Note</p> <p>From Version 5.3, TheHive supports Elasticsearch 8.0 and 7.x. Previous TheHive versions only support Elasticsearch 7.x.</p> <p>Note</p> <p>Starting from TheHive 5.3, for advanced use-cases, OpenSearch is also supported.</p> <p> </p>"},{"location":"thehive/installation/step-by-step-installation-guide/#installation_1","title":"Installation","text":"DEBRPMOther Installation Methods <ol> <li> <p>Add Elasticsearch repository references</p> <ul> <li>To add Elasticsearch repository keys, execute the following command:</li> </ul> <pre><code>wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch |  sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg\nsudo apt-get install apt-transport-https\n</code></pre> <ul> <li>Add the repository to your system by appending the following line to the /etc/apt/sources.list.d/elastic-7.x.list file. This file may not exist, and you may need to create it</li> </ul> <pre><code>echo \"deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/7.x/apt stable main\" |  sudo tee /etc/apt/sources.list.d/elastic-7.x.list \n</code></pre> </li> <li> <p>Install the package</p> <ul> <li>Once the repository references are added, update your package index and install Elasticsearch using the following commands:</li> </ul> <pre><code>sudo apt update\nsudo apt install elasticsearch\n</code></pre> </li> </ol> <ol> <li> <p>Add Elasticsearch repository references</p> <ul> <li>To add Elasticsearch repository keys, execute the following command:</li> </ul> <pre><code>rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch\n</code></pre> </li> <li> <p>Add the RPM repository of Elasticsearch to <code>/etc/yum.repos.d/elasticsearch.repo</code></p> <ul> <li>To add the repository configuration for Elasticsearcg, you need to create a new file named <code>/etc/yum.repos.d/elasticsearch.repo</code> and add the following content to it:</li> </ul> /etc/yum.repos.d/elasticsearch.repo<pre><code>[elasticsearch]\nname=Elasticsearch repository for 7.x packages\nbaseurl=https://artifacts.elastic.co/packages/7.x/yum\ngpgcheck=1\ngpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch\nenabled=0\nautorefresh=1\ntype=rpm-md\n</code></pre> </li> <li> <p>Install the package</p> <ul> <li>After adding the repository configuration, install Elasticsearch using the following command:</li> </ul> <pre><code>sudo yum install --enablerepo=elasticsearch elasticsearch\n</code></pre> </li> </ol> <p>Please refer to the official Elasticsearch documentation website for the most up-to-date instructions: https://www.elastic.co/guide/en/elasticsearch/reference/current/rpm.html</p> <p>Download the tar.gz archive from http://cassandra.apache.org/download/ and extract it into the folder of your choice. You can use utilities like wget to download the archive.</p> <p> </p>"},{"location":"thehive/installation/step-by-step-installation-guide/#configuration_1","title":"Configuration","text":"<p>You can configure Elasticsearch by modifying settings within the <code>/etc/elasticsearch/elasticsearch.yml</code> file.</p> <p>1. Locate the Elasticsearch Configuration File:</p> <ul> <li>Navigate to the directory containing the Elasticsearch configuration file <code>/etc/elasticsearch/</code>.</li> </ul> <p>2. Edit the elasticsearch.yml File:</p> <ul> <li>Open the <code>elasticsearch.yml</code> file in a text editor with appropriate permissions.</li> </ul> <p>3. Configure HTTP and Transport Hosts:</p> <ul> <li>Set the <code>http.host</code> and <code>transport.host</code> parameters to <code>127.0.0.1</code> or the desired IP address.</li> </ul> <p>4. Configure Cluster Name:</p> <ul> <li>Set the <code>cluster.name</code> parameter to the desired name. This name helps identify the Elasticsearch cluster.</li> </ul> <p>5. Configure Thread Pool Search Queue Size:</p> <ul> <li>Set the <code>thread_pool.search.queue_size</code> parameter to the desired value, such as <code>100000</code>.</li> </ul> <p>6. Configure Paths for Logs and Data:</p> <ul> <li>Set the <code>path.logs</code> and <code>path.data</code> parameters to the desired directories, such as <code>\"/var/log/elasticsearch\"</code> and <code>\"/var/lib/elasticsearch\"</code>, respectively.</li> </ul> <p>7. Configure X-Pack Security (Optional):</p> <ul> <li>If you're not using X-Pack security, ensure that <code>xpack.security.enabled</code> is set to <code>false</code>.</li> </ul> <p>8. Configure Script Allowed Types (Optional):</p> <ul> <li>If needed, set the <code>script.allowed_types</code> parameter to specify allowed script types.</li> </ul> <p>9. Save the Changes:</p> <ul> <li>After making the necessary configurations, save the changes to the <code>elasticsearch.yml</code> file.</li> </ul> <p>10. Custom JVM Options:</p> <ul> <li>Create the file <code>/etc/elasticsearch/jvm.options.d/jvm.options</code> if it doesn't exist.</li> </ul> <p>11. Custom JVM Options:</p> <ul> <li> <p>Inside <code>jvm.options</code>, add the desired JVM options, such as:</p> <pre><code>-Dlog4j2.formatMsgNoLookups=true\n-Xms4g\n-Xmx4g\n</code></pre> <p>Adjust the memory settings (<code>-Xms</code> and <code>-Xmx</code>) according to the available memory.</p> </li> </ul> /etc/elasticsearch/elasticsearch.yml<pre><code>http.host: 127.0.0.1\ntransport.host: 127.0.0.1\ncluster.name: hive\nthread_pool.search.queue_size: 100000\npath.logs: \"/var/log/elasticsearch\"\npath.data: \"/var/lib/elasticsearch\"\nxpack.security.enabled: false\nscript.allowed_types: \"inline,stored\"\n</code></pre> <p>Info</p> <ul> <li>Index creation occurs during TheHive's initial startup, which may take some time to complete.</li> <li>Similar to data and files, indexes should be included in the backup policy to ensure their preservation.</li> <li>Indexes can be removed and re-created as needed.</li> </ul> <p> </p>"},{"location":"thehive/installation/step-by-step-installation-guide/#sart-the-service","title":"Sart the service","text":"DEBRPM <ol> <li> <p>Start the Service</p> <ul> <li>Execute the following command to start the Elasticsearch service:</li> </ul> <pre><code>sudo systemctl start elasticsearch\n</code></pre> </li> <li> <p>Ensure Service Restarts After Reboot:</p> <ul> <li>Enable the Elasticsearch service to restart automatically after a system reboot:</li> </ul> <pre><code>sudo systemctl enable elasticsearch\n</code></pre> </li> <li> <p>(Optional) Remove Existing Data Before Starting</p> <ul> <li>If the Elasticsearch service was started automatically before configuring it, it's recommended to stop it, remove existing data, and restart it once the configuration is updated. Execute the following commands:</li> </ul> <pre><code>sudo systemctl stop elasticsearch\nsudo rm -rf /var/lib/elasticsearch/*\n</code></pre> </li> </ol> <ol> <li> <p>Start the Service</p> <ul> <li>Start the Elasticsearch service by running:</li> </ul> <pre><code>sudo systemctl daemon-reload\nsudo service elasticsearch start\n</code></pre> </li> <li> <p>Ensure Service Restarts After Reboot</p> <ul> <li>Enable the Elasticsearch service to restart automatically after a system reboot:</li> </ul> <pre><code>sudo systemctl enable elasticsearch\n</code></pre> </li> </ol>"},{"location":"thehive/installation/step-by-step-installation-guide/#file-storage","title":"File Storage","text":"<p>For standalone production and test servers, we recommend using the local filesystem. However, if you are considering building a cluster with TheHive, there are several possible solutions available, including NFS or S3 services. For further details and an example involving MinIO servers, please refer to the related guide.</p> Local FilesystemS3 with Min.io <p>To utilize the local filesystem for file storage, begin by selecting a dedicated folder. By default, this folder is located at <code>/opt/thp/thehive/files</code>:</p> <pre><code>sudo mkdir -p /opt/thp/thehive/files\n</code></pre> <p>This path will be utilized in the configuration of TheHive. After installing TheHive, it's important to ensure that the user thehive owns the chosen path for storing files:</p> <pre><code>chown -R thehive:thehive /opt/thp/thehive/files\n</code></pre> <p>Detailed documentation on the installation, configuration, and usage of Min.IO can be found in this documentation.</p>"},{"location":"thehive/installation/step-by-step-installation-guide/#thehive-installation-and-configuration","title":"TheHive Installation and Configuration","text":"<p>This section provides detailed instructions for installing and configuring TheHive.</p> <p> </p>"},{"location":"thehive/installation/step-by-step-installation-guide/#installation_2","title":"Installation","text":"<p>All required packages are available on our package repository. We support Debian and RPM packages, as well as binary packages in zip format. All packages are signed using our GPG key 562CBC1C with the fingerprint <code>0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C</code>.</p> DEBRPM <p>For Debian systems, use the following commands:</p> <pre><code>wget -O- https://raw.githubusercontent.com/StrangeBeeCorp/Security/main/PGP%20keys/packages.key | sudo gpg --dearmor -o /usr/share/keyrings/strangebee-archive-keyring.gpg\n</code></pre> <p>For RPM-based systems, follow these steps:</p> <pre><code>sudo rpm --import https://raw.githubusercontent.com/StrangeBeeCorp/Security/main/PGP%20keys/packages.key\n</code></pre> <p>Install TheHive package by using the following commands:</p> DEBRPMOther Installation Methods <pre><code>echo 'deb [arch=all signed-by=/usr/share/keyrings/strangebee-archive-keyring.gpg] https://deb.strangebee.com thehive-5.3 main' |sudo tee -a /etc/apt/sources.list.d/strangebee.list\nsudo apt-get update\nsudo apt-get install -y thehive\n</code></pre> <ol> <li> <p>Import the RPM repository key:</p> <pre><code>sudo rpm --import https://raw.githubusercontent.com/StrangeBeeCorp/Security/main/PGP%20keys/packages.key\n</code></pre> </li> <li> <p>Create and edit the file /etc/yum.repos.d/strangebee.repo:</p> /etc/yum.repos.d/strangebee.repo<pre><code>[thehive]\nenabled=1\npriority=1\nname=StrangeBee RPM repository\nbaseurl=https://rpm.strangebee.com/thehive-5.3/noarch\ngpgkey=https://raw.githubusercontent.com/StrangeBeeCorp/Security/main/PGP%20keys/packages.key\ngpgcheck=1\n</code></pre> </li> <li> <p>Then install the package using <code>yum</code>:</p> <pre><code>sudo yum install thehive\n</code></pre> </li> </ol> <p>If you prefer a binary package, follow these steps:</p> <ol> <li> <p>Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under <code>/opt</code>.</p> <pre><code>cd /opt\nwget https://archives.strangebee.com/zip/thehive-latest.zip\nunzip thehive-latest.zip\nsudo ln -s thehive-x.x.x thehive\n</code></pre> </li> <li> <p>Prepare the system. It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in <code>/opt/thehive/logs</code>.</p> <pre><code>sudo addgroup thehive\nsudo adduser --system thehive\nsudo chown -R thehive:thehive /opt/thehive\nsudo mkdir /etc/thehive\nsudo touch /etc/thehive/application.conf\nsudo chown root:thehive /etc/thehive\nsudo chgrp thehive /etc/thehive/application.conf\nsudo chmod 640 /etc/thehive/application.conf\n</code></pre> </li> <li> <p>Copy the systemd script in <code>/etc/systemd/system/thehive.service</code>.</p> <pre><code>cd /tmp\nwget https://raw.githubusercontent.com/TheHive-Project/TheHive/master/package/thehive.service\nsudo cp thehive.service /etc/systemd/system/thehive.service\n</code></pre> </li> </ol> <p> </p>"},{"location":"thehive/installation/step-by-step-installation-guide/#configuration_2","title":"Configuration","text":"<p>The setup provided with binary packages is tailored for a standalone installation, with all components hosted on the same server. At this point, it's crucial to fine-tune the following parameters as necessary:</p> /etc/thehive/application.conf<pre><code>[..]\n# Service configuration\napplication.baseUrl = \"http://localhost:9000\" # (1)\nplay.http.context = \"/\"                       # (2)\n[..]\n</code></pre> <ol> <li> Define the scheme, hostname, and port for accessing the application</li> <li> Indicate if a custom path is being used (default is /)</li> </ol> <p>The following configurations are necessary for successful initiation of TheHive:</p> <ul> <li>Secret key configuration</li> <li>Database configuration</li> <li>File storage configuration</li> </ul> <p> </p>"},{"location":"thehive/installation/step-by-step-installation-guide/#secret-key-configuration","title":"Secret key configuration","text":"DEBRPMOther <p>The secret key is automatically generated and stored in <code>/etc/thehive/secret.conf</code> during package installation.</p> <p>The secret key is automatically generated and stored in <code>/etc/thehive/secret.conf</code> during package installation.</p> <p>To set up a secret key, execute the following command:</p> <pre><code>cat &gt; /etc/thehive/secret.conf &lt;&lt; _EOF_\nplay.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\"\n_EOF_\n</code></pre>"},{"location":"thehive/installation/step-by-step-installation-guide/#database-index","title":"Database &amp; index","text":"<p>By default, TheHive is configured to connect to local Cassandra and Elasticsearch databases.</p> /etc/thehive/application.conf<pre><code># Database and index configuration\n# By default, TheHive is configured to connect to local Cassandra 4.x and a\n# local Elasticsearch services without authentication.\ndb.janusgraph {\nstorage {\n    backend = cql\n    hostname = [\"127.0.0.1\"]\n    # Cassandra authentication (if configured)\n    # username = \"thehive\"\n    # password = \"password\"\n    cql {\n    cluster-name = thp\n    keyspace = thehive\n    }\n}\nindex.search {\n    backend = elasticsearch\n    hostname = [\"127.0.0.1\"]\n    index-name = thehive\n}\n}\n</code></pre>"},{"location":"thehive/installation/step-by-step-installation-guide/#file-storage_1","title":"File storage","text":"<p>The default file storage location of TheHive is <code>/opt/thp/thehive/files</code>.</p> Local filesystemS3 <p>If you decide to store files on the local filesystem:</p> <ol> <li> <p>Ensure thehive user has permissions on the destination folder:</p> <pre><code>chown -R thehive:thehive /opt/thp/thehive/files\n</code></pre> </li> <li> <p>Default values in the configuration file </p> /etc/thehive/application.conf<pre><code># Attachment storage configuration\n# By default, TheHive is configured to store files locally in the folder.\n# The path can be updated and should belong to the user/group running thehive service. (by default: thehive:thehive)\nstorage {\nprovider = localfs\nlocalfs.location = /opt/thp/thehive/files\n}\n</code></pre> </li> </ol> <p>If you opt for MinIO and an S3 object storage system to store files in a filesystem, append the following lines to TheHive configuration file (/etc/thehive/application.conf):</p> /etc/thehive/application.conf<pre><code>## Storage configuration\nstorage {\n    provider: s3\n    s3 {\n    bucket = \"thehive\"\n    readTimeout = 1 minute\n    writeTimeout = 1 minute\n    chunkSize = 1 MB\n    endpoint = \"http://&lt;IP_ADDRESS&gt;:9100\"\n    accessKey = \"&lt;MINIO ACCESS KEY&gt;\"\n    secretKey = \"&lt;MINIO SECRET KEY&gt;\"\n    region = \"us-east-1\"\n    }\n}\nalpakka.s3.path-style-access = force\n</code></pre>"},{"location":"thehive/installation/step-by-step-installation-guide/#cortex-misp","title":"Cortex &amp; MISP","text":"<p>The initial configuration file packaged with the software contains the following lines, which enable the Cortex and MISP modules by default. If you're not utilizing either of these modules, you can simply comment out the corresponding line and restart the service.</p> /etc/thehive/application.conf<pre><code># Additional modules\n#\n# TheHive is strongly integrated with Cortex and MISP.\n# Both modules are enabled by default. If not used, each one can be disabled by\n# ommenting the configuration line.\nscalligraph.modules += org.thp.thehive.connector.cortex.CortexModule\nscalligraph.modules += org.thp.thehive.connector.misp.MispModule\n</code></pre> <p> </p>"},{"location":"thehive/installation/step-by-step-installation-guide/#run","title":"Run","text":"<p>To start TheHive service and enable it to run on system boot, execute the following commands in your terminal:</p> <pre><code>sudo systemctl start thehive\nsudo systemctl enable thehive\n</code></pre> <p>Please be aware that the service may take some time to start initially.</p> <p>After the service has successfully started, launch your web browser and navigate to <code>http://YOUR_SERVER_ADDRESS:9000/</code></p> <p>The default admin user credentials are as follows:</p> <pre><code>Username: admin@thehive.local\nPassword: secret\n</code></pre> <p>For security reasons, it is strongly advised to change the default password after logging in.</p>"},{"location":"thehive/installation/step-by-step-installation-guide/#advanced-configuration","title":"Advanced Configuration","text":"<p>For further customization options, please consult the Configuration &amp; Operations section.</p> <p>To configure HTTPS, follow the instructions on the dedicated page.</p> <p> </p>"},{"location":"thehive/installation/system-requirements/","title":"System Requirements","text":""},{"location":"thehive/installation/system-requirements/#hardware-requirements","title":"Hardware Requirements","text":"<p>The hardware requirements for TheHive depend on factors such as the number of concurrent users (including integrations) and their usage patterns. Below are recommended hardware thresholds for hosting all services on the same machine:</p> Number of Users TheHive Cassandra Elasticsearch  &lt; 10 2  / 2 GB  2  / 2 GB  2  / 2 GB   &lt; 20 2-4  / 4 GB  2-4  / 4 GB  2-4  / 4 GB   &lt; 50 4-6  / 8 GB  4-6  / 8 GB  4-6  / 8 GB  <p>Note</p> <p>When deploying all services on the same server, it's recommended to have at least 4 cores and 16 GB of RAM. Additionally, ensure that <code>jvm.options</code> is configured appropriately for Elasticsearch.</p>"},{"location":"thehive/installation/system-requirements/#operating-system","title":"Operating System","text":"<p>TheHive has been tested and is officially supported on the following operating systems:</p> <ul> <li>Ubuntu 20.04 LTS &amp; 22.04 LTS</li> <li>Debian 11</li> <li>RHEL 8</li> <li>Fedora 35 &amp; 37</li> </ul> <p>Additionally, an official Docker image is available for users who prefer containerized deployments.</p> <p> </p>"},{"location":"thehive/installation/upgrade-from-4.x/","title":"Upgrade from Version 4.x","text":""},{"location":"thehive/installation/upgrade-from-4.x/#upgrade-from-thehive-4x","title":"Upgrade from TheHive 4.x","text":"<p>This guide provides comprehensive instructions for upgrading TheHive from version 4.1.x to 5.0.x. Please ensure that your system meets the following requirements:</p> <ul> <li>The application is running on a supported Linux operating system.</li> <li>The server meets prerequisites regarding CPU &amp; RAM.</li> </ul> <p>If you are using a cluster setup, specific notes are provided to guide you through the process.</p>"},{"location":"thehive/installation/upgrade-from-4.x/#important-considerations","title":"Important Considerations","text":"<p>Switch to Elasticsearch as indexing engine: TheHive 5.x utilizes Elasticsearch as the indexing engine. If you were using Lucene as the indexing engine with TheHive 4.1.x, reindexing the data is mandatory. Please note that this process may take some time depending on the size of your database.</p>"},{"location":"thehive/installation/upgrade-from-4.x/#preparation","title":"Preparation","text":"I'm Using a Cluster <p>Please ensure that the instructions under this section are followed on all nodes of the cluster.</p> <p> </p>"},{"location":"thehive/installation/upgrade-from-4.x/#database-backup","title":"Database Backup","text":"<p>Before proceeding with the upgrade, ensure to back up the following components:</p> <ul> <li>Database</li> <li>Index</li> <li>Files</li> </ul> <p>For detailed instructions on how to perform backups, refer to our backup and restore guide.</p> <p> </p>"},{"location":"thehive/installation/upgrade-from-4.x/#ensure-admin-user-access","title":"Ensure Admin User Access","text":"<p>Ensure that you can log in as an admin user with a password in TheHive database. By default, the local auth provider should be enabled.</p> <p> </p>"},{"location":"thehive/installation/upgrade-from-4.x/#stop-all-running-applications","title":"Stop all Running Applications","text":"<ol> <li> <p>Start by stopping TheHive:</p> <pre><code>sudo systemctl stop thehive\n</code></pre> </li> <li> <p>Once TheHive is successfully stopped, stop the database service:</p> <pre><code>sudo systemctl stop cassandra\n</code></pre> </li> <li> <p>If already using Elasticsearch as the indexing engine, stop the Elasticsearch service:</p> <pre><code>sudo systemctl stop elasticsearch\n</code></pre> </li> </ol>"},{"location":"thehive/installation/upgrade-from-4.x/#upgrade-java","title":"Upgrade Java","text":"I'm Using a Cluster <p>Please ensure that the instructions under this section are followed on all nodes of the cluster.</p> <p>Follow the installation process to install the required version of Java.</p>"},{"location":"thehive/installation/upgrade-from-4.x/#upgrade-or-install-elasticsearch","title":"Upgrade or Install Elasticsearch","text":"I'm Using a Cluster <p>Elasticsearch is crucial for TheHive 5.x clusters. However, if an update isn't urgently required, focus on upgrading Cassandra first.</p> <p>Elasticsearch is mandatory for TheHive 5.x clusters. Follow the installation process to install and configure the required version.</p>"},{"location":"thehive/installation/upgrade-from-4.x/#upgrade-cassandra","title":"Upgrade Cassandra","text":"I'm Using a Cluster <p>For each node within the Cassandra cluster, it is essential to follow this procedure. Ensure that all nodes in the Cassandra cluster are successfully restarted before proceeding with the upgrade of all nodes in TheHive cluster to version 5.</p>"},{"location":"thehive/installation/upgrade-from-4.x/#backup-configuration-file","title":"Backup Configuration File","text":"<p>Save the existing configuration file for Cassandra 3.x. It will be used later to configure Cassandra 4:</p> <pre><code>  sudo cp /etc/cassandra/cassandra.yaml /etc/cassandra/cassandra3.yaml.bak\n</code></pre> <p> </p>"},{"location":"thehive/installation/upgrade-from-4.x/#install-cassandra","title":"Install Cassandra","text":"<p>Follow the installation process to install the required version. During the installation process, replace existing configuration files as necessary.</p> <p> </p>"},{"location":"thehive/installation/upgrade-from-4.x/#configuration","title":"Configuration","text":"<p>Update the new configuration file and ensure the following parameters are correctly set with these values:</p> <pre><code>cluster_name: 'thp'\nnum_tokens: 256\n</code></pre> <p>Info</p> <p>If you have a customized configuration file for Cassandra 3.x, it is advisable to carefully review the entire file and make any necessary adjustments to ensure compatibility and proper functioning.</p> <p> </p>"},{"location":"thehive/installation/upgrade-from-4.x/#start-the-service","title":"Start the Service","text":"<p>Use the following command to start the Cassandra service:</p> <pre><code>sudo systemctl start cassandra\n</code></pre> <p> </p>"},{"location":"thehive/installation/upgrade-from-4.x/#upgrade-sstables","title":"Upgrade SSTables","text":"<p>On each Cassandra node, upgrade the SSTables:</p> <pre><code>nodetool upgradesstables\n</code></pre> <p>Then repair the keyspaces:</p> <pre><code>nodetool repair --full\n</code></pre>"},{"location":"thehive/installation/upgrade-from-4.x/#install-thehive","title":"Install TheHive","text":""},{"location":"thehive/installation/upgrade-from-4.x/#preparing-for-the-new-installation","title":"Preparing for the New Installation","text":"I'm using a cluster <p>Before initiating the installation process, it is crucial to ensure that your Cassandra cluster is fully operational. Follow these steps:</p> <ul> <li>Run the command <code>nodetool status</code> to check the status of your Cassandra cluster.</li> </ul> <pre><code>  nodetool status\n</code></pre> <p>The output should display information about the nodes in your cluster, including their status, load, tokens, and other relevant details.</p> Example output<pre><code># nodetool status\n\nDatacenter: datacenter1\n=======================\n\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address      Load      Tokens  Owns (effective)  Host ID                               Rack\nUN  10.1.1.2  1.41 GiB  256     100.0%            ba6daa4e-6d14-4b21-a06c-d01b3bdd659d  rack1\nUN  10.1.1.3  1.39 GiB  256     100.0%            201ab99c-8e16-49b1-9b66-5444043eb1cd  rack1\nUN  10.1.1.4  1.36 GiB  256     100.0%            a79c9a8c-c99b-4d74-8e78-6b0c252aeb86  rack1\n</code></pre> <ul> <li> <p>Ensure that all nodes in the cluster are in an operational state (UN), indicating that they are up and running normally.</p> </li> <li> <p>Before proceeding with the installation, it's recommended to perform the following steps:</p> <p>Stop Existing Nodes: Stop all existing nodes of TheHive (4.x).</p> <p>Upgrade and Start a Single Node: Begin by upgrading and starting only one node to TheHive 5.0.0. Verify that everything functions correctly with this node before proceeding further.</p> <p>Update and Start Other Nodes: Once the initial node is successfully upgraded and operational, proceed to update and start the remaining nodes.</p> </li> </ul> <p>TheHive configuration file: /etc/thehive/application.conf</p> <p>Starting from TheHive 5.0.0, the configuration process has been simplified, with most administration parameters configurable directly within the user interface (UI). The configuration file (/etc/thehive/application.conf) should only contain essential information required for the successful startup of the application, including:</p> <ul> <li>Secret</li> <li>Database</li> <li>Indexing Engine</li> <li>File Storage</li> <li>Enabled Connectors </li> <li>Akka Configuration (for clusters)</li> </ul> <p>Authentication, Webhooks, Cortex, and MISP configurations can now be conveniently set within the UI. </p> <p>!!! \"Note on Configuration Changes\"     Please note the following changes in configuration keys:</p> <pre><code>- The configuration keys for Cortex and MISP connector modules have been renamed from play.modules.enabled to scalligraph.modules. Update your configuration files accordingly to reflect these changes.\n</code></pre> Standalone serverCluster <ul> <li>Save your current configuration file:</li> </ul> <pre><code>sudo cp /etc/thehive/application.conf /etc/thehive/application.conf.bak\n</code></pre> <ul> <li>For the current scenario, which involves a standalone server, the ultimate configuration file should resemble the following:</li> </ul> sample of /etc/thehive/application.conf<pre><code># TheHive configuration - application.conf\n#\n#\n# This is the default configuration file.\n# This is prepared to run with all services locally:\n# - Cassandra for the database\n# - Elasticsearch for index engine\n# - File storage is local in /opt/thp/thehive/files\n#\n# If this is not your setup, please refer to the documentation at:\n# https://docs.thehive-project.org/thehive/\n#\n#\n# Secret key - used by Play Framework\n# If TheHive is installed with DEB/RPM package, this is automatically generated\n# If TheHive is not installed from DEB or RPM packages run the following\n# command before starting thehive:\n#   cat &gt; /etc/thehive/secret.conf &lt;&lt; _EOF_\n#   play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 |#   head -n 1)\"\n#   _EOF_\ninclude \"/etc/thehive/secret.conf\"\n\n\n# Database and index configuration\n# By default, TheHive is configured to connect to local Cassandra 4.x and a\n# local Elasticsearch services without authentication.\ndb.janusgraph {\n  storage {\n    backend = cql\n    hostname = [\"127.0.0.1\"]\n    # Cassandra authentication (if configured)\n    # username = \"thehive\"\n    # password = \"password\"\n    cql {\n      cluster-name = thp\n      keyspace = thehive\n    }\n  }\n  index.search {\n    backend = elasticsearch\n    hostname = [\"127.0.0.1\"]\n    index-name = thehive\n  }\n}\n\n# Attachment storage configuration\n# By default, TheHive is configured to store files locally in the folder.\n# The path can be updated and should belong to the user/group running thehive service. (by default: thehive:thehive)\nstorage {\n  provider = localfs\n  localfs.location = /opt/thp/thehive/files\n}\n\n# Define the maximum size for an attachment accepted by TheHive\nplay.http.parser.maxDiskBuffer = 1GB\n# Define maximum size of http request (except attachment)\nplay.http.parser.maxMemoryBuffer = 10M\n\n# Service configuration\napplication.baseUrl = \"http://localhost:9000\"\nplay.http.context = \"/\"\n\n# Additional modules\n#\n# TheHive is strongly integrated with Cortex and MISP.\n# Both modules are enabled by default. If not used, each one can be disabled by\n# commenting the configuration line.\nscalligraph.modules += org.thp.thehive.connector.cortex.CortexModule\nscalligraph.modules += org.thp.thehive.connector.misp.MispModule\n</code></pre> <ul> <li>Save your current configuration file:</li> </ul> <pre><code>sudo cp /etc/thehive/application.conf /etc/thehive/application.conf.bak\n</code></pre> <ul> <li>The second configuration includes settings for setting up TheHive in a clustered environment. It extends upon the first one with additional settings for cluster configuration using Akka:</li> </ul> sample of /etc/thehive/application.conf<pre><code># TheHive configuration - application.conf\n#\n#\n# This is the default configuration file.\n# This is prepared to run with all services locally:\n# - Cassandra for the database\n# - Elasticsearch for index engine\n# - File storage is local in /opt/thp/thehive/files\n#\n# If this is not your setup, please refer to the documentation at:\n# https://docs.thehive-project.org/thehive/\n#\n#\n# Secret key - used by Play Framework\n# If TheHive is installed with DEB/RPM package, this is automatically generated\n# If TheHive is not installed from DEB or RPM packages run the following\n# command before starting thehive:\n#   cat &gt; /etc/thehive/secret.conf &lt;&lt; _EOF_\n#   play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 |#   head -n 1)\"\n#   _EOF_\ninclude \"/etc/thehive/secret.conf\"\n\n\n# Database and index configuration\n# By default, TheHive is configured to connect to local Cassandra 4.x and a\n# local Elasticsearch services without authentication.\ndb.janusgraph {\n  storage {\n    backend = cql\n    hostname = [\"127.0.0.1\"]\n    # Cassandra authentication (if configured)\n    # username = \"thehive\"\n    # password = \"password\"\n    cql {\n      cluster-name = thp\n      keyspace = thehive\n    }\n  }\n  index.search {\n    backend = elasticsearch\n    hostname = [\"127.0.0.1\"]\n    index-name = thehive\n  }\n}\n\n# Attachment storage configuration\n# By default, TheHive is configured to store files locally in the folder.\n# The path can be updated and should belong to the user/group running thehive service. (by default: thehive:thehive)\nstorage {\n  provider = localfs\n  localfs.location = /opt/thp/thehive/files\n}\n\n# Define the maximum size for an attachment accepted by TheHive\nplay.http.parser.maxDiskBuffer = 1GB\n# Define maximum size of http request (except attachment)\nplay.http.parser.maxMemoryBuffer = 10M\n\n# Service configuration\napplication.baseUrl = \"http://localhost:9000\"\nplay.http.context = \"/\"\n\n# Additional modules\n#\n# TheHive is strongly integrated with Cortex and MISP.\n# Both modules are enabled by default. If not used, each one can be disabled by\n# commenting the configuration line.\nscalligraph.modules += org.thp.thehive.connector.cortex.CortexModule\nscalligraph.modules += org.thp.thehive.connector.misp.MispModule\n\n# Cluster configuration\nakka {\n  cluster.enable = on\n  actor {\n    provider = cluster\n  }\nremote.artery {\n  canonical {\n    hostname = \"&lt;My IP address&gt;\"\n    port = 2551\n  }\n}\n## seed node list contains at least one active node\ncluster.seed-nodes = [\n                      \"akka://application@&lt;Node 1 IP address&gt;:2551\",\n                      \"akka://application@&lt;Node 2 IP address&gt;:2551\",\n                      \"akka://application@&lt;Node 3 IP address&gt;:2551\"\n                    ]\n}\n</code></pre> <p>Note</p> <p>By default, both Cortex and MISP modules are enabled in TheHive. If you do not intend to use one or both of these modules, you can comment out the corresponding lines in the configuration file.</p> <p>Recommendation: It's advisable to utilize the default configuration sample provided, customize it with your specific parameter values, and retain the original file for configuring services through the web UI.</p> <p> </p>"},{"location":"thehive/installation/upgrade-from-4.x/#specific-configuration-for-upgrade-only","title":"Specific Configuration for Upgrade Only","text":"I'm using a cluster <p>This section pertains solely to the initial node, which will initiate the database and index upgrade process.</p> <p>These lines are to be included in the configuration file exclusively during the upgrade to version 5 and should be subsequently removed thereafter.</p> <pre><code>db.janusgraph.forceDropAndRebuildIndex = true\n</code></pre> <p> </p>"},{"location":"thehive/installation/upgrade-from-4.x/#installing-thehive","title":"Installing TheHive","text":"<p>If you're utilizing DEB packages for TheHive installation, follow these steps:</p> DEBRPM <ol> <li>Update Repository Address: Run the following commands to update the repository address:</li> </ol> <pre><code>wget -O- https://raw.githubusercontent.com/StrangeBeeCorp/Security/main/PGP%20keys/packages.key | sudo gpg --dearmor -o /usr/share/keyrings/strangebee-archive-keyring.gpg\nsudo rm /etc/apt/sources.list.d/thehive-project.list ; echo 'deb [arch=all signed-by=/usr/share/keyrings/strangebee-archive-keyring.gpg] https://deb.strangebee.com thehive-5.3 main' | sudo tee -a /etc/apt/sources.list.d/strangebee.list\n</code></pre> <ol> <li>Install the New Package: Execute the following commands to update and install the new package. This will automatically remove the old package of thehive4:</li> </ol> <pre><code>sudo apt update\nsudo apt install thehive\n</code></pre> <ol> <li> <p>Add Cassandra Repository Keys: Import the Cassandra repository keys with the following command:</p> <pre><code>sudo rpm --import https://raw.githubusercontent.com/StrangeBeeCorp/Security/main/PGP%20keys/packages.key\n</code></pre> </li> <li> <p>Configure RPM Repository: Create and edit the file /etc/yum.repos.d/strangebee.repo with the following content:</p> /etc/yum.repos.d/strangebee.repo<pre><code>[thehive]\nenabled=1\npriority=1\nname=StrangeBee RPM repository\nbaseurl=https://rpm.strangebee.com/thehive-5.3/noarch\ngpgkey=https://raw.githubusercontent.com/StrangeBeeCorp/Security/main/PGP%20keys/packages.key\ngpgcheck=1\n</code></pre> </li> <li> <p>Install TheHive Package: Install the package using yum:</p> <pre><code>sudo yum install thehive\n</code></pre> </li> </ol> <p> </p>"},{"location":"thehive/installation/upgrade-from-4.x/#starting-services","title":"Starting Services","text":"<p>Ensure that the required services are started for TheHive to function properly. Follow these steps:</p> <ol> <li>Reload Systemd Daemon - Execute the following command to reload the systemd daemon:</li> </ol> <pre><code>sudo systemctl daemon-reload\n</code></pre> <ol> <li>Start Cassandra (if not already started) - If Cassandra is not already running, start it with:</li> </ol> <pre><code>sudo systemctl start cassandra\n</code></pre> <ol> <li>Start Elasticsearch (if not already started) - If Elasticsearch is not running, start it using:</li> </ol> <pre><code>sudo systemctl start elasticsearch\n</code></pre> <ol> <li>Start TheHive -  Once both database services are running, start TheHive by executing:</li> </ol> <pre><code>sudo systemctl start thehive\n</code></pre> <p>Note</p> <p>The first start of TheHive 5.x may take some time as it updates the database schema and proceeds with reindexing. Progress can be monitored in the log file <code>/var/log/thehive/application.log</code>. Refer to the troubleshooting section for further assistance.</p> <p> </p>"},{"location":"thehive/installation/upgrade-from-4.x/#restarting-the-service","title":"Restarting the Service","text":"<p>After successfully starting the service, follow these steps to update the configuration file and restart TheHive:</p> <ol> <li>Update Configuration File - Remove the following lines from the configuration file <code>/etc/thehive/application.conf</code>:</li> </ol> <pre><code>db.janusgraph.forceDropAndRebuildIndex = true\n</code></pre> <ol> <li>Restart TheHive - Restart TheHive using the following command:</li> </ol> <pre><code>sudo systemctl restart thehive\n</code></pre> I'm using a cluster <p>If you're deploying TheHive in a cluster, you can proceed to install and start TheHive on all other nodes following similar steps.</p>"},{"location":"thehive/installation/upgrade-from-4.x/#troubleshooting","title":"Troubleshooting","text":"<p>During the update, few logs can be seen in TheHive <code>application.log</code> file. </p> <p>Example of logs and what they mean</p> <pre><code>[INFO] from org.janusgraph.graphdb.database.management.GraphIndexStatusWatcher in application-akka.actor.default-dispatcher-11 [|] Some key(s) on index global2 do not currently have status(es) [REGISTERED, ENABLED]: dateValue=INSTALLED,externalLink=INSTALLED,origin=INSTALLED,patternId=INSTALLED,revoked=INSTALLED,mandatory=INSTALLED,content=INSTALLED,isAttachment=INSTALLED,writable=INSTALLED,tactic=INSTALLED,stringValue=INSTALLED,owningOrganisation=INSTALLED,permissions=INSTALLED,actionRequired=INSTALLED,integerValue=INSTALLED,details=INSTALLED,locked=INSTALLED,slug=INSTALLED,cortexId=INSTALLED,owner=INSTALLED,workerId=INSTALLED,apikey=INSTALLED,level=INSTALLED,floatValue=INSTALLED,version=INSTALLED,occurDate=INSTALLED,url=INSTALLED,report=INSTALLED,tactics=INSTALLED,booleanValue=INSTALLED,cortexJobId=INSTALLED,category=INSTALLED,workerName=INSTALLED\n</code></pre> TheHive install indexes of the new schema in the database <pre><code>[INFO] from org.janusgraph.graphdb.olap.job.IndexRepairJob in Thread-97 [|] Index global2 metrics: success-tx: 1 doc-updates: 100 succeeded: 100\n</code></pre> TheHive reindexes all data <pre><code>* UPDATE SCHEMA OF thehive-enterprise (1): Create initial values\n[INFO] from org.thp.scalligraph.models.Operations in application-akka.actor.default-dispatcher-11 [d471d8b643d17b6d|d88fe62679b77ab1] Adding initial values for GDPRDummy\n[..]\n[INFO] from org.thp.scalligraph.models.Operations in application-akka.actor.default-dispatcher-11 [|] Update graph in progress (100): Add pap and ignoreSimilarity to observables\n</code></pre> Migrating data from v4. to v5 <pre><code>[WARN] from org.thp.thehive.enterprise.services.LicenseSrv in main [ef39c95eaa6de532|0ccf187e40a4cd34] No license found\n</code></pre> No license found. This is a normal behavior during the upgrade from versions 4 to 5 <pre><code>INFO] from play.core.server.AkkaHttpServer in main [|] Listening for HTTP on /0:0:0:0:0:0:0:0:9000\n</code></pre> The service is available. Users/Administrators can log in <pre><code>[INFO] from org.thp.thehive.connector.cortex.services.CortexDataImportActor in application-akka.actor.default-dispatcher-16 [|] Analyzer templates already present (found 203), skipping\n[..]\n[INFO] from org.thp.thehive.services.ttp.PatternImportActor in application-akka.actor.default-dispatcher-14 [|] Import finished, 707 patterns imported\n</code></pre> Few operations are processed after making the service available, like installing MITRE Enterprise ATT&amp;CK patterns catalog or Analyzers templates. <pre><code>[ERROR] from org.janusgraph.diskstorage.log.util.ProcessMessageJob in pool-22-thread-1 [|] Encountered exception when processing message [Message@2022-03-24T16:50:40.655134Z:7f0001017672-ubuntu2=0x809F9F0568850528850550850558850570850600850610850618850650850668850710850738850758850760850808850900850910850A60850A70850A78850B00850B08853520853B3885150E8941608541688541788542088542688542708581] by reader [org.janusgraph.graphdb.database.management.ManagementLogger@3e1a6eae]:java.lang.IllegalStateException: Cannot access element because its enclosing transaction is closed and unbound\nat org.janusgraph.graphdb.transaction.StandardJanusGraphTx.getNextTx(StandardJanusGraphTx.java:380)\nat org.janusgraph.graphdb.vertices.AbstractVertex.it(AbstractVertex.java:61)\nat org.janusgraph.graphdb.relations.CacheVertexProperty.&lt;init&gt;(CacheVertexProperty.java:38)\nat org.janusgraph.graphdb.transaction.RelationConstructor.readRelation(RelationConstructor.java:88)\nat org.janusgraph.graphdb.transaction.RelationConstructor.readRelation(RelationConstructor.java:71)\nat org.janusgraph.graphdb.transaction.RelationConstructor$1.next(RelationConstructor.java:57)\nat org.janusgraph.graphdb.transaction.RelationConstructor$1.next(RelationConstructor.java:45)\nat org.janusgraph.graphdb.types.vertices.JanusGraphSchemaVertex.getDefinition(JanusGraphSchemaVertex.java:94)\nat org.janusgraph.graphdb.transaction.StandardJanusGraphTx.expireSchemaElement(StandardJanusGraphTx.java:1599)\nat org.janusgraph.graphdb.database.management.ManagementLogger.read(ManagementLogger.java:97)\nat org.janusgraph.diskstorage.log.util.ProcessMessageJob.run(ProcessMessageJob.java:46)\nat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\nat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\nat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\nat java.base/java.lang.Thread.run(Thread.java:829)\n</code></pre> During indexing, Janusgraph may display this message, this error is coming from a bug in janusgraph, don't mind it as the indexing will continue normally. This will have no impact on TheHive <p> </p>"},{"location":"thehive/installation/upgrade-from-5.x/","title":"Upgrade from Version 5.x","text":""},{"location":"thehive/installation/upgrade-from-5.x/#upgrade-from-thehive-5x","title":"Upgrade from TheHive 5.x","text":""},{"location":"thehive/installation/upgrade-from-5.x/#important-considerations","title":"Important Considerations","text":"<p>Before proceeding with the upgrade, please keep the following points in mind:</p> <ol> <li> <p>Database Backup: We strongly recommend performing a full database backup before upgrading. For detailed instructions on how to perform a backup, please refer to the backup instructions.</p> </li> <li> <p>Downgrade Limitation: Once upgraded to TheHive 5.3, your instance cannot be downgraded. This means reverting to a previous version of TheHive 5 will require restoring your data from the backup.</p> </li> <li> <p>When upgrading an existing TheHive 5.x instance, the first application launch will trigger a database evolution, including schema and data updates. This operation may take some time depending on your database size.</p> </li> <li> <p>Since version 5.1, TheHive no longer supports the Lucene backend as the index engine. Lucene was an option for handling data indexing with TheHive 4.1.x. To migrate your index to Elasticsearch, please follow the provided guide.</p> </li> </ol>"},{"location":"thehive/installation/upgrade-from-5.x/#overview","title":"Overview","text":"<p>This guide provides step-by-step instructions for upgrading an existing TheHive 5.x instance to TheHive 5.3.x.</p>"},{"location":"thehive/installation/upgrade-from-5.x/#upgrade-instructions","title":"Upgrade Instructions","text":"<p>TheHive 5.x deliverables are hosted in distinct package repositories. Depending on your installation method, follow the instructions below:</p> DEB Package (Debian/Ubuntu)RPM Package (Red Hat/CentOS)Docker <ol> <li> <p>(Optional) Install the package repository signature key, if not already installed:</p> <pre><code>wget -O- https://raw.githubusercontent.com/StrangeBeeCorp/Security/main/PGP%20keys/packages.key | sudo gpg --dearmor -o /usr/share/keyrings/strangebee-archive-keyring.gpg\n</code></pre> </li> <li> <p>Edit the file <code>/etc/apt/sources.list.d/strangebee.list</code> and adjust the repository address as follows:</p> <pre><code>deb [arch=all signed-by=/usr/share/keyrings/strangebee-archive-keyring.gpg] https://deb.strangebee.com thehive-5.3 main\n</code></pre> </li> <li> <p>Install TheHive package:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y thehive \n</code></pre> </li> </ol> <ol> <li> <p>(Optional) Install the package repository signature key, if not already installed:</p> <pre><code>sudo rpm --import https://raw.githubusercontent.com/StrangeBeeCorp/Security/main/PGP%20keys/packages.key\n</code></pre> </li> <li> <p>Edit the file <code>/etc/yum.repos.d/strangebee.repo</code> and adjust the repository address as follows:</p> <pre><code>[thehive]\nenabled=1\npriority=1\nname=StrangeBee RPM repository\nbaseurl=https://rpm.strangebee.com/thehive-5.3/noarch\ngpgkey=https://raw.githubusercontent.com/StrangeBeeCorp/Security/main/PGP%20keys/packages.key\ngpgcheck=1\n</code></pre> </li> <li> <p>Then install the package using yum:</p> <pre><code>sudo yum update\nsudo yum install thehive \n</code></pre> </li> </ol> <p>Update your existing TheHive 5.x Docker stack (docker-compose file or similar) using the image named <code>strangebee/thehive:5.3</code></p> <p>Important note</p> <p>Ensure that you update your Docker tags accordingly. The strangebee/thehive:latest tag is deprecated and remains associated with TheHive 5.0.x versions. A new strangebee/thehive:5.3 tag is now available and associated with the latest 5.3.x version.</p>"},{"location":"thehive/installation/upgrade-from-5.x/#health-checks","title":"Health Checks","text":"<p>If you have health checks on the application HTTP interface, they should be disabled during the upgrade process. Otherwise, the orchestrator may kill TheHive during the update process.</p> <p> </p>"},{"location":"thehive/installation/docker/docker-compose-networking/","title":"docker","text":""},{"location":"thehive/installation/docker/docker-compose-overview/","title":"Overview","text":""},{"location":"thehive/installation/docker/docker-compose-overview/#introduction-to-docker-and-docker-compose","title":"Introduction to Docker and Docker Compose","text":""},{"location":"thehive/installation/docker/docker-compose-overview/#what-is-docker","title":"What is Docker?","text":"<p>Docker is a platform that allows you to develop, ship, and run applications in containers. Containers are lightweight, portable, and ensure that your application works seamlessly in any environment. Here are some key points about Docker:</p> <ul> <li>Isolation: Containers isolate your application from the underlying system, ensuring consistency across different environments.</li> <li>Portability: Docker containers can run on any machine that has Docker installed, making it easy to move applications between development, testing, and production environments.</li> <li>Efficiency: Containers share the host system's kernel, making them more lightweight than traditional virtual machines.</li> </ul>"},{"location":"thehive/installation/docker/docker-compose-overview/#installing-docker","title":"Installing Docker","text":"<p>To install Docker on your system, follow these steps:</p> <ol> <li>Go to the Docker website and download Docker Desktop for your operating system.</li> <li>Follow the installation instructions for your OS.</li> <li>After installation, open Docker Desktop to ensure it is running correctly.</li> </ol>"},{"location":"thehive/installation/docker/docker-compose-overview/#basic-docker-commands","title":"Basic Docker Commands","text":"<p>Here are some basic Docker commands to get you started:</p> <ul> <li><code>docker pull &lt;image&gt;</code>: Downloads a Docker image from Docker Hub.</li> <li><code>docker run &lt;image&gt;</code>: Runs a container from the specified image.</li> <li><code>docker ps</code>: Lists all running containers.</li> <li><code>docker stop &lt;container_id&gt;</code>: Stops a running container.</li> <li><code>docker rm &lt;container_id&gt;</code>: Removes a stopped container.</li> <li><code>docker images</code>: Lists all downloaded Docker images.</li> </ul>"},{"location":"thehive/installation/docker/docker-compose-overview/#what-is-docker-compose","title":"What is Docker Compose?","text":"<p>Docker Compose is a tool that allows you to define and manage multi-container Docker applications. Using a YAML file, you can specify the services, networks, and volumes needed for your application. This makes it easy to manage and deploy complex applications with a single command.</p>"},{"location":"thehive/installation/docker/docker-compose-overview/#installing-docker-compose","title":"Installing Docker Compose","text":"<p>Docker Compose is included with Docker Desktop. If you need to install it separately, follow these steps:</p> <ol> <li>Go to the Docker Compose release page.</li> <li>Download the appropriate version for your operating system.</li> <li>Follow the installation instructions for your OS.</li> </ol>"},{"location":"thehive/installation/docker/docker-compose-overview/#basic-docker-compose-commands","title":"Basic Docker Compose Commands","text":"<p>Here are some basic Docker Compose commands to get you started:</p> <ul> <li><code>docker-compose up</code>: Builds, (re)creates, starts, and attaches to containers for a service.</li> <li><code>docker-compose down</code>: Stops and removes containers, networks, images, and volumes created by <code>docker-compose up</code>.</li> <li><code>docker-compose build</code>: Builds or rebuilds services.</li> <li><code>docker-compose ps</code>: Lists containers.</li> <li><code>docker-compose logs</code>: Shows logs of running services.</li> </ul>"},{"location":"thehive/installation/docker/docker-compose-overview/#example-docker-compose-file","title":"Example Docker Compose File","text":"<p>Below is an example of a simple <code>docker-compose.yml</code> file that defines a web application and a database service:</p> <pre><code>version: '3'\nservices:\n  web:\n    image: nginx:latest\n    ports:\n      - \"80:80\"\n  db:\n    image: postgres:latest\n    environment:\n      POSTGRES_PASSWORD: example\n</code></pre>"},{"location":"thehive/installation/docker/docker-compose-overview/#conclusion","title":"Conclusion","text":"<p>This guide provides a brief overview of Docker and Docker Compose. These tools are powerful and can greatly simplify the process of developing, shipping, and running applications. In the next sections, we will focus on how to use these tools specifically with TheHive.</p>"},{"location":"thehive/installation/docker/docker-compose-prod/","title":"docker","text":""},{"location":"thehive/installation/docker/docker-compose-test/","title":"docker","text":""},{"location":"thehive/installation/docker/docker/","title":"Running TheHive with Docker","text":""},{"location":"thehive/installation/docker/docker/#running-thehive-with-docker","title":"Running TheHive with Docker","text":"<p>Docker provides a convenient way to package, distribute, and run applications in lightweight containers, enabling seamless deployment across various environments. TheHive leverages Docker to offer users a straightforward method for deploying its platform for incident response and case management.</p> <p>TheHive fully supports Docker, allowing users to quickly deploy and manage their instance of the platform using Docker containers. By utilizing Docker images provided by TheHive project, users can streamline the setup process and focus on leveraging TheHive's powerful features for incident response and collaboration.</p>"},{"location":"thehive/installation/docker/docker/#quick-start","title":"Quick Start","text":"<p>To deploy TheHive (and Cortex) using Docker, follow these steps:</p> <ol> <li> <p>Prepare Docker Compose File: Create a Docker Compose file to orchestrate the deployment of TheHive and Cortex. You can use the following example as a starting point:</p> <pre><code>version: \"3\"\nservices:\n  thehive:\n    image: strangebee/thehive:5.2\n    depends_on:\n      - cassandra\n      - elasticsearch\n      - minio\n      - cortex\n    mem_limit: 1500m\n    ports:\n      - \"9000:9000\"\n    environment:\n      - JVM_OPTS=\"-Xms1024M -Xmx1024M\"\n    command:\n      - --secret\n      - \"mySecretForTheHive\"\n      - \"--cql-hostnames\"\n      - \"cassandra\"\n      - \"--index-backend\"\n      - \"elasticsearch\"\n      - \"--es-hostnames\"\n      - \"elasticsearch\"\n      - \"--s3-endpoint\"\n      - \"http://minio:9000\"\n      - \"--s3-access-key\"\n      - \"minioadmin\"\n      - \"--s3-secret-key\"\n      - \"minioadmin\"\n      - \"--s3-bucket\"\n      - \"thehive\"\n      - \"--s3-use-path-access-style\"\n      - \"--cortex-hostnames\"\n      - \"cortex\"\n      - \"--cortex-keys\"\n      # put cortex api key once cortex is bootstraped\n      - \"&lt;cortex_api_key&gt;\"\n\n  cassandra:\n    image: 'cassandra:4'\n    mem_limit: 1600m\n    ports:\n      - \"9042:9042\"\n    environment:\n      - MAX_HEAP_SIZE=1024M\n      - HEAP_NEWSIZE=1024M\n      - CASSANDRA_CLUSTER_NAME=TheHive\n    volumes:\n      - cassandradata:/var/lib/cassandra\n    restart: on-failure\n\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.12\n    mem_limit: 1500m\n    ports:\n      - \"9200:9200\"\n    environment:\n      - discovery.type=single-node\n      - xpack.security.enabled=false\n    volumes:\n      - elasticsearchdata:/usr/share/elasticsearch/data\n\n  minio:\n    image: quay.io/minio/minio\n    mem_limit: 512m\n    command: [\"minio\", \"server\", \"/data\", \"--console-address\", \":9090\"]\n    environment:\n      - MINIO_ROOT_USER=minioadmin\n      - MINIO_ROOT_PASSWORD=minioadmin\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - \"miniodata:/data\"\n\n  cortex:\n    image: thehiveproject/cortex:3.1.7\n    depends_on:\n      - elasticsearch\n    environment:\n      - job_directory=/tmp/cortex-jobs\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /tmp/cortex-jobs:/tmp/cortex-jobs\n    ports:\n      - \"9001:9001\"\n\nvolumes:\n  miniodata:\n  cassandradata:\n  elasticsearchdata:\n</code></pre> </li> <li> <p>Run Docker Compose: Execute the following command in the directory containing your Docker Compose file to start TheHive and Cortex:</p> <pre><code>docker-compose up -d\n</code></pre> </li> <li> <p>Access TheHive Web Interface: Once the containers are up and running, access TheHive web interface by navigating to <code>http://localhost:9000</code> in your web browser. You can now proceed with configuring and using TheHive for your incident response and case management needs.</p> </li> <li> <p>(Optional) Access Cortex Web Interface: Cortex web interface can be accessed by visiting <code>http://localhost:9001</code> in your browser. Cortex serves as an analysis engine for observables and can be integrated seamlessly with TheHive.</p> </li> </ol>"},{"location":"thehive/installation/docker/docker/#default-credentials","title":"Default Credentials","text":"<p>The default administrative credentials for TheHive are as follows:</p> <ul> <li>Username: <code>admin@thehive.local</code></li> <li>Password: <code>secret</code></li> </ul>"},{"location":"thehive/installation/docker/docker/#using-your-own-configuration-file","title":"Using Your Own Configuration File","text":"<p>You have the option to use your own configuration file when deploying TheHive. Below are the steps to do so:</p> <ol> <li> <p>Ensure you have a custom configuration file ready.</p> </li> <li> <p>Update your Docker Compose file or Docker run command to include the custom configuration file:</p> <pre><code>  thehive:\n    image: strangebee/thehive:&lt;version&gt;\n    depends_on:\n      - cassandra\n      - elasticsearch\n      - minio\n      - cortex\n    mem_limit: 1500m\n    ports:\n      - \"9000:9000\"\n    environment:\n      - JVM_OPTS=\"-Xms1024M -Xmx1024M\"\n    volumes:\n      - &lt;host_conf_folder&gt;:/data/conf\n    command:\n      - --no-config\n      - --config-file\n      - /data/conf/application.conf\n</code></pre> <pre><code>docker run --rm -p 9000:9000 -v &lt;host_conf_folder&gt;:/data/conf strangebee/thehive:&lt;version&gt; --no-config --config-file /data/conf/application.conf \n</code></pre> </li> <li> <p>Ensure that the <code>&lt;host_conf_folder&gt;</code> directory contains the <code>application.conf</code> file.</p> <p>Note</p> <p>To prevent conflict with your custom file, make sure to use the --no-config flag to instruct the entrypoint not to generate any configuration. Otherwise, the entry point will generate a default configuration that may conflict with your settings.</p> </li> </ol>"},{"location":"thehive/installation/docker/docker/#using-command-line-arguments","title":"Using Command Line Arguments","text":"<p>When deploying TheHive, it's recommended to utilize Cassandra and Elasticsearch for data storage, along with MinIO for file storage. You can pass the hostnames of your instances as arguments when running the Docker container:</p> <pre><code>docker run --rm -p 9000:9000 strangebee/thehive:&lt;version&gt; \\\n    --secret &lt;secret&gt;\n    --cql-hostnames &lt;cqlhost1&gt;,&lt;cqlhost2&gt;,...\n    --cql-username &lt;cqlusername&gt;\n    --cql-password &lt;cqlusername&gt;\n    --index-backend elasticsearch\n    --es-hostnames &lt;eshost1&gt;,&lt;eshost2&gt;,...\n    --s3-endpoint &lt;minio_endpoint&gt;\n    --s3-access-key &lt;minio_access_key&gt;\n    --s3-secret-key &lt;minio_secret_key&gt;\n</code></pre> <p>This command connects your Docker container to external Cassandra and Elasticsearch nodes while storing data files on MinIO. The container exposes TheHive on port 9000.</p>"},{"location":"thehive/installation/docker/docker/#all-options","title":"All Options","text":"<p>To view a list of all supported options for the Docker entry point, use the -h flag:</p> <pre><code>docker run --rm strangebee/thehive:&lt;version&gt; -h\n</code></pre> <p>The output will display available options, allowing you to configure TheHive according to your requirements.</p> <p>Available Options:</p> <ul> <li><code>--config-file &lt;file&gt;</code>: Specifies the path to the configuration file.</li> <li><code>--no-config</code>: Prevents TheHive from attempting to configure itself, including adding secrets and Elasticsearch settings.</li> <li><code>--no-config-secret</code>: Excludes the addition of a randomly generated secret from the configuration.</li> <li><code>--secret &lt;secret&gt;</code>: Sets the secret used to secure sessions.</li> <li><code>--show-secret</code>: Displays the generated secret.</li> <li><code>--no-config-db</code>: Disables automatic configuration of the database.</li> <li><code>--cql-hostnames &lt;host&gt;,&lt;host&gt;,...</code>: Resolves these hostnames to locate Cassandra instances.</li> <li><code>--cql-username &lt;username&gt;</code>: Specifies the username for the Cassandra database.</li> <li><code>--cql-password &lt;password&gt;</code>: Specifies the password for the Cassandra database.</li> <li><code>--no-cql-wait</code>: Skips waiting for Cassandra to become available.</li> <li><code>--bdb-directory &lt;path&gt;</code>: Defines the location of the local database if Cassandra is not used (default: /data/db).</li> <li><code>--index-backend</code>: Specifies the backend to use for index, either 'lucene' or 'elasticsearch' (default: lucene).</li> <li><code>--es-hostnames</code>: Specifies the Elasticsearch instances used for index.</li> <li><code>--es-index</code>: Specifies the Elasticsearch index name to be used (default: thehive).</li> <li><code>--no-config-storage</code>: Disables automatic configuration of storage.</li> <li><code>--storage-directory &lt;path&gt;</code>: Specifies the location of local storage if S3 is not used (default: /data/files).</li> <li><code>--s3-endpoint &lt;endpoint&gt;</code>: Specifies the endpoint of S3 or other object storage if used, with 's3.amazonaws.com' for AWS S3.</li> <li><code>--s3-region &lt;region&gt;</code>: Specifies the S3 region, optional for MinIO.</li> <li><code>--s3-bucket &lt;bucket&gt;</code>: Specifies the name of the bucket to use (default: thehive), which must already exist.</li> <li><code>--s3-access-key &lt;key&gt;</code>: Specifies the S3 access key (required for S3).</li> <li><code>--s3-secret-key &lt;key&gt;</code>: Specifies the S3 secret key (required for S3).</li> <li><code>--s3-use-path-access-style</code>: Sets this flag if using MinIO or another non-AWS S3 provider, defaulting to virtual host style.</li> <li><code>--no-config-cortex</code>: Excludes Cortex configuration.</li> <li><code>--cortex-proto &lt;proto&gt;</code>: Defines the protocol to connect to Cortex (default: http).</li> <li><code>--cortex-port &lt;port&gt;</code>: Defines the port to connect to Cortex (default: 9001).</li> <li><code>--cortex-hostnames &lt;host&gt;,&lt;host&gt;,...</code>: Resolves these hostnames to locate Cortex instances.</li> <li><code>--cortex-keys &lt;key&gt;,&lt;key&gt;,...</code>: Defines Cortex keys.</li> <li><code>--kubernetes</code>: Utilizes the Kubernetes API to join other nodes.</li> <li><code>--kubernetes-pod-label-selector &lt;selector&gt;</code>: Specifies the selector to use to select other pods running the app (default app=thehive).</li> <li><code>--cluster-min-nodes-count &lt;count&gt;</code>: Specifies the minimum number of nodes to form a cluster (default to 1).</li> <li><code>migrate &lt;param&gt; &lt;param&gt; ...</code>: Runs the migration tool.</li> <li><code>cloner &lt;param&gt; &lt;param&gt; ...</code>: Runs the cloner tool. </li> </ul>"},{"location":"thehive/installation/docker/docker/#usage-in-kubernetes","title":"Usage in Kubernetes","text":"<p>For instructions on how to deploy TheHive on Kubernetes, please refer to the dedicated page.</p>"},{"location":"thehive/installation/docker/docker/#additional-recommendations","title":"Additional Recommendations","text":"<p>When deploying TheHive using Docker, consider the following recommendations:</p> <ul> <li>Always create the bucket named \"thehive\" in MinIO.</li> <li>Change the default credentials and secrets to enhance security.</li> <li>It's highly recommended to set a specific version for your Docker image in production scenarios instead of relying on the <code>latest</code> tag. The <code>latest</code> tag refers to the latest 5.0.x version of TheHive.</li> </ul> <p>Lucene Index Deprecation</p> <p>Starting from version 5.1, TheHive no longer supports the Lucene index. Usage of the Lucene index may result in issues, especially when querying custom fields. Therefore, it's strongly advised to use Elasticsearch instead.</p> <p>Refer to this page for instructions on how to perform the update.</p>"},{"location":"thehive/installation/docker/docker/#troubleshooting","title":"Troubleshooting","text":"<p>Below are some common issues that may arise when running TheHive with Docker:</p> <ul> <li> <p>Example 1: Container Exited with Code 137</p> <p>If one of your containers exits with code 137, it indicates that it's using more memory than allowed by Docker. </p> <p>Adjust the values 1024M according to your specific memory requirements. This will help optimize memory usage and prevent the container from exiting with code 137.</p> <p>Resolution:</p> <p>To resolve this issue, you can increase the <code>mem_limit</code> parameter to allocate more memory for the container.</p> <p>For JVM-based applications such as TheHive, Cassandra, and Elasticsearch, you can further optimize memory usage by tuning the JVM parameters. Specifically for TheHive, you can utilize the JVM_OPTS environment variable to set the maximum heap size.</p> <p>Here's an example of setting JVM options for TheHive:</p> <pre><code>JVM_OPTS=\"-Xms1024M -Xmx1024M\"\n</code></pre> </li> </ul> <p> </p>"},{"location":"thehive/operations/backup-restore/","title":"Backup & Restore Operations","text":""},{"location":"thehive/operations/backup-restore/#backup-and-restore-guide","title":"Backup and Restore Guide","text":"<p>Warning</p> <p>This guide has only been tested on a single node Cassandra server.</p>"},{"location":"thehive/operations/backup-restore/#overview","title":"Overview","text":"<p>To successfully restore TheHive, the following data needs to be saved:</p> <ul> <li>The database</li> <li>Files</li> <li>Optionally, the index</li> </ul>"},{"location":"thehive/operations/backup-restore/#backing-up-the-index","title":"Backing Up the Index","text":""},{"location":"thehive/operations/backup-restore/#option-1-backup-only-the-data","title":"Option 1: Backup Only the Data","text":"<p>You can use Cassandra snapshots to back up the data of a Cassandra node. This can be done while the application is running, meaning there is no downtime. With this option, the index is not backed up and will be rebuilt from the data during restoration.</p> <p>If the index doesn't exist, it is built when TheHive starts.</p> <p>Pros:</p> <ul> <li>No downtime during backup</li> <li>Backups take less space</li> </ul> <p>Cons:</p> <ul> <li>Restoration can be lengthy (requires a full reindexation of the data)</li> </ul> <p> </p>"},{"location":"thehive/operations/backup-restore/#option-2-backup-the-data-and-the-index","title":"Option 2: Backup the Data and the Index","text":"<p>To ensure the data and the index are synchronized, TheHive must be stopped before backing up Cassandra and Elasticsearch.</p> <p>Pros:</p> <ul> <li>TheHive can be quickly restored from a backup</li> </ul> <p>Cons:</p> <ul> <li>Downtime of TheHive during the backup</li> <li>Backups take more space</li> </ul>"},{"location":"thehive/operations/backup-restore/#cassandra","title":"Cassandra","text":""},{"location":"thehive/operations/backup-restore/#prerequisites","title":"Prerequisites","text":"<p>To back up or export the database from Cassandra, the following information is required:</p> <ul> <li>Cassandra admin password</li> <li>Keyspace used by TheHive (default = <code>thehive</code>). This can be checked in the <code>application.conf</code> configuration file, in the database configuration under storage, cql, and <code>keyspace</code> attribute.</li> </ul> <p>Tip</p> <p>This information can be found in TheHive configuration:</p> <pre><code>db.janusgraph {\n    storage {\n        backend: cql\n        hostname: [\"127.0.0.1\"]\n        cql {\n            cluster-name: thp\n            keyspace: thehive\n        }\n    }\n}\n</code></pre> <p> </p>"},{"location":"thehive/operations/backup-restore/#backup","title":"Backup","text":"<p>Following actions should be performed to backup the data successfully: </p> <ol> <li>Create a snapshot</li> <li>Save the data</li> </ol> <p> </p>"},{"location":"thehive/operations/backup-restore/#create-a-snapshot-and-an-archive","title":"Create a snapshot and an archive","text":"<p>Considering that your keyspace is <code>${KEYSPACE}</code> (<code>thehive</code> by default) and <code>${BACKUP}</code> is the name of the snapshot, run the following commands:</p> <ol> <li> <p>Before taking snapshots</p> <pre><code>nodetool cleanup ${KEYSPACE}\n</code></pre> </li> <li> <p>Take a snapshot</p> <pre><code>nodetool snapshot ${KEYSPACE} -t ${BACKUP}\n</code></pre> </li> <li> <p>Create and archive with the snapshot data: </p> <pre><code>tar cjf backup.tbz /var/lib/cassandra/data/${KEYSPACE}/*/snapshots/${BACKUP}/\n</code></pre> </li> <li> <p>Remove old snapshots (if necessary)</p> <pre><code>nodetool -h localhost -p 7199 clearsnapshot -t ${BACKUP}\n</code></pre> </li> </ol> <p> </p>"},{"location":"thehive/operations/backup-restore/#example","title":"Example","text":"<p>Example of script to generate backups of TheHive keyspace</p> <pre><code>#!/bin/bash\n\n## Create a tbz archive containing the snapshot\n\n## Complete variables before running:\n## KEYSPACE: Identify the right keyspace to save in cassandra\n## SNAPSHOT: choose a name for the backup\n\nSOURCE_KEYSPACE=\"thehive\"\nSNAPSHOT=\"thehive-backup\"\nSNAPSHOT_DATE=\"$(date +%F)\"\n\n# Backup Cassandra\nnodetool cleanup ${SOURCE_KEYSPACE}\n\nnodetool snapshot ${SOURCE_KEYSPACE}  -t ${SNAPSHOT}_${SNAPSHOT_DATE}\n\ntar cjf ${SNAPSHOT}_${SNAPSHOT_DATE}.tbz /var/lib/cassandra/data/${SOURCE_KEYSPACE}/*/snapshots/${SNAPSHOT}_${SNAPSHOT_DATE}/\n</code></pre> <p> </p>"},{"location":"thehive/operations/backup-restore/#restore-data","title":"Restore data","text":""},{"location":"thehive/operations/backup-restore/#data-and-index","title":"Data and index","text":"<p>Before starting TheHive, if there is an index, ensure the index in Elasticsearch matches the data in Cassandra otherwise this would imply unpredictive behavior. You can force resync (reindex all data) by adding <code>db.janusgraph.forceDropAndRebuildIndex = true</code> in application.conf.</p> <p>Remove this line once TheHive is started</p> <p> </p>"},{"location":"thehive/operations/backup-restore/#pre-requisites","title":"Pre requisites","text":"<p>Following data is required to restore TheHive database successfully: </p> <ul> <li>A backup of the database (<code>${SNAPSHOT}_${SNAPSHOT_DATE}.tbz</code>)</li> <li>Keyspace to restore does not exist in the database (or it will be overwritten)</li> </ul> <p> </p>"},{"location":"thehive/operations/backup-restore/#restore","title":"Restore","text":"<ol> <li> <p>Create the keyspace</p> <pre><code>cqlsh -u cassandra -p ${CASSANDRA_PASSWORD} ${CASSANDRA_ADDRESS} -e \"\n    CREATE KEYSPACE ${TARGET_KEYSPACE}\n    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}\n    AND durable_writes = true;\"\n</code></pre> <p>Here it is possible to use a different keyspace name.</p> </li> <li> <p>Unarchive backup files: </p> <pre><code>RESTORE_PATH=$(mktemp -d)\ntar jxf ${SNAPSHOT}_${SNAPSHOT_DATE}.tbz -C ${RESTORE_PATH}\n</code></pre> </li> <li> <p>Create tables from archive</p> <p>The archive contains the table schemas. They must be executed in the new keyspace. The schema files are in <code>${RESTORE_PATH}/var/lib/cassandra/data/${SOURCE_KEYSPACE}/${TABLE}/snapshots/${SNAPSHOT}_${SNAPSHOT_DATE}/schema.cql</code></p> <pre><code>for CQL in $(find ${RESTORE_PATH} -name schema.cql)\ndo\n  cqlsh cassandra -f $CQL\ndone\n</code></pre> <p>If you want to change the name of the keyspace (<code>${SOURCE_KEYSPACE}</code> =&gt; <code>${TARGET_KEYSPACE}</code>), you need to rewrite the cql command:</p> <pre><code>for CQL in $(find ${RESTORE_PATH} -name schema.cql)\ndo\n  cqlsh cassandra -e \"$(sed -e '/CREATE TABLE/s/'${SOURCE_KEYSPACE}/${TARGET_KEYSPACE}/ $CQL)\"\ndone\n</code></pre> </li> <li> <p>Reorganize snapshot files:</p> <p>The files in a snapshot follow the structure: <code>var/lib/cassandra/data/${KEYSPACE}/${TABLE}/snapshots/${SNAPSHOT}/</code>. In order to import files, the structure must be: <code>.../${KEYSPACE}/${TABLE}/</code>. The following command lines move files to match the expected file organization.</p> <pre><code>mkdir -p ${RESTORE_PATH}/${TARGET_KEYSPACE}\nfor TABLE in ${RESTORE_PATH}/var/lib/cassandra/data/${SOURCE_KEYSPACE}/*\ndo\n  mv $TABLE/snapshots/${SNAPSHOT}_${SNAPSHOT_DATE}/* ${RESTORE_PATH}/${TARGET_KEYSPACE}/$(basename $TABLE)\ndone  \n</code></pre> </li> <li> <p>Load table data</p> <pre><code>for TABLE in ${RESTORE_PATH}/${TARGET_KEYSPACE}/*\ndo \n  sstableloader -d ${CASSANDRA_IP} $TABLE\ndone\n</code></pre> </li> <li> <p>Cleanup</p> <pre><code>rm -rf ${RESTORE_PATH}\n</code></pre> </li> </ol> <p>Ensure no Commitlog file exist before restarting Cassandra service. (<code>/var/lib/cassandra/commitlog</code>)</p> <p>Example of script to restore TheHive keyspace in Cassandra</p> <pre><code>#!/bin/bash\n\n## Restore a KEYSPACE and its data from a CQL file with the schema of the\n## KEYSPACE and an tbz archive containing the snapshot\n\n## Complete variables before running:\n## CASSANDRA_ADDRESS: IP of cassandra server\n## RESTORE_PATH: choose a TMP folder !!! this folder will be removed if exists.\n## SOURCE_KEYSPACE: KEYSPACE used in the backup\n## TARGET_KEYSPACE: new KEYSPACE name ; use same name of SOURCE_KEYSPACE if no changes\n## SNAPSHOT: choose a name for the backup\n## SNAPSHOT_DATE: date of the snapshot to restore\n\nCASSANDRA_ADDRESS=\"10.1.1.1\"\nRESTORE_PATH=$(mktemp -d)\nSOURCE_KEYSPACE=\"thehive\"\nTARGET_KEYSPACE=\"thehive_restore\"\nSNAPSHOT=\"thehive-backup\"\nSNAPSHOT_DATE=\"2022-11-29\"\n\n## Uncompress data in TMP folder\nrm -rf ${RESTORE_PATH} &amp;&amp; mkdir ${RESTORE_PATH} \ntar jxf ${SNAPSHOT}_${SNAPSHOT_DATE}.tbz -C ${RESTORE_PATH}\n\n## Read Cassandra password\necho -n \"Cassandra admin password: \" \nread -s CASSANDRA_PASSWORD\n\n# Create the keyspace\ncqlsh -u cassandra -p ${CASSANDRA_PASSWORD} ${CASSANDRA_ADDRESS} -e \"\n    CREATE KEYSPACE ${TARGET_KEYSPACE}\n    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}\n    AND durable_writes = true;\"\n\n# Create table in keyspace\nfor CQL in $(find ${RESTORE_PATH} -name schema.cql)\ndo\n  cqlsh cassandra -e \"$(sed -e '/CREATE TABLE/s/'${SOURCE_KEYSPACE}/${TARGET_KEYSPACE}/ $CQL)\"\ndone\n\n# Reorganiza snapshot files\nmkdir -p ${RESTORE_PATH}/${TARGET_KEYSPACE}\nfor TABLE in ${RESTORE_PATH}/var/lib/cassandra/data/${SOURCE_KEYSPACE}/*\ndo\n  mv $TABLE/snapshots/${SNAPSHOT}_${SNAPSHOT_DATE}/* ${RESTORE_PATH}/${TARGET_KEYSPACE}/$(basename $TABLE)\ndone  \n\n## Load data\nfor TABLE in ${RESTORE_PATH}/${TARGET_KEYSPACE}/*\ndo \n  sstableloader -d ${CASSANDRA_IP} $TABLE\ndone\n</code></pre>"},{"location":"thehive/operations/backup-restore/#index","title":"Index","text":"<p>Several solutions exist regarding the index:</p> <ol> <li>Save the Elasticsearch index and restore it ; follow Elasticsearch guides to perform this action</li> <li>Rebuild the index on the new server, when TheHive start for the first time.</li> </ol> <p> </p>"},{"location":"thehive/operations/backup-restore/#rebuild-the-index","title":"Rebuild the index","text":"<p>Once Cassandra database is restored, update the configuration of TheHive to rebuild the index.</p> <p>These lines should be added to the configuration file only for the first start of TheHive application, and removed later on.</p> extract from /etc/thehive/application.conf<pre><code>db.janusgraph.forceDropAndRebuildIndex = true\n</code></pre> <p>Once started, both lines should be removed or commented from the configuration file of TheHive</p>"},{"location":"thehive/operations/backup-restore/#files","title":"Files","text":""},{"location":"thehive/operations/backup-restore/#backup_1","title":"Backup","text":"<p>Wether you use local or distributed files system storage, copy the content of the folder/bucket.</p> <p> </p>"},{"location":"thehive/operations/backup-restore/#restore_1","title":"Restore","text":"<p>Restore the saved files into the destination folder/bucket that will be used by TheHive.</p>"},{"location":"thehive/operations/backup-restore/#troubleshooting","title":"Troubleshooting","text":"<p>The first start can take some time, especially if the application has to rebuild the index. Refer to this troubleshooting section to ensure everything goes well with reindexation.</p> <p>Note</p> <p>References: - Backing up and restoring data</p> <p> </p>"},{"location":"thehive/operations/cassandra-cluster/","title":"Cassandra Cluster Operations","text":""},{"location":"thehive/operations/cassandra-cluster/#cassandra-cluster-operations","title":"Cassandra Cluster Operations","text":"<p>This guide provides comprehensive instructions for performing key operations on a Cassandra cluster, including adding nodes to an existing cluster and managing node removal.</p>"},{"location":"thehive/operations/cassandra-cluster/#adding-a-node-to-a-cassandra-cluster","title":"Adding a Node to a Cassandra Cluster","text":"<p>To add a Cassandra node to an existing cluster without downtime, follow these steps:</p> <ol> <li> <p>Install the New Node:</p> <ul> <li>Install Cassandra on the new node using the same configuration file as the existing nodes, with adjustments made to the <code>listen_address</code> and <code>rpc_address</code> settings to reflect the node's IP address. Set the IP address of any existing node in the seeds section of the configuration file (cassandra.yaml). </li> </ul> </li> <li> <p>Start the Node</p> <ul> <li>Once installed and configured, start the Cassandra service on the new node.</li> </ul> </li> <li> <p>Check Cluster Status</p> <ul> <li>Use the nodetool status command to monitor the cluster status and verify that the new node is recognized and in the joining state.</li> </ul> Display Cassandra Nodes Status<pre><code># nodetool status\n\nDatacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address     Load       Tokens  Owns (effective)  Host ID                               Rack \nUN  172.24.0.2  3.68 GiB   256      100.0%            048a870f-d6d5-405e-8d0d-43dbc12be747  rack1\nUJ  172.24.0.3  89.54 MiB  256      ?                 f192ef8f-a4fd-4e24-99a7-0f92605a7cb6  rack1\n</code></pre> </li> </ol> <p>After the new node is fully operational, re-run the nodetool status command to ensure that the cluster status reflects the successful addition of the new node:</p> Display Cassandra Nodes status<pre><code># nodetool status\n\nDatacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address     Load      Tokens  Owns (effective)  Host ID                               Rack \nUN  172.24.0.2  3.68 GiB  256      48.8%             048a870f-d6d5-405e-8d0d-43dbc12be747  rack1\nUN  172.24.0.3  1.91 GiB  256      51.2%             f192ef8f-a4fd-4e24-99a7-0f92605a7cb6  rack1\n</code></pre> <p>The updated status will indicate the UN (Up/Normal) state for the new node, showing its load, tokens, ownership percentage, host ID, and rack assignment.</p> <p>Creating a Cluster from a Standalone Server</p> <p>If you are transitioning from a standalone server to a multi-node cluster by adding a new node, ensure that you update  the confiuration of the existing standalone server by modifying the <code>cassandra.yaml</code> file to replace the <code>localhost</code> in <code>listen_address</code> and <code>rpc_address</code> with the server's actual IP address. </p>"},{"location":"thehive/operations/cassandra-cluster/#removing-an-alive-node-from-a-cassandra-cluster","title":"Removing an Alive Node from a Cassandra Cluster","text":"<p>To safely remove an alive node from your Cassandra cluster, follow these steps:</p> <ol> <li>Ensure Compatibility with Replication Factor</li> </ol> <p>Before proceeding with node removal, ensure that the replication factor is suitable for the desired number of nodes in the cluster. If necessary, update the replication factor to maintain adequate data redundancy.</p> <ol> <li>Decommission the Node</li> </ol> <p>To remove an alive node gracefully, connect to the node you intend to remove and execute the following nodetool command:</p> <pre><code>nodetool decommission\n</code></pre> <p>This command initiates the decommissioning process for the node, allowing it to transfer its data to other nodes in the cluster before removal.</p> <p>Monitor the decommissioning progress using <code>nodetool status</code> on other nodes in the cluster. Verify that the decommissioned node transitions to a <code>Leaving state</code> and completes data transfer successfully.</p>"},{"location":"thehive/operations/cassandra-cluster/#removing-a-dead-node-from-a-cassandra-cluster","title":"Removing a Dead Node from a Cassandra Cluster","text":"<p>If a node has crashed and cannot be repaired, you can remove it from the cluster using the <code>nodetool removenode</code> command followed by the node <code>id</code>.</p> <p>Steps to Remove a Dead Node:</p> <ol> <li>Check Current Cluster Status:</li> <li> <p>Use <code>nodetool status</code> to view the current status of the Cassandra cluster:      </p><pre><code>nodetool status\n</code></pre> </li> <li> <p>Identify the Dead Node:</p> </li> <li> <p>Locate the node with a <code>DN</code> (Down) status in the cluster output.</p> </li> <li> <p>Execute <code>nodetool removenode</code> Command:</p> </li> <li> <p>Run the following command to remove the dead node from the cluster:      </p><pre><code>nodetool removenode &lt;node_id&gt;\n</code></pre>      Replace <code>&lt;node_id&gt;</code> with the ID of the dead node obtained from the <code>nodetool status</code> output. </li> <li> <p>Verify Cluster Status:</p> </li> <li>After executing the <code>nodetool removenode</code> command, check the cluster status again using <code>nodetool status</code> to ensure that the dead node has been successfully removed and that the remaining nodes are in a <code>Normal</code> state.</li> </ol>"},{"location":"thehive/operations/cassandra-cluster/#increasing-replication-factor-in-cassandra","title":"Increasing Replication Factor in Cassandra","text":"<p>When you have multiple nodes, increasing the replication factor enhances fault tolerance by creating additional copies in the cluster. More copies mean greater resilience to hardware failures. However, this also means more disk space is used due to the increased data redundancy, potentially resulting in slower write access.</p> <p>To enhance fault tolerance in your Cassandra cluster by increasing the replication factor, follow these steps:</p> <ol> <li>Connect to cqlsh</li> </ol> <p>Use cqlsh to connect to your Cassandra cluster:</p> <ol> <li>Modify Keyspace Replication</li> </ol> <p>Execute the following ALTER KEYSPACE command to increase the replication factor for a specific keyspace (replace thehive with your keyspace name):</p> <pre><code>ALTER KEYSPACE thehive WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3 };\n</code></pre> <ol> <li>Run nodetool repair -full on Each Node</li> </ol> <p>After modifying the keyspace replication, execute <code>nodetool repair -full</code> on each node in your Cassandra cluster to ensure data is fully replicated and consistent across the cluster.</p> <p>It's recommended to increase the replication factor for system keyspaces to ensure high availability and reliability of critical Cassandra system data.</p> <p>To view the current replication settings for all keyspaces in your cluster, use the following cqlsh query:</p> Checking Current Keyspace Information<pre><code>&gt; SELECT * FROM system_schema.keyspaces;\n\n keyspace_name      | durable_writes | replication\n--------------------+----------------+-------------------------------------------------------------------------------------\n        system_auth |           True | {'class': 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '2'}\n      system_schema |           True |                             {'class': 'org.apache.cassandra.locator.LocalStrategy'}\n system_distributed |           True | {'class': 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '2'}\n             system |           True |                             {'class': 'org.apache.cassandra.locator.LocalStrategy'}\n      system_traces |           True | {'class': 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '2'}\n            thehive |           True | {'class': 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '2'}\n</code></pre> <p>This query displays detailed information about each keyspace, including its name and replication strategy with the associated replication factor.</p> <p> </p>"},{"location":"thehive/operations/cassandra-security/","title":"Cassandra Security Operations","text":""},{"location":"thehive/operations/cassandra-security/#security-in-apache-cassandra","title":"Security in Apache Cassandra","text":""},{"location":"thehive/operations/cassandra-security/#authentication-with-cassandra","title":"Authentication with Cassandra","text":"<p>References</p> <p>Internal authentication</p> <ul> <li>https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureInternalAuthenticationTOC.html</li> </ul> <p>To authenticate with Cassandra and manage permissions, follow these steps:</p> <ol> <li>Create a Role and Grant Permissions:</li> </ol> <pre><code>CREATE ROLE thehive WITH PASSWORD = 'thehive1234' AND LOGIN = true;\nGRANT ALL PERMISSIONS ON KEYSPACE thehive TO thehive;\n</code></pre> <ol> <li> <p>Configure TheHive with the Account:</p> <p>Update <code>/etc/thehive/application.conf</code> with the Cassandra authentication details:</p> <pre><code>  db.janusgraph {\n    storage {\n      ## Cassandra configuration\n      # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql\n      backend: cql\n      hostname: [\"xxx.xxx.xxx.xxx\"]\n      # Cassandra authentication (if configured)\n      username: \"thehive\" \n      password: \"thehive1234\" \n      cql {\n        cluster-name: thp\n        keyspace: thehive\n      }\n    }\n</code></pre> <p>Ensure to replace xxx.xxx.xxx.xxx with the appropriate Cassandra hostname or IP address.</p> </li> </ol>"},{"location":"thehive/operations/cassandra-security/#configuring-node-to-node-encryption-for-cassandra","title":"Configuring Node-to-Node Encryption for Cassandra","text":"<p>References</p> <p>Node-to-Node Encryption</p> <ul> <li>https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLNodeToNode.html</li> </ul> <p>To enable node-to-node encryption, modify the <code>cassandra.yaml</code> configuration file with the following settings:</p> <pre><code>server_encryption_options:\n    internode_encryption: all\n    keystore: /path/to/keystore.jks\n    keystore_password: keystorepassword\n    truststore: /path/to/truststore.jks\n    truststore_password: truststorepassword\n    # More advanced defaults below:\n    protocol: TLS\n    algorithm: SunX509\n    store_type: JKS\n    cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_\nAES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_R\nSA_WITH_AES_256_CBC_SHA]\n    require_client_auth: false\n</code></pre> <p> </p>"},{"location":"thehive/operations/cassandra-security/#setting-up-cassandra-dedicated-ssl-port-optional","title":"Setting Up Cassandra Dedicated SSL Port (Optional)","text":"<p>Optionally, you can configure a dedicated port for SSL communication in Cassandra. Setting up a dedicated SSL port provides enhanced security for communication with your Cassandra cluster. Follow these steps to update the <code>/etc/cassandra/cassandra.yaml</code> configuration file on each node:</p> <ol> <li> <p>Open the cassandra.yaml configuration file on each node.</p> </li> <li> <p>Locate the native_transport_port_ssl parameter within the file.</p> </li> <li> <p>Update the native_transport_port_ssl parameter to specify the desired SSL port number (e.g., 9142):</p> <pre><code>native_transport_port_ssl: 9142\n</code></pre> </li> </ol> <p>By specifying a dedicated <code>native_transport_port_ssl</code>, all SSL communications will utilize this port instead of the default port configured for non-SSL traffic (<code>native_transport_port</code>). </p>"},{"location":"thehive/operations/cassandra-security/#client-to-node-encryption","title":"Client to Node Encryption","text":"<p>References</p> <p>Client to Node Encryption</p> <ul> <li>https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLClientToNode.html</li> <li>https://docs.janusgraph.org/basics/configuration-reference/#storagecqlssl</li> </ul> <p>This guide explains how to establish a secure connection between Cassandra clients (specifically TheHive) and the Cassandra server.</p> <p> </p>"},{"location":"thehive/operations/cassandra-security/#requirements","title":"Requirements","text":"<p>To set up secure encryption, you'll need the following:</p> <ol> <li>X509 Certificate for Cassandra Service</li> <li>Ensure the certificate has the following standard properties:<ul> <li>Key Usage: Digital Signature, Non-Repudiation, Key Encipherment, Key Agreement</li> <li>Extended Key Usage: TLS Web Server Authentication</li> <li>Certificate Type: SSL Server</li> </ul> </li> <li>Include a \"Subject Alternative Name\" (DNS name and/or IP address) that matches how the Cassandra server is identified by the client.</li> <li> <p>The certificate file format should be PKCS12 (with a <code>.p12</code> extension).</p> </li> <li> <p>Truststore</p> </li> <li>Create a truststore that contains the Certificate Authority (CA) used to generate the Cassandra server's certificate.</li> <li>The truststore should be in Java Keystore (JKS) format.</li> <li>If your CA file is named <code>ca.crt</code>, generate the truststore file using the following command:      <pre><code>keytool -import -file /path/to/ca.crt -alias CA -keystore ca.jks\n</code></pre>      You will be prompted to set a password for file integrity checking.</li> </ol> <p>Note: The <code>keytool</code> command is available as part of any JDK distribution.</p> <p> </p>"},{"location":"thehive/operations/cassandra-security/#steps-to-set-up-encryption","title":"Steps to Set Up Encryption","text":"<p>Follow these steps to enable encryption between Cassandra clients (TheHive) and the Cassandra server:</p> <ol> <li> <p>Obtain the Server Certificate</p> <ul> <li>Acquire a valid X509 certificate for the Cassandra service with the specified properties.</li> </ul> </li> <li> <p>Configure the Server Certificate</p> <ul> <li>Ensure the certificate includes the necessary key usage and extended key usage properties.</li> </ul> </li> <li> <p>Create the Truststore</p> <ul> <li>Use the <code>keytool</code> command to import the CA certificate into a Java Keystore (JKS) file (<code>ca.jks</code>).</li> </ul> <pre><code>keytool -import -file /path/to/ca.crt -alias CA -keystore ca.jks\n</code></pre> <p>You will be prompted to set a password for the truststore.</p> </li> <li> <p>Configuring Cassandra</p> <ul> <li>Locate the section <code>client_encryption_options</code> and set the following options:</li> </ul> <pre><code>client_encryption_options:\n    enabled: true\n    # If enabled and optional is set to true encrypted and unencrypted connections are handled.\n    optional: false\n    keystore: /pat/to/keystore.jks\n    keystore_password: keystorepassword\n    require_client_auth: false\n    # Set trustore and truststore_password if require_client_auth is true\n    # truststore: conf/.truststore\n    # truststore_password: cassandra\n    # More advanced defaults below:\n    protocol: TLS\n    algorithm: SunX509\n    store_type: JKS\n    cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_\nAES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_R\nSA_WITH_AES_256_CBC_SHA]\n</code></pre> <ul> <li>After making these changes, restart the Cassandra service.</li> </ul> </li> <li> <p>Configure TheHive to Use Encryption</p> <ul> <li>Update TheHive's Cassandra client configuration to specify the location of the truststore (<code>ca.jks</code>) and provide any additional connection properties required for SSL/TLS encryption.</li> </ul> <pre><code>db.janusgraph {\n  storage {\n    ## Cassandra configuration\n    # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql\n    backend: cql\n    hostname: [\"ip_node_1\", \"ip_node_2\", \"ip_node_3\"]\n    # Cassandra authentication (if configured)\n    username: \"thehive\"\n    password: \"thehive1234\"\n    port: 9142 # if alternative port has been set in Cassandra configuration\n    cql {\n      cluster-name: thp\n      keyspace: thehive\n      ssl {\n        enabled: true\n        truststore {\n          location: \"/path/to/truststore.jks\"\n          password: \"truststorepassword\"\n        }\n      }\n    }\n  }\n</code></pre> <ul> <li>After making these changes, restart TheHive to apply the new configuration settings.</li> </ul> </li> </ol> <p> </p>"},{"location":"thehive/operations/change-index/","title":"Index Management","text":""},{"location":"thehive/operations/change-index/#change-index-from-lucene-to-elasticsearch","title":"Change Index from Lucene to Elasticsearch","text":"<p>Lucene index support was removed starting from TheHive version 5.1. Follow this procedure to change your index to Elasticsearch. This procedure is compatible with any 5.x version.</p>"},{"location":"thehive/operations/change-index/#setup-your-elasticsearch-server","title":"Setup Your Elasticsearch Server","text":"<p>Please refer to our installation manual or use your own server. Ensure that Elasticsearch is started.</p>"},{"location":"thehive/operations/change-index/#update-thehive-configuration","title":"Update TheHive Configuration","text":"DEB / RPM <p>Update your <code>/etc/thehive/application.conf</code> file:</p> <pre><code># Update this configuration section\ndb.janusgraph {\n    # Retain this section for now, as TheHive will need it to migrate the index correctly\n    index.search {\n        backend: lucene\n        directory: ...\n    }\n\n    # Add this section below the Lucene configuration\n    index.search {\n        backend: elasticsearch\n        # Hostname(s) of your Elasticsearch server(s)\n        hostname: [\"localhost\"]\n        # Default is \"thehive\"\n        index-name: thehive\n    }\n}\n</code></pre> <p>Refer to the configuration reference for more options.</p> Docker <p>Update your Docker arguments:</p> <pre><code># Docker compose file\ncommand:\n    # ... other args\n    - \"--index-backend\"\n    - \"elasticsearch\"\n    - \"--es-hostnames\"\n    - \"elasticsearch\"\n</code></pre>"},{"location":"thehive/operations/change-index/#restart-thehive-application","title":"Restart TheHive Application","text":"<p>TheHive will detect the index change, create the new index in Elasticsearch, and start reindexing the documents into the new index. This process may take some time depending on the size of your database.</p> <p>In your logs, you should see the following lines:</p> <pre><code>[warn] o.j.d.c.b.ReadConfigurationBuilder [] Local setting index.search.backend=elasticsearch (Type: GLOBAL_OFFLINE) is overridden by globally managed value (lucene). Use the ManagementSystem interface instead of the local configuration to control this setting.\n</code></pre> <p>During the reindexing process, you will see lines similar to these (the name <code>global1</code> may be different for you):</p> <pre><code>[info] o.j.g.o.j.IndexRepairJob [] Index global1 metrics: success-tx: 110 doc-updates: 10992 succeeded: 10992\n[info] o.j.g.o.j.IndexRepairJob [] Index global1 metrics: success-tx: 111 doc-updates: 10992 succeeded: 10992\n[info] o.j.g.d.m.ManagementSystem [] Index update job successful for [global1]\n</code></pre> <p>Finally, the application will start listening on the HTTP port, and you will be able to access the interface.</p>"},{"location":"thehive/operations/change-index/#remove-the-lucene-configuration","title":"Remove the Lucene Configuration","text":"<p>Once the reindexing is complete, you can remove the Lucene configuration from your configuration file.</p> DEB / RPM <p>Update your <code>/etc/thehive/application.conf</code> file:</p> <pre><code># Update this configuration section\ndb.janusgraph {\n    # Delete this section now\n    index.search {\n        backend: lucene\n        directory: ...\n    }\n\n    # Retain this section\n    index.search {\n        backend: elasticsearch\n        # Hostname(s) of your Elasticsearch server(s)\n        hostname: [\"localhost\"]\n        # Default is \"thehive\"\n        index-name: thehive\n    }\n}\n</code></pre>"},{"location":"thehive/operations/minio-cluster/","title":"MinIO Cluster Operations","text":""},{"location":"thehive/operations/minio-cluster/#minio-cluster-operations","title":"MinIO Cluster Operations","text":""},{"location":"thehive/operations/minio-cluster/#replace-a-disk","title":"Replace a Disk","text":"<p>If a disk in your MinIO cluster is faulty and needs to be replaced, you can perform this operation without downtime thanks to MinIO's support for hot-swapping disks:</p> <ol> <li> <p>Unmount the Faulty Disk</p> <ul> <li> <p>Safely unmount the faulty disk from the server where it is installed.</p> </li> <li> <p>Ensure that any data on the disk is backed up or transferred to another location if possible.</p> </li> </ul> </li> <li> <p>Install the Replacement Disk</p> <ul> <li> <p>Install the healthy replacement disk in the same location as the faulty disk.</p> </li> <li> <p>Connect the replacement disk to the server and ensure it is properly recognized by the operating system.</p> </li> </ul> </li> <li> <p>Mount the Replacement Disk</p> <ul> <li>Mount the replacement disk to the same mount point previously used by the faulty disk.</li> </ul> </li> <li> <p>Check MinIO Logs</p> <ul> <li>Check the MinIO logs to ensure that the new disk has been recognized by the system:   <pre><code>mc admin service logs &lt;myminio&gt;\n</code></pre></li> </ul> </li> <li> <p>Initiate Disk Heal</p> <ul> <li>Run a disk heal operation to synchronize data and ensure data integrity across the cluster with the new disk:   <pre><code>mc admin heal &lt;myminio&gt;\n</code></pre></li> </ul> </li> </ol>"},{"location":"thehive/operations/minio-cluster/#replace-a-node","title":"Replace a Node","text":"<p>In the event that a server within your MinIO cluster has crashed and cannot be recovered, you can install a new server to replace it. Ensure that the new node is configured with the same IP address or hostname as the old node. Once the new server is operational and has joined the cluster, initiate a disk heal by running <code>mc admin heal</code>.</p> <p> </p>"},{"location":"thehive/operations/minio-cluster/#step-by-step-guide-replace-a-node-in-minio-cluster","title":"Step-by-Step Guide: Replace a Node in MinIO Cluster","text":"<ol> <li> <p>Prepare the Replacement Server</p> <ul> <li> <p>Set up a new server that meets the hardware and network requirements for running MinIO.</p> </li> <li> <p>Ensure the new server has the same MinIO version as the existing cluster nodes.</p> </li> <li> <p>Assign the same IP address or hostname as the node being replaced.</p> </li> </ul> </li> <li> <p>Prepare the Existing Cluster</p> <ul> <li>Identify the node that needs to be replaced and ensure it is no longer part of the active cluster. Stop the MinIO service on this node.</li> </ul> </li> <li> <p>Transfer Data</p> <ul> <li>If possible, transfer any data stored on the node being replaced to other nodes in the cluster to avoid data loss.</li> </ul> </li> <li> <p>Distribute the Configuration</p> <ul> <li>Copy the modified <code>config.json</code> file to the new replacement node's MinIO configuration directory.</li> </ul> </li> <li> <p>Start MinIO Service on Replacement Node</p> <ul> <li>Start the MinIO service on the new replacement node and ensure it joins the cluster successfully:   <pre><code>systemctl start minio\n</code></pre></li> </ul> </li> <li> <p>Verify Node Replacement</p> <ul> <li>Check the cluster status to ensure that the new replacement node has been successfully added and is part of the cluster:   <pre><code>mc admin info myminio\n</code></pre></li> </ul> </li> <li> <p>Initiate Disk Heal</p> <ul> <li>Run a disk heal operation to synchronize data and ensure data integrity across the cluster with the new replacement node:   <pre><code>mc admin heal myminio\n</code></pre></li> </ul> </li> <li> <p>Monitor Cluster Health</p> <ul> <li>Use MinIO's monitoring tools (<code>mc admin top</code> or web-based console) to monitor the health and performance of the cluster with the new replacement node.</li> </ul> </li> </ol> <p>By following these steps, you can safely replace a node in your MinIO cluster while maintaining data integrity and cluster stability. Ensure that all operations are performed during a maintenance window to minimize impact on production services.</p>"},{"location":"thehive/operations/minio-cluster/#add-a-node-to-the-cluster","title":"Add a Node to the Cluster","text":"<p>Adding a new server to an existing MinIO cluster involves restarting all MinIO nodes and updating their configuration to account for the new node. It's crucial to synchronize the configuration across all nodes. After the cluster is successfully started with the new node, perform a disk heal operation using <code>mc admin heal</code>.</p> <p>Note: MinIO strongly recommends restarting all nodes simultaneously when adding or removing nodes from a cluster. Avoid \"rolling\" restarts (i.e., restarting one node at a time).</p> <p> </p>"},{"location":"thehive/operations/minio-cluster/#step-by-step-guide-adding-a-node-to-minio-cluster","title":"Step-by-Step Guide: Adding a Node to MinIO Cluster","text":"<ol> <li> <p>Prepare the New Server</p> <ul> <li> <p>Set up a new server (physical or virtual) that meets the hardware and network requirements for running MinIO.</p> </li> <li> <p>Ensure the new server has the same MinIO version as the existing cluster nodes.</p> </li> <li> <p>Assign a static IP address or configure a hostname for the new node that matches the existing cluster nodes.</p> </li> </ul> </li> <li> <p>Update MinIO Configuration</p> <ul> <li> <p>Access the configuration file (<code>config.json</code>) of each existing MinIO node in the cluster.</p> </li> <li> <p>Add the details of the new node (IP address or hostname) to the <code>config.json</code> file of each node under the <code>cluster</code> section:   </p><pre><code>{\n   \"version\": \"minio\",\n   \"cluster\": {\n      \"nodes\": [\n      {\"endpoint\": \"existing-node1:9000\"},\n      {\"endpoint\": \"existing-node2:9000\"},\n      {\"endpoint\": \"existing-node3:9000\"},\n      {\"endpoint\": \"new-node:9000\"}\n      ]\n   }\n}\n</code></pre> </li> <li> <p>Save the updated configuration file.</p> </li> </ul> </li> <li> <p>Distribute the Updated Configuration</p> <ul> <li>Copy the modified <code>config.json</code> file to the new node's MinIO configuration directory.</li> </ul> </li> <li> <p>Restart MinIO Service</p> <ul> <li>On each existing MinIO node, restart the MinIO service to apply the updated configuration:   <pre><code>systemctl restart minio\n</code></pre></li> </ul> </li> <li> <p>Verify Node Addition</p> <ul> <li>Once all nodes have been restarted, check the cluster status to ensure the new node has been successfully added:   <pre><code>mc admin info &lt;myminio&gt;\n</code></pre></li> </ul> </li> <li> <p>Initiate Disk Heal</p> <ul> <li>Run a disk heal operation to ensure data consistency across the cluster with the new node:   <pre><code>mc admin heal &lt;myminio&gt;\n</code></pre></li> </ul> </li> </ol>"},{"location":"thehive/operations/minio-cluster/#remove-a-node-from-cluster","title":"Remove a node from cluster","text":"<p>When removing a node from your MinIO cluster, follow the same procedure as adding a node: update the cluster's configuration file to reflect the removal and restart all nodes in the cluster to apply the changes.</p>"},{"location":"thehive/operations/minio-cluster/#minio-and-high-availability","title":"MINIO and High Availability","text":"<p>MinIO supports high availability by configuring a single S3 endpoint in applications like TheHive. To achieve high availability across MinIO nodes, it's recommended to use load balancers and virtual IP addresses to distribute connections evenly among the cluster nodes.</p> <p>Follow the guide under deploying a cluster to configure MinIO with load balancer and virtual IP.</p> <p> </p>"},{"location":"thehive/operations/troubleshooting/","title":"Troubleshooting Guide","text":""},{"location":"thehive/operations/troubleshooting/#troubleshooting","title":"Troubleshooting","text":"<p>For some issues, additional information in logs is needed to troubleshoot and understand the root causes. To gather and share this information, please carefully read and follow these steps.</p> <p>Warning</p> <p>ENABLING TRACE LOGS HAS A SIGNIFICANT IMPACT ON PERFORMANCE. DO NOT ENABLE IT ON PRODUCTION SERVERS.</p>"},{"location":"thehive/operations/troubleshooting/#step-1-stop-thehive-service","title":"Step 1: Stop TheHive Service","text":"<p>First, stop TheHive service:</p> <pre><code>service thehive stop\n</code></pre> <p>Ensure the service is stopped with the following command:</p> <pre><code>service thehive status\n</code></pre>"},{"location":"thehive/operations/troubleshooting/#step-2-renew-applicationlog-file","title":"Step 2: Renew <code>application.log</code> File","text":"<p>Move the existing <code>application.log</code> file to a backup location:</p> <pre><code>mv /var/log/thehive/application.log /var/log/thehive/application.log.bak\n</code></pre>"},{"location":"thehive/operations/troubleshooting/#step-3-update-log-configuration","title":"Step 3: Update Log Configuration","text":"<p>Edit the log configuration file <code>/etc/thehive/logback.xml</code>. Locate the line containing <code>&lt;logger name=\"org.thp\" level=\"INFO\"/&gt;</code> and update it to the following:</p> <pre><code>    &lt;logger name=\"org.thp\" level=\"TRACE\"/&gt;\n</code></pre> <p>Save the file after making the changes.</p>"},{"location":"thehive/operations/troubleshooting/#step-4-restart-thehive-service","title":"Step 4: Restart TheHive Service","text":"<p>Restart TheHive service:</p> <pre><code>service thehive start\n</code></pre> <p>A new log file <code>/var/log/thehive/application.log</code> will be created and will contain extensive logging information.</p>"},{"location":"thehive/operations/troubleshooting/#step-5-monitor-and-save-logs","title":"Step 5: Monitor and Save Logs","text":"<p>Wait for the issue to occur or for the application to stop. Then, copy the log file to a safe location:</p> <pre><code>cp /var/log/thehive/application.log /root\n</code></pre>"},{"location":"thehive/operations/troubleshooting/#step-6-share-the-logs","title":"Step 6: Share the Logs","text":"<p>Create an issue on GitHub and include the following information:</p> <ul> <li>Context:</li> <li>Instance type (single node/cluster, backend type, index engine)</li> <li> <p>System details (Operating System, amount of RAM, number of CPUs for each server/node)</p> </li> <li> <p>Symptoms:</p> </li> <li> <p>Actions taken, how the situation occurred, and what happened</p> </li> <li> <p>Log File:</p> </li> <li>Attach the log file with trace information</li> </ul>"},{"location":"thehive/operations/troubleshooting/#step-7-revert-log-configuration","title":"Step 7: Revert Log Configuration","text":"<p>To revert to the normal log configuration:</p> <ol> <li>Stop TheHive service.</li> <li>Edit the <code>logback.xml</code> file to restore the previous log level configuration.</li> <li>Restart TheHive service.</li> </ol> <pre><code>service thehive stop\n# Restore the logback.xml file to previous state\nservice thehive start\n</code></pre> <p> </p>"},{"location":"thehive/operations/monitoring/monitoring/","title":"Monitoring Setup","text":""},{"location":"thehive/operations/monitoring/monitoring/#monitoring-thehive","title":"Monitoring TheHive","text":"<p>Monitoring your TheHive instance enables you to gather essential metrics regarding its performance, including request times, CPU usage, and memory utilization.</p> <p>TheHive leverages the Kamon library for monitoring purposes, which is disabled by default.</p> <p>TheHive includes integration with the Prometheus reporter out-of-the-box. Other reporters are not included. If you wish to have a specific reporter included by default in TheHive, please contact us.</p>"},{"location":"thehive/operations/monitoring/monitoring/#configuring-metrics-with-prometheus-and-grafana","title":"Configuring Metrics with Prometheus and Grafana","text":"<p>To configure TheHive for metrics reporting, add the following section to your <code>application.conf</code> file:</p> <pre><code>kamon {\n    # Activate Kamon module - disabled by default\n    enabled = true\n\n    # Enable the Prometheus reporter\n    modules {\n      prometheus-reporter.enabled = yes\n    }\n\n    environment.tags {\n        # Configure additional tags to be sent to Prometheus \n        # See https://kamon.io/docs/latest/reporters/prometheus/#sending-environment-tags-to-prometheus\n        # Example: env = prod\n    }\n\n    # Reference: https://kamon.io/docs/latest/reporters/prometheus/#configuration\n    prometheus {\n      include-environment-tags = true\n      # Start an embedded server on the specified port. \n      # If using Docker, ensure this port is accessible\n      embedded-server {\n        hostname = 0.0.0.0\n        port = 9095\n      }\n    }\n}\n</code></pre> <p>You will need to restart TheHive for the configuration changes to take effect.</p> <p>To verify that the Prometheus reporter is functioning correctly, navigate to http://THEHIVE:9095/metrics. You should see a list of metrics reported by TheHive.</p> <p> </p>"},{"location":"thehive/operations/monitoring/monitoring/#prometheus-configuration","title":"Prometheus configuration","text":"<p>Add the scrape configuration to prometheus configuration <code>prometheus.yml</code></p> <pre><code>scrape_configs:\n  # ...  other scrape configs \n\n  - job_name: 'thehive'\n    scrape_interval: 30s\n    static_configs:\n      - targets: ['THEHIVE:9095'] # set the ip or hostname for TheHive\n</code></pre> <p>In a dynamic environment like kubernetes, the TheHive service can be automatically discovered by prometheus. You can enable this with labels on your pod or by adding a <code>PodMonitor</code> resource. See the adaquate documentation: Prometheus configuration or Prometheus operator</p> <p> </p>"},{"location":"thehive/operations/monitoring/monitoring/#grafana-configuration","title":"Grafana configuration","text":"<ul> <li>Make sure that prometheus is setup as a Datasource inside Grafana</li> <li> <p>Import dashboards or create your own. We recommend the following dashboards (these dashboards were not created by Strangebee):</p> <ul> <li>Kamon 2.x - API dashboard: see API metrics like throughoutput, latency, % of error status. Note that TheHive frontend uses long polling, some requests take 60 seconds and they will appear as outliers in this dashboard</li> <li>Kamon 2.x - System metrics dashboard: see info about CPU or memory usage, JVM metrics like Heap usage or GC</li> <li>Kamon 2.x - Akka: info about Akka system, actors, processing time</li> </ul> <p>To make these dashboards work, you may need to edit the dashboard variables </p> </li> </ul>"},{"location":"thehive/overview/","title":"Overview","text":""},{"location":"thehive/overview/#thehive-documentation","title":"TheHive Documentation","text":""},{"location":"thehive/overview/#overview","title":"Overview","text":"<p>TheHive offers a comprehensive 4-in-1 Security Incident Response Platform, serving as a vital tool for Security Operations Centers (SOCs), Computer Security Incident Response Teams (CSIRTs), Computer Emergency Response Teams (CERTs), and all information security professionals involved in swift and effective handling of security incidents. It composes of a robust suite of features designed to streamline incident response workflows, enhance collaboration, and empower information security practitioners to effectively investigate and mitigate security threats. With its seamless integration with MISP and advanced capabilities for task management, evidence handling, and threat intelligence integration, TheHive is an indispensable tool for modern SOC, CSIRT, and CERT teams.</p>"},{"location":"thehive/overview/#key-features","title":"Key Features","text":"<p>Integration with MISP:  Tightly integrated with MISP (Malware Information Sharing Platform) for seamless collaboration and information sharing.</p> <p>Real-Time Collaboration:  Multiple analysts can collaborate simultaneously with live stream updates on cases, tasks, observables, and Indicators of Compromise (IOCs).</p> <p>Efficient Task Management:  Special notifications enable efficient task handling and assignment, with previews and imports from various sources such as email reports, CTI providers, and SIEMs.</p> <p>Customizable Templates:  Create cases and tasks using a flexible template engine, allowing customization with metrics and custom fields to drive team activity and identify areas for automation.</p> <p>Evidence Management:  Analysts can record progress, attach evidence or files, add tags, and import password-protected ZIP archives containing suspicious data securely.</p> <p>Observables Management:  Easily add and manage observables, either individually or in bulk, with options to import directly from MISP events or alerts. Triaging and filtering capabilities streamline the process.</p> <p>Threat Intelligence Integration:  Utilize Cortex and its analyzers and responders to gain insights, accelerate investigations, and contain threats. Leverage tags, flag IOCs, and identify previously seen observables to enrich threat intelligence.</p>"},{"location":"thehive/overview/#architecture","title":"Architecture","text":"<p>TheHive can be set up on either a single server or as a cluster (a group of servers) to accommodate different levels of growth requirements.  TheHive's architecture is highly modular, allowing each layer (TheHive application, database &amp; indexing engine, and file storage) to be deployed independently as standalone nodes or as part of a clustered setup. This flexibility enables complex clustered architectures with virtual IP addresses and load balancers for optimal performance and scalability. </p> <p>The essential components of TheHive's setup include:</p> <ul> <li> Apache Cassandra for robust data storage, with support for version 4.x.</li> <li> Elasticsearch, serving as a powerful indexing engine, with support for version 7.x.</li> <li> A file storage solution, which can be the local filesystem of the server hosting the application for standalone setups or, NFS or S3 MINIO for clustered environments.</li> </ul> <p></p> <p>Using Lucene</p> <p>Starting from version 5.1, TheHive no longer supports the Lucene backend for indexing. Users who were previously utilizing Lucene with TheHive 4.1.x are advised to migrate their index to Elasticsearch using this comprehensive guide.</p> Standalone ServerCluster or Hybrid Architecture <p></p> <p>A standalone server setup involves installing all necessary components on a single server:</p> <ul> <li>Cassandra</li> <li>Elasticsearch</li> <li>File storage on the local filesystem (or MinIO if desired)</li> <li>TheHive</li> <li>Optional NGINX for managing HTTPS communications</li> </ul> <p>For detailed installation instructions, refer to the step-by-step installation guide.</p> <p>TheHive and its associated applications offer flexibility in choosing the right setup based on specific requirements. This includes the ability to mix and match different nodes and applications within a cluster.</p> <p></p> <p>Each layer and node within the architecture can be installed on dedicated operating systems, allowing for tailored configurations. The installation guide for a 3-node cluster provides comprehensive instructions for setting up a more complex clustered environment.</p> <p> </p>"},{"location":"thehive/release-notes/release-notes-5.0/","title":"Release Notes for Version 5.0","text":""},{"location":"thehive/release-notes/release-notes-5.0/#release-notes-of-50-series","title":"Release Notes of 5.0 series","text":""},{"location":"thehive/release-notes/release-notes-5.0/#5026-27th-february-2023","title":"5.0.26 - 27th February 2023","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fix issue that prevented closing a case when a custom field was not set</li> <li>Fix indentation issue in TTP list</li> <li>Fix overflow in pages with long content</li> <li>Fix loading of multiple comments</li> <li>Allow importing observables from analyzers when running on an alert</li> <li>Improve error message when closing a case with mandatory custom field</li> <li>Fix style for custom field input</li> <li>Typo fixes</li> </ul> <p>Backend:</p> <ul> <li>Fix v1 api for page which used the former v0 style: new fields were added and some fields are now marked as deprecated and will be removed in the future</li> <li>Add caseId in analyer input: field <code>message</code> should now contain the observable's parent id (alert or case)</li> <li>Fix a message serialization error when running in cluster</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5025-2nd-february-2023","title":"5.0.25 - 2nd February 2023","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_1","title":"Fixes","text":"<p>Backend:</p> <ul> <li>Fix TLP and PAP of MISP events</li> <li>Fix download of attachment when content type is malformed</li> <li>TheHive 3 migration: Fix parsing of old format of Cortex jobs</li> <li>Fix serialisation of configuration that contains a pipe character</li> </ul> <p>UI:</p> <ul> <li>Remove the limit on the number of techniques when selecting a TTP tactic</li> </ul> <p>Docker:</p> <ul> <li>Add parameter for Elasticsearch authentication</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5024-9th-january-2023","title":"5.0.24 - 9th January 2023","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_2","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fix issue in slack notifier form</li> <li>Fix colored tags in several parts of the UI: tags should now be more colorful</li> </ul> <p>Backend:</p> <ul> <li>When changing a tag name, the name is also changed in the cases and alerts</li> <li>When an alert or case changes of stage, the stage is added to the audit details</li> <li>Slack notifier now correctly sends the username</li> <li>Update analyzer reports</li> <li>Improve api docs</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5023-20th-december-2022","title":"5.0.23 - 20th December 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_3","title":"Fixes","text":"<ul> <li>Downgrade logback library to version 1.3.5 which still supports Java 8</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5022-19th-december-2022","title":"5.0.22 - 19th December 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_4","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fix: \"Add share\" window is empty when no target org</li> <li>Search: Observables search result direct link not working</li> <li>Search: results for Task Logs do not show a link to the case</li> <li>Markdown table issue with vertical bar: add support for <code>&amp;vert;</code> in cell tables</li> <li>Dashboard:<ul> <li>Bar Chart with tasks by status all have same colors</li> <li>Bar chart: add an option to display or not empty values</li> </ul> </li> <li>Long alert titles cut off other UI fields</li> <li>Bad cache when listing similar alerts</li> <li>Wrong drawer title when editing a case template</li> </ul> <p>Backend:</p> <ul> <li>Update dependencies</li> <li>API Query filter _startsWith don't work on customFields</li> <li>Pages in knowledge base incorrectly display pages from cases too</li> <li>Search on absent field does not use index</li> <li>Changing auth configuration does not work for v1 routes</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#improvements","title":"Improvements","text":"<ul> <li>LDAP/AD: add option to ignore the realm/domain of the login</li> <li>Cortex: Send observable attribute <code>sightedAt</code> to Cortex</li> <li>UI: in a task preview, add the possibility to edit or delete a task log</li> <li>UI: standardize disposition of TLP, PAP and SEV across forms and elements.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5021-2nd-december-2022","title":"5.0.21 - 2nd December 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_5","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fix JavaScript error</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5020-1st-december-2022","title":"5.0.20 - 1st December 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_6","title":"Fixes","text":"<p>UI:</p> <ul> <li>Dashboard:<ul> <li>Fix broken export as CSV for bar chart widget</li> <li>Remove some entities (patterns) that are not useful in dashboard view</li> <li>add live feed for dashboard</li> <li>re-enable aggregation on some fields and display a warning if the query is slow</li> <li>In widgets mark mandatory fields</li> <li>Some filter queries on string field (like title) were not correctly built</li> </ul> </li> <li>Admin: be less restrictive for urls (cortex and misp)</li> <li>Case sharing:<ul> <li>Fix request timing issue when sharing a case that prevented from sharing the tasks</li> <li>Update wording when sharing a case</li> </ul> </li> <li>When a customfield has options, prevent the user from selecting an other value</li> <li>Fix issue when updating custom fields in case templates</li> <li>Fix count of similar alerts </li> <li>When creating an observable the option \"one observable per line\" is now the default</li> <li>Fix duplicated refetch when updating an entity</li> <li>Fix live feed when updating a case</li> </ul> <p>Backend:</p> <ul> <li>Oauth2 connector will read the value for its proxy from <code>application.conf</code> &gt; <code>wsConfig.proxy</code></li> <li>Create audit logs when tasks are cancelled when closing a case</li> <li>In API allow status length of 64 chars for case and alerts (was wrongly 32 chars previously)</li> <li>Fix GDPR service that did not include all the tasks, task logs and observables</li> <li>Fix permission issue where analysts could no longer generate their Api Key (regression introduced in 5.0.19)</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5019-16th-november-2022","title":"5.0.19 - 16th November 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_7","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fix urls in mardown that contain <code>&amp;</code> character</li> <li>Platform configuration: <ul> <li>Webhook version was always set to 1</li> <li>Global endpoints can now be edited</li> </ul> </li> <li>Dashboards:<ul> <li>Fix issue with dashboards using relative times (like \"last 3 months\")</li> <li>Fix crash when deleting a widget in certain conditions</li> <li>Improve conversion of TheHive 4 dashboards</li> </ul> </li> <li>Display sightedAt date when an observable is set as sighted</li> </ul> <p>Backend:</p> <ul> <li>Fix permission issue for user edition</li> <li>Fix issue when trying to repair the index with a vertex containing non indexed fields</li> <li>Refresh license quotas faster after an organisation is locked</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#security-updates","title":"Security Updates","text":"<ul> <li>Update dependency on org.apache.commons:commons-text to 1.10.0</li> <li>Remove dependency on org.apache.ivy:ivy</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5018-27th-october-2022","title":"5.0.18 - 27th October 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#updates","title":"Updates","text":"<ul> <li>Update Cortex analyzer templates</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#fixes_8","title":"Fixes","text":"<p>UI:</p> <ul> <li>Dashboard:<ul> <li>Fix counter widget not working with observables</li> <li>Fix entity change not updating form changes</li> <li>Fix legends in dashboard that may be truncated</li> </ul> </li> <li>Custom fields options are shown even if the field is not empty</li> <li>Fix icons in analyser reports when serving TheHive on a non root path</li> <li>Fix issue when creating a case template with an already existing name</li> <li>Fix bad autocompletion on non tag fields</li> <li>Markdown tables now respect the alignment</li> <li>Fix drawer title when editing a dashboard</li> <li>Fix rare issue where the observable counter was not refreshed when an observable was added</li> </ul> <p>API docs</p> <ul> <li>Field <code>date</code> in Alert is now corectly marked as optional</li> <li>Add api docs for observable types</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_1","title":"Improvements","text":"<p>UI:</p> <ul> <li>Faster load of analyzer templates when viewing an analyzer report</li> <li>Sort search results by <code>_createdAt</code> date</li> <li>Sort analyzer list</li> <li>Dashboard:<ul> <li>color can now be customized for the field status</li> </ul> </li> </ul> <p>Backend:</p> <ul> <li>Make alert merging faster</li> <li>Fix chart time interval when empty series</li> <li>Set size limit on custom field values (8191 chars)</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5017-11th-october-2022","title":"5.0.17 - 11th October 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_9","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fix the search field in alert and case merging drawer</li> <li>Add missing validation button on the UI settings when choosing a date format</li> <li> <p>Fix case export button when MISP is available</p> </li> <li> <p>Dashboard:</p> <ul> <li>Fix filters during the import of dashboards, they was ignored</li> <li>Fix text and counter widget serie filters. They was ignored in 5.0.16</li> </ul> </li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_2","title":"Improvements","text":"<p>Backend:</p> <ul> <li>Use index to count the number of waiting tasks</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5016-7th-october-2022","title":"5.0.16 - 7th October 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_10","title":"Fixes","text":"<p>UI:</p> <ul> <li>Removed clickable description</li> <li>Fixed crash on alert merge in a new case</li> <li>Fixed rule to enable/disable button for the case export (MISP and archive capability)</li> </ul> <p>API:</p> <ul> <li>startDate in task set even if we go from Waiting to Completed</li> <li>Notify user when Cortex requires admin actions to update analyzers</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_3","title":"Improvements","text":"<p>UI:</p> <ul> <li>Dashboard:<ul> <li>Added time selection in custom period picker</li> <li>Improved performances of Text and Counter widgets</li> </ul> </li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5015-30th-septembre-2022","title":"5.0.15 - 30th Septembre 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_11","title":"Fixes","text":"<p>API:</p> <ul> <li>Fix \"none of\" filter which could return wrong result when using in dashboards</li> <li>Fix result of dashboard based on custom fields</li> </ul> <p>UI:</p> <ul> <li>Case count now takes filters into account</li> <li>Array and title panel in markdown edition was disappearing</li> <li>Deleting a filter had a weird behavior before (going back to the previous applied state), it now only deletes the selected filter</li> <li>Border for cortex/misp logo fixed for Safari 14</li> <li>Task completion bar is now refreshed when a task status is updated</li> <li>Fixed empty page sometimes when login</li> <li>Webhook endpoint version is now editable</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_4","title":"Improvements","text":"<p>API:</p> <ul> <li>Add \"alertsCount\" extraData in case list</li> </ul> <p>UI:</p> <ul> <li>Dashboard duplication feature has been added</li> <li>Count and link for similar cases/alert has been added to alert preview</li> </ul> <p>Description:</p> <ul> <li>textarea for edition is now resizable</li> <li>linebreak in text renders with a newline in markdown</li> <li>we can now click on the description to edit it</li> <li>font size when editing is now monospace</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5014-19th-september-2022","title":"5.0.14 - 19th September 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_12","title":"Fixes","text":"<p>API:</p> <ul> <li>Fix an issue where case template prefix was duplicated in case title</li> <li>Observables with files were not linked correctly to their attachments which impacted the merge of observables</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_5","title":"Improvements","text":"<p>UI:</p> <ul> <li>Notifications: display the name of the webhook</li> <li>Ability to filter users by profile when listing the users of an organisation</li> <li>In search page, add link to observable parent (case or alert)</li> <li>In live stream, display full date instead of 'x seconds ago'</li> <li>Dashboard:<ul> <li>Display users name instead of login</li> <li>Make time intervals aware of user timezone and locale, so that months and weeks aggregation behave correctly</li> </ul> </li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5013-8th-september-2022","title":"5.0.13 - 8th September 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_13","title":"Fixes","text":"<p>UI:</p> <ul> <li>Dashboard:<ul> <li>Remove some entities from the entity list for which dashboards were not usefull or working (like comments or actions)</li> <li>Clicking on a counter or donut now correctly sets the filters in the search page</li> <li>conversion of v4 dashboards failed with <code>not</code> operator</li> </ul> </li> <li>Improvement around time filters (custom and periods)</li> <li>Assignees from other organisations are visible and searchable in case assignment</li> <li>Fix bad date format when using am/pm</li> <li>Case sharing displayed an empty list of organisation</li> <li>Quote in the markdown editor only worked for the first line</li> <li>On small screen preview button was not visible when using Firefox</li> <li>Fix bug in list of TTP tactics</li> </ul> <p>API:</p> <ul> <li>Fix issue with integrity check of Observable</li> <li>Fix issues around Case Number Actor (responsible to give a sequential number for cases)</li> <li>Avoid locks when trying to fix inconsistencies in index</li> <li>Fix an issue that could happend during schema evolution</li> <li>Fix parsing of Cortex information (analyzers without dataType)</li> <li>Notifications: Email will now be sent in the format set by the text template (html or plain text)</li> <li>Prevent user ldap synchronisation if license is incompatible</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_6","title":"Improvements","text":"<p>UI:</p> <ul> <li>Notify user when an update in Cortex is available</li> <li>Dashboard:<ul> <li>Default period is now 3 months instead of \"All\"</li> <li>Remove some entities from the entity list for which dashboards were not usefull or working (like comments or actions)</li> <li>An org admin can disallow the \"All\" period in the dasboards (organisation settings &gt; UI Configuration)</li> <li>Widgets legends now show fully</li> </ul> </li> <li>Add an indicator on the number of open requests made by the browser</li> <li>Admin can now configure the default user domain from the admin settings</li> <li>Admin can change the type of a user (Service or Normal users)</li> <li>Improvements around the Proxy and SSL forms (used in misp, cortex or endpoints definition)</li> </ul> <p>API:</p> <ul> <li>Add the ability to log the content of query request</li> <li>Admin can now write in the knowledge base (the permission <code>manageKnowledgeBase</code> was added to the <code>admin</code> profile)</li> <li>Improve templating capabilites in notifiers:<ul> <li>Add <code>dateFormat</code> helper (<code>{{dateFormat audit._createdAt \"EEEEE dd MMMMM yyyy\" \"fr\" }}</code> =&gt; <code>jeudi 01 septembre 2022</code>)</li> <li>Helper <code>eq</code> now supports numbers: <code>{{#if (eq object.severity 2) }}MEDIUM {{else}}Other {{/if}}</code></li> <li>Add helpers <code>tlpLabel</code>, <code>papLabel</code>, <code>severityLabel</code>: <code>{{tlpLabel object.tlp}}</code> =&gt; <code>Amber</code></li> </ul> </li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5012-16th-august-2022","title":"5.0.12 - 16th August 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_14","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fix issue where case assignee was not displayed (because user was in an other organisation)</li> <li>Fix org switch not working when using http context (bug introduced in 5.0.11)</li> </ul> <p>API:</p> <ul> <li>Don't fail integrityChecks if field <code>_createdAt</code> is missing</li> <li>Sync cortex jobs with status <code>Deleted</code></li> <li>Don't fail queries if a phamtom vertex is encountered</li> </ul> <p>Migration:</p> <ul> <li>Fix issue where users could be duplicated by the migration 3-&gt;5 process</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_7","title":"Improvements","text":"<p>UI:</p> <ul> <li>Add field to set the expiration token duration for reset password links</li> <li>In Proxy forms, add input field to set the port of the proxy</li> </ul> <p>API:</p> <ul> <li>Alert can be created even if they contain duplicated observables of type files. Before it triggered an \"AlreadyExits\" error.</li> <li>Remove orphan cortex jobs (when observable was deleted)</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5011-5th-august-2022","title":"5.0.11 - 5th August 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_15","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fix import of .thar files</li> <li>When switching of organisation, the user is redirected to her homepage.</li> <li>Filters on number types showed <code>-1</code> in the preview</li> <li>In endpoints configuration, the setting \"Do not check Certificate Authority\" was not sent correctly</li> <li>Fix filters when using between dates with an hour or minute precision</li> </ul> <p>API:</p> <ul> <li>Cases created from an alert with a case template could have duplicated custom fields</li> <li>Cases created from an alert with a case template had duplicated tasks and prefix (from 5.0.10)</li> <li>Prevent failures during migration from v4 to v5: <ul> <li>TheHive will automatically reindex its data when a change in the index is detected (change from lucene to elasticsearch)</li> <li>TheHive will no longer try to run migrations when the setting <code>db.janusgraph.index.search.elasticsearch.bulk-refresh = false</code> is present</li> </ul> </li> <li>Fix issue where uploaded files like observable attachments were deleted from the server before being processed, resulting in \"File Not Found\" errors</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_8","title":"Improvements","text":"<p>UI:</p> <ul> <li>Improve the search experience when merging a case </li> <li>Ctrl+click on a case title in the list view will open the case in a new tab</li> </ul> <p>API:</p> <ul> <li>TheHive will try to fix ghost vertices when encountering one (a vertex that is present in the index but not in the database)</li> <li>a new field <code>extendedStatus</code> is passed to Cortex responders which represents the status of the Case which can be customized. The field <code>status</code> is still a fixed enumeration.</li> <li>Add integrity checks for <code>Share</code>s and <code>Role</code>s entities</li> <li>Some cortex jobs could be stuck in \"Waiting\" status: now when TheHive starts it will try to fix those jobs.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#5010-21st-july-2022","title":"5.0.10 - 21st July 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_16","title":"Fixes","text":"<p>UI:</p> <ul> <li>Dashboard:<ul> <li>fix style issue when loading the knowledge base at the same time</li> <li>update api used by bar widget to fix issues with custom fields query</li> </ul> </li> <li>Markdown preview in full screen mode now uses the full height of the screen</li> <li>Fix a filter when using enumeration (severity, pap)</li> <li>MISP: add settings in the UI to be able to export tags on Case and Observable from TheHive to MISP</li> <li>Cortex: <ul> <li>UI is notified when an analyzer has finished adding all the observables to a report</li> <li>Fix report view for Spamhaus analyzer</li> </ul> </li> </ul> <p>API:</p> <ul> <li>Fix issue that caused the user <code>system@thehive.local</code> to be deleted by the user integrity check: this could cause issues with MISP or Cortex synchronization (Cortex jobs triggered by notifiers were left in \"Waiting\" state). If the user was missing from your instance, it will be recreated after an update of TheHive. </li> <li>Increase the timeout for count requests to 10s and add a config to increase this value (<code>db.limitedCountTimeout</code>). If a count takes longer that this duration, a truncated result is returned.</li> <li>Dashboard queries: Filter on custom fields was not applied on the name of the custom field (only on the value)</li> <li>Fix an issue that prevented TheHive from starting when misp or cortex were not configured correctly (missing <code>http</code> scheme in url for instance)</li> </ul> <p>Docker:</p> <ul> <li>Fix issue with argument <code>--no-cql-wait</code> that caused the next argument to be ignored</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_9","title":"Improvements","text":"<p>UI:</p> <ul> <li>Responder jobs list are now sorted by start date </li> <li>Users in Assignee field are now sorted by alphabetical order</li> <li>When showing an observable, prevent from loading all the reports details when loading the job list. Report details are now loaded only when opening the report drawer.</li> </ul> <p>API:</p> <ul> <li>Add new parameters to the endpoint \"create a case from an alert\".</li> <li>When creating a case from an alert, the events <code>CaseCreated</code> and <code>AlertImported</code> are triggered once all the alert observables are imported inside the case.</li> <li>When merging alerts inside a case, triggger the event <code>AlertImported</code> once all observables for an alert are imported inside the case.</li> <li>Emailer will now send html emails instead of text emails. This means that you use advanced formatting and styles in your emails.</li> <li>Cortex job reports are not included in the list anymore. An extra data needs to be used to retrieve them.</li> </ul> <p>Docker:</p> <ul> <li>Misp and Cortex modules are enabled by default when using the entrypoint. (They are not enabled if you use <code>--no-config</code>)</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#509-1st-july-2022","title":"5.0.9 - 1st July 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_17","title":"Fixes","text":"<p>UI:</p> <ul> <li>Dashboard:<ul> <li>Fix radar when aggregating on custom fields</li> <li>Radar widget now uses a more efficient query</li> <li>Clicks on donut and counter widgets correctly sets the filters on the search page</li> </ul> </li> <li>Reset scroll when page is changed</li> <li>In alert observable list, remove the share column</li> <li>Fix french translation in quick views</li> <li>Fix links in search results for observables</li> </ul> <p>API:</p> <ul> <li>When merging an alert in a case, merge the tag reports for the observable</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_10","title":"Improvements","text":"<p>UI:</p> <ul> <li>In analyzer reports, add the tags on the observables</li> <li>Sort case template by name when creating a case</li> <li>Add support for cortex entities in charts and dashboard</li> <li>Related case view: sort cases by matching percentage</li> </ul> <p>API:</p> <ul> <li>Limit the duration of count queries not using the index to 5 seconds</li> <li>Add support for cortex entities for charts api</li> <li>Add TheHive version in logs</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#508-15-june-2022","title":"5.0.8 - 15 June 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#security","title":"Security","text":"<p>Fix issue related to AD/LDAP module.</p> <p>If you are using the ad/ldap authentification, you should update to TheHive 5.0.8 or later</p>"},{"location":"thehive/release-notes/release-notes-5.0/#fixes_18","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fixes issues around importing observables from an analyzer report</li> <li>Analyzer template list was not be exhaustive in certain cases</li> <li>Fix pagination and sorting when listing similar cases</li> <li>In related cases list, show several matching observables instead of only one</li> </ul> <p>API:</p> <ul> <li>Removed full text search on tags: that caused slow queries as the index cannot be used here</li> <li>Fixed regression from TH4: when merging alerts into a case, observables could be duplicated if they appeared in several alerts</li> <li>Add ability to filter and sort by case/alert status/stage on api v0</li> </ul> <p>Docker:</p> <ul> <li>Fix entrypoint for s3 configuration</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_11","title":"Improvements","text":"<p>API:</p> <ul> <li>Log login success and failures: those logs are useful for auditing purpose, to detect password guessing attacks via large unsuccessful logon attempts</li> <li>Prevent duplication of custom fields values</li> <li>Improve queries when filtering on a custom field with a high number of matches</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#507-31-may-2022","title":"5.0.7 - 31 May 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_19","title":"Fixes","text":"<p>UI:</p> <ul> <li>When creating a case from an alert, the correct case template is selected if the field was set in the alert</li> <li>Dashboard: fix an issue when converting a v4 dashboard</li> <li>Case count was not refreshed when adding a case</li> <li>Refresh comment section</li> <li>Misp configuration: Fix organisation selection</li> <li>Analyzer reports:<ul> <li>in an alert, display the extractable observables</li> <li>can now import an observable of type file</li> </ul> </li> <li>Custom Fields:<ul> <li>Limit the size of custom fields in list views</li> <li>use the display name in list views</li> </ul> </li> <li>When closing a case, custom fields are no longer deleted</li> </ul> <p>API:</p> <ul> <li>Fix breaking change in api V0: don't limit the size of observable data in json. This prevented the creation of files in observables. Note: with v1 the prefered way is to use a multipart request.</li> </ul> <p>Migration 3 to 5</p> <ul> <li>Fix migration of custom fields of type number</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_12","title":"Improvements","text":"<p>UI:</p> <ul> <li>Add ability to manually refresh a list when auto-refresh is disabled</li> <li>Notifications: Add a json validator when creating a custom filter</li> <li>Prevent automatic scroll when an entity is updated</li> <li>Fix flickering of updated data fields when updating an entity</li> <li>Other UI improvements</li> </ul> <p>API:</p> <ul> <li>Check for duplicated files (by filename) when attaching a file to a case or to a log</li> <li>Add field <code>stage</code> to alert and case in api v0</li> <li>Users can manage their own api key without the permission <code>manageUsers</code></li> <li>Add new auth mecanism based on htpasswd file</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#506-17-may-2022","title":"5.0.6 - 17 May 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_20","title":"Fixes","text":"<p>UI:</p> <ul> <li>List of analyzers did not refresh correctly for alert observables</li> <li>Fix error when trying to download a task log attachment with the char <code>{</code> in the name</li> <li>logout popin remained open after a reconnection</li> <li>API docs did not appear when setting an http context</li> <li>Fix an issue where breadcrumps were not displayed correctly</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#improvements_13","title":"Improvements","text":"<p>UI:</p> <ul> <li>Faster render of big markdown section: use <code>marked-react</code> library instead of <code>react-markdown</code></li> <li>Adjust fanged message</li> <li>Improve sorting of tasks when in group mode</li> <li>Update observable count when an observable is added or removed</li> <li>Improvements for \"Required Action\" on tasks</li> </ul> <p>Migration 4 to 5:</p> <ul> <li>Improve migration for custom fields where previous script could overload the application</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#505-5-may-2022","title":"5.0.5 - 5 May 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_21","title":"Fixes","text":"<p>UI:</p> <ul> <li>Analyzer templates: optimize rendering time</li> <li>Fixed SSO login when using an http context</li> </ul> <p>API:</p> <ul> <li>Dashboard: increase number of retrieved values for aggregations (eg. chart on custom field values)</li> <li>Fix for permission <code>manageConfig</code></li> <li>Improve support for AWS Keyspace: add retries on some failed queries</li> <li>Fix endpoint for deletion of catalog of ttp</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#504-3-may-2022","title":"5.0.4 - 3 May 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#fixes_22","title":"Fixes","text":"<p>UI:</p> <ul> <li>Tasks are now displayed by their order</li> <li>Changed color of field for search by case id</li> <li>Fixed an issue where custom fields were deleted when editing a case template</li> <li>Fix download link of attachments when http context is set</li> <li>Update of vulnerable libraries</li> <li>When using bulk edit, \"Add tags\" and \"Remove tags\" now work</li> </ul> <p>API:</p> <ul> <li>Fixed bug introduced in 5.0.3 where TTPs were not linked to their tactics and reported <code>&lt;unknown&gt;</code></li> <li>It's now possible to delete the title prefix in a case template</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#503-15-april-2022","title":"5.0.3 - 15 April 2022","text":""},{"location":"thehive/release-notes/release-notes-5.0/#new-features","title":"New Features","text":"<p>API:</p> <ul> <li>Add support to procedures (TTPs) when creating alerts in TheHive</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#fixes_23","title":"Fixes","text":"<p>UI:</p> <ul> <li>When importing a case from misp: additional parameters (custom fields, shares) are correctly sent</li> <li>When converting an alert to case, custom fields are no longer lost during the process</li> <li>Update moment.js library</li> </ul> <p>Migration tool:</p> <ul> <li>Fix typo in migration tool configuration</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#502-8-april-2022","title":"5.0.2 - 8 April 2022","text":"<p>We found a bug that prevents a user from using the reset pasword flow (present in 5.0.0). We recommend all 5.0.x users to update to this version.</p> <p>Fixes:</p> <ul> <li>Fix 404 page during the reset password flow</li> <li>Dashboard: include end date in time interval</li> <li>Fix detached live feed when using an http context</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.0/#501-7-april-2022","title":"5.0.1 - 7 April 2022","text":"<p>TheHive 5.0.1 is the first patch release in the 5.0 series. It contains fixes and improvements for the UI and some small changes for the API compared to 5.0.0.</p> <p>We recommend all 5.0.0 users to update to this version.</p>"},{"location":"thehive/release-notes/release-notes-5.0/#notables-changes","title":"Notables changes","text":"<p>UI:</p> <ul> <li>fix description display on search page</li> <li>display org admin tabs only with required permissions</li> <li>fix permissions checks in the application</li> <li>improve case sharing user experience</li> <li>notifications - fix urls of http endpoints</li> <li>notifications - improve editor when using template variables</li> <li>be able to download a file attachment for an alert</li> <li>forms and drawers don't lose user data when the entity is refreshed by the feed</li> <li>improve live feed for cortex jobs on observables</li> </ul> <p>API:</p> <ul> <li>add ability to create alert with observables (of type string and files), see. API docs for more information</li> <li>rename field user to assignee in case creation</li> <li>rename field customFieldValues to customFields in alert creation</li> </ul> <p>Backend:</p> <ul> <li>fix tag edition</li> <li>fix permissions check for observables in case of sharing</li> <li>fix user deletion (user could be left without org)</li> <li>fix license reload on cluster nodes</li> <li>add more check to ensure uniqueness of data</li> <li>increase quotas for service users (set to unlimited) and cluster nodes (unlimited for platinum plan)</li> <li>update of dependencies</li> </ul> <p>Docs:</p> <ul> <li>fix url to website</li> <li>add documentation for MISP endpoints</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/","title":"Release Notes for Version 5.1","text":""},{"location":"thehive/release-notes/release-notes-5.1/#release-notes-of-51-series","title":"Release Notes of 5.1 series","text":"<p>Danger</p> <p>The 5.1 release comes with some changes on the database schema that can't be reversed. Please make sure to make a backup of your database before upgrading.</p> <p>This release also comes with some breaking changes, please review them below</p> <p>Info</p> <p>An upgrade guide is available to help you migrate from TheHive 5.0</p>"},{"location":"thehive/release-notes/release-notes-5.1/#5112-9th-april-2024","title":"5.1.12 - 9th April 2024","text":"<ul> <li>Fix a regression following a security fix that made the MFA authentication impossible.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#5111-4th-april-2024","title":"5.1.11 - 4th April 2024","text":"<ul> <li>Fix security vulnerability. An advisory will be published in the coming weeks.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#5110-21st-december-2023","title":"5.1.10 - 21st December 2023","text":""},{"location":"thehive/release-notes/release-notes-5.1/#fixes","title":"Fixes","text":"<p>Security:</p> <ul> <li>Fix security vulnerabilities. Please read the detailed advisories.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#519-29th-june-2023","title":"5.1.9 - 29th June 2023","text":""},{"location":"thehive/release-notes/release-notes-5.1/#fixes_1","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fix display of task attachments in attachment listing</li> <li>Improve error message when merging cases</li> </ul> <p>Backend:</p> <ul> <li>Fix error \"Tag not Found\" which could occur when creating alerts and cases</li> <li>Fix link duplication when importing several times a TTP catalog</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#improvements","title":"Improvements","text":"<p>UI:</p> <ul> <li>Improve form for SAML authentication</li> </ul> <p>Backend:</p> <ul> <li>Add MISP event UUID to alert description in imported alerts</li> <li>Add validation for SAML configuration</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#518-20th-june-2023","title":"5.1.8 - 20th June 2023","text":""},{"location":"thehive/release-notes/release-notes-5.1/#fixes_2","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fix the order of Cortex job reports in observable page</li> <li>Add events related to case templates in live feed</li> <li>Change rules of user quota</li> <li>Display the callback URL in SAML configuration</li> <li>Fix icon in timeline</li> <li>Fix format of some old generated dashboard widgets</li> </ul> <p>Backend:</p> <ul> <li>Change format of the date custom field values Ids</li> <li>Fix between filter on custom fields</li> <li>Ldap sync: add support of several mapping for the same group</li> <li>Ldap sync: fix conflict when several configurations exist with the same suffix</li> <li>Cortex job endDate is not set if the job is not finished</li> <li>Fix filter value when it contains only wilcards</li> <li>Make readonly profile not editable</li> <li>Add properties for alert KPIs</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#517-1st-june-2023","title":"5.1.7 - 1st June 2023","text":""},{"location":"thehive/release-notes/release-notes-5.1/#fixes_3","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fixes on long names</li> </ul> <p>Backend:</p> <ul> <li>Link correctly task and assignee when updating a task</li> <li>Limit the size of cortex response</li> <li>Cortex: Fix sync issue when multiple jobs are submitted at the same time</li> <li>Fix full text filter for custom fields</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#improvements_1","title":"Improvements","text":"<p>UI:</p> <ul> <li>Allow search for task assignee input</li> </ul> <p>Backend:</p> <ul> <li>Optimize resource usage when reading cortex jobs </li> <li>Replace default \"Job\" dashboard with a better TTP dashboard</li> <li>Cortex:<ul> <li>Add timeout for jobs (3 hours by default)</li> </ul> </li> <li>Deduplicate custom field values on case merge</li> <li>Increase observable data length in api v1 to 4096 chars</li> </ul> <p>Migration tool:</p> <ul> <li>Improve mapping of TheHive 3 Alert status </li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#security","title":"Security","text":"<ul> <li>Update libraries</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#516-15th-may-2023","title":"5.1.6 - 15th May 2023","text":""},{"location":"thehive/release-notes/release-notes-5.1/#fixes_4","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fix edition of property \"includeInTimeline\" for task activities</li> <li>Fix case template search when creating a new case</li> <li>Admin: show username and organisation name as required in forms</li> <li>other small fixes</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#improvements_2","title":"Improvements","text":"<p>API:</p> <ul> <li>Improve performances in some areas</li> <li>Replace org default dashboard \"jobs\" with TTP</li> <li>Improve performance when running notifiers</li> <li>It's now possible to upload two attachments with the same name in a case (the second attachment will be renamed automatically)</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#security_1","title":"Security","text":"<ul> <li>Update libraries and remove unused dependencies</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#515-27th-april-2023","title":"5.1.5 - 27th April 2023","text":""},{"location":"thehive/release-notes/release-notes-5.1/#fixes_5","title":"Fixes","text":"<p>UI:</p> <ul> <li>Case template: fix issue where prefix was deleted when editing a template</li> <li>Sharing: Fix display of share options on small display</li> <li>Responder reports should now appear in the observable preview when triggered</li> </ul> <p>Backend:</p> <ul> <li>Fix Case dates when creating a case with status \"Closed\"</li> <li>Fix issue that created phamtom tags on alerts or case</li> <li>Fix issue that removed field <code>_id</code> from audits sent via notifiers (like webhook)</li> <li>Fix empty response when using the extraData <code>similarAlerts</code> on case queries</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#improvements_3","title":"Improvements","text":"<p>UI:</p> <ul> <li>Better user selector</li> <li>Tasks: redirect to task list after deleting a task</li> <li>Change wording in search bar</li> <li>Alert list: add capability to filter by assignee when clicking on an assignee</li> <li>Filters: assignee field will filter the dropdown list when typing</li> <li>Functions: several improvments</li> <li>Comments: use \"Enter\" to save a comment</li> <li>Responders: Sort responder reports by startDate</li> </ul> <p>API:</p> <ul> <li>Better error codes when making queries or when sending an http entity too large</li> <li>Allow attachment download using its <code>id</code> or <code>_id</code></li> <li>Fix some errors in the documentation</li> </ul> <p>Backend:</p> <ul> <li>Improve tag creation performances</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#514-13th-april-2023","title":"5.1.4 - 13th April 2023","text":""},{"location":"thehive/release-notes/release-notes-5.1/#fixes_6","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fix crash when live feed is opened and a case is deleted</li> <li>Pages: several images with the same name can now be added (also fixes an issue with pasted images)</li> <li>Hide locked users in task assignee</li> <li>Custom fields of type boolean now display a set of options</li> <li>You can now press <code>Enter</code> to save a comment</li> <li>Fix report template download link</li> </ul> <p>Backend:</p> <ul> <li>Fix potential memory leak that could lead to a crash (from 5.1, issue not present in 5.0)</li> <li>Return error 413 when http input payload is too large</li> <li>Report error when email sending fails during password reset</li> <li>Fix api documentation for operator <code>_between</code> in queries</li> <li>Add field <code>_updatedBy</code> in comments</li> <li>Don't ignore cortex jobs when the report contains an unknown operation</li> <li>Deprecate page slugs (in api docs only)</li> </ul> <p>Warning</p> <p>In our next major release 5.2, the field <code>slug</code> for object <code>Page</code> will be removed. We are starting to deprecate it starting from this release</p>"},{"location":"thehive/release-notes/release-notes-5.1/#513-29th-march-2023","title":"5.1.3 - 29th March 2023","text":""},{"location":"thehive/release-notes/release-notes-5.1/#fixes_7","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fix filtering of case templates when creating a case</li> <li>When importing a case template that already exists, ask for a new name</li> <li>In dashboard list, be able to filter on group name</li> <li>Fix task list not refreshed when applying a case template on a case</li> <li>Fix url of popup livefeed when the application uses an http context</li> <li>Avoid tags deletion when editing a case template</li> </ul> <p>Backend:</p> <ul> <li>Improve error message when merging case</li> <li>Fix sharing not applied after merging cases</li> </ul> <p>Docker:</p> <ul> <li>Fixes <code>--storage-directory</code> option</li> <li>Don't show cassandra password</li> <li>Correctly escape Elasticsearch password</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#512-14th-march-2023","title":"5.1.2 - 14th March 2023","text":""},{"location":"thehive/release-notes/release-notes-5.1/#fixes_8","title":"Fixes","text":"<p>Backend:</p> <ul> <li>Fix SSO user autocreate</li> <li>Improve the application behavior when Janusgraph configuration gets updated while another connection exists</li> <li>Fix permission checks for case template creation</li> <li>Fix permission checks on pages</li> <li>Improve the observable type length check</li> </ul> <p>Cortex connector:</p> <ul> <li>Send the submit message to Cortex job actor when the transaction is committed</li> <li>Improve Cortex pending jobs recovery</li> <li>Fix: Recover only 100 jobs at startup. When the queue is empty, recover 100 more jobs.</li> <li>Updates of job with status \u201cDeleted\u201d are retried</li> <li>Improve logs</li> </ul> <p>MISP connector:</p> <ul> <li>MISP synchronisation misses some events in case of timeouts during synchronization</li> </ul> <p>UI:</p> <ul> <li>Refresh the task list when after bulk editing of assignee</li> <li>Fix tooltip overlap in some dashboard widgets</li> <li>Dashboard labels for cases/alerts are mixed</li> <li>Remove owner field from dashboard import drawer</li> <li>Fix the UI refresh when launching an analyzer job</li> <li>Fix incorrect search when click on donut due to persisting keyword \"domain\"</li> <li>Fix Customfield selector in case template editor</li> <li>Improve notifier name and suffix with using an existing endpoint</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#511-3rd-march-2023","title":"5.1.1 - 3rd March 2023","text":""},{"location":"thehive/release-notes/release-notes-5.1/#fixes_9","title":"Fixes","text":"<p>API:</p> <ul> <li>Fix a change in the observable creation API (regression from 5.0)</li> </ul> <p>UI:</p> <ul> <li>Date fields can be set using keyboard input</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#510-1st-march-2023","title":"5.1.0 - 1st March 2023","text":""},{"location":"thehive/release-notes/release-notes-5.1/#breaking-changes","title":"Breaking changes","text":"<ul> <li> <p>Remove support for Hadoop for filestorage</p> <p>From this new release, Hadoop can no longer be used a file storage (it was removed from 5.0 documentation but could still be used)</p> </li> <li> <p>Drop support for java 8</p> <p>Java 8 version is no longer supported by TheHive. Please update to java 11 at least Our setup guide can help you on how to install a jvm</p> </li> <li> <p>Drop support for Lucene as index backend</p> <p>Former versions of TheHive supported lucene and elasticsearch as indexing engines for the data. We then encountered limitations while using the Lucene index (especially when making queries based on Custom Fields). With TheHive 5.0, we pushed users to install and migrate to Elasticsearch. Finally with TheHive 5.1, the support of Lucene index is removed: the application will start but queries involving Custom Fields will return wrong results. To migrate your index to Elasticsearch, follow this guide.</p> </li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#main-features","title":"Main features","text":"<ul> <li> <p>Apply case template on existing case: enrich a case with tasks, custom fields, tags from other case templates</p> <p>This will allow your organisation to create more reusable case templates and apply them during the lifecycle of a case</p> </li> <li> <p>KPIs</p> <p>Get more indicators about your organisation response: Time to Respond, Time to Acknowledge ...</p> </li> <li> <p>Global search</p> <p>You can now search on all elements of the database instead of choosing a scope</p> </li> <li> <p>Custom Field db model update</p> <p>Database model was updated to be able to better support dashboard and queries based on custom fields. Now dashboard using filters or aggregation on custom fields should be way faster</p> </li> <li> <p>New permissions</p> <p>Split some permissions to make them more granular. For instance <code>manageAlert</code> was split into 4 permissions: <code>manageAlert/create</code>, <code>manageAlert/delete</code>, <code>manageAlert/update</code>, <code>manageAlert/import</code>. Case permissions were also split</p> </li> <li> <p>History list for object</p> <p>Added a new tab in the UI to list the changes made on an Alert or Case.</p> </li> <li> <p>Mandatory Tasks</p> <p>Some tasks can be defined as mandatory. To close a case, those tasks need to be closed and contain at least one activity.</p> </li> <li> <p>SAML Support</p> <p>You can now use SAML as an SSO source for your users (requires platinum license)</p> </li> <li> <p>Functions (Beta)</p> <p>See dedicated page for more information (requires platinum license)</p> </li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#other-features","title":"Other features","text":"<ul> <li>New type of custom fields: url</li> <li>Change case ownership</li> <li>Similarity between observable now works with observable of kind attachment</li> <li>Improve Cortex connector resources usage</li> <li>Dashboards on org creation: default dashboards are now provided to new orgs</li> <li>Add cache for dashboard</li> <li>Auto import observables from Cortex: when an analyzer extract observables into its report, it can flag some observable to be automatically imported into the case/alert</li> <li>Experimental support for Elasticsearch 8</li> <li>Rework UX to add TTP</li> <li>Move interface date format to user settings (and localstorage) instead of org settings</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.1/#fixes_10","title":"Fixes","text":"<ul> <li>Notifications improvement: notifications are now triggered for each observable when creating an alert with multiple observables</li> <li>Several other fixes in the application and UI</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/","title":"Release Notes for Version 5.2","text":""},{"location":"thehive/release-notes/release-notes-5.2/#release-notes-of-52-series","title":"Release Notes of 5.2 series","text":"<p>Danger</p> <p>The 5.2 release comes with some changes on the database schema that can't be reversed. Please make sure to make a backup of your database before upgrading.</p> <p>This release also comes with some breaking changes, please review them below</p> <p>Info</p> <p>An upgrade guide is available to help you migrate from TheHive 5.x</p>"},{"location":"thehive/release-notes/release-notes-5.2/#5214-9th-april-2024","title":"5.2.14 - 9th April 2024","text":""},{"location":"thehive/release-notes/release-notes-5.2/#fix","title":"Fix","text":"<ul> <li>Fix a regression following a security fix that made the MFA authentication impossible.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#5213-5th-april-2024","title":"5.2.13 - 5th April 2024","text":""},{"location":"thehive/release-notes/release-notes-5.2/#security","title":"Security","text":"<ul> <li>Fix Username Enumeration vulnerability.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#fix_1","title":"Fix","text":"<p>Filter/Search</p> <ul> <li>Fixed the search when using a \"not\" operator on full-text data.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#5212-12th-march-2024","title":"5.2.12 - 12th March 2024","text":""},{"location":"thehive/release-notes/release-notes-5.2/#fixes","title":"Fixes","text":"<p>Cortex connector fixes:</p> <ul> <li>Enhanced Stability: Fixed an issue with the Cortex connector that had issues after updating it's configuration. Additionally, the \"Test connection\" button is no more preventing to cancel the configuration defined in the drawer.</li> </ul> <p>Notification fixes and enhancements:</p> <ul> <li>Slack notifier: Slack notifications are now sending the accurate case titles. Also, we fixed an issue with the \"Advanced settings\" that was sent to the backend, even if the box wasn't checked.</li> <li>HTTP notifications: Fixed interpretation of backslashes, ensuring correct rendering and transmission of notifications without unexpected behavior.</li> </ul> <p>Performances improvements:</p> <ul> <li>Similar Cases/Alerts pages: Optimized the Similar Cases/Alerts pages by limiting the correlation count, improving performances and preventing the application ressource exhaustion.</li> </ul> <p>MISP Connector:</p> <ul> <li>Observables export enhancement: Mapping between TheHive observables dataTypes and MISP attributes types has been reworked.</li> </ul> <p>User Experience Enhancements:</p> <ul> <li>Quick filters: Resolved issues with open case quick filter functionality.</li> <li>Shared tasks: Fixed display of User ID on task logs.</li> <li>OAuth2 authentication: We improved the drawer by renaming some fields to be more explicit. Additionally, fixing a bug that was preventing to disable the OAuth2 authentication provider.</li> <li>Dashboard donut widget: Fixed an issue with the category filter in the donut widget in dashboards.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#security-updates","title":"Security updates","text":"<p>The library dependencies has been updated. The following vulnerabilities has been removed: - CVE-2020-8908 - CVE-2023-2976 - CVE-2023-46749 - CVE-2023-34454 - CVE-2023-34455 - CVE-2023-43642 - CVE-2023-5072 - CVE-2022-45688 - CVE-2023-44483 - CVE-2023-34462 - CVE-2023-3635 - CVE-2023-46120</p>"},{"location":"thehive/release-notes/release-notes-5.2/#5211-8th-february-2024","title":"5.2.11 - 8th February 2024","text":""},{"location":"thehive/release-notes/release-notes-5.2/#fixes_1","title":"Fixes","text":"<p>User Interface:</p> <ul> <li>Dashboard Revitalization: The Donut widget shows again the total.</li> <li>Streamlined Dashboard Filtering: We've refined dashboard filtering to utilize associated field text instead of custom label text, enhancing usability and clarity in data presentation.</li> <li>Observables Management: only protect http(s) string if it is part of a url.</li> <li>Improved Linked Alert: In the Linked Alerts tab, all types of alerts, whether with or without observables, are now seamlessly displayed for enhanced visibility and swift action.</li> <li>Efficient Alert Assignment: Fix bulk assignment issues for alerts, ensuring smooth and efficient workflow management.</li> </ul> <p>API:</p> <ul> <li>Seamless Integration with MISP: Experience effortless file upload to MISP with identical files from TheHive, streamlining your incident response workflow and ensuring data consistency across platforms.</li> <li>Custom tags: fix tags global integrity check to delete orphan custom tags</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#improvements","title":"Improvements","text":"<p>SAML Authentication Enhancement:</p> <ul> <li>Custom Attribute Support: Introducing a new custom attribute - the Login Name field - in SAML forms, providing greater flexibility and customization options for user authentication.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#5210-10th-january-2024","title":"5.2.10 - 10th January 2024","text":""},{"location":"thehive/release-notes/release-notes-5.2/#fixes_2","title":"Fixes","text":"<p>API:</p> <ul> <li>Performance Enhancement: Improved performance when loading case pages with similar/related cases or alerts. Displays \"99+\" and seamlessly loads without blocking the page.</li> <li>TheHive Alert Creation: Fixing an observable deduplication issue while creating alerts. Merged description tags unless identical.</li> <li>Similar Cases Matching: Fixed a bug where in specific scenario, a case could match itself as similar.</li> </ul> <p>MISP:</p> <ul> <li>Optimized MISP Synchronization: Resolved abnormal CPU consumption during MISP synchronization when an event contains many observables.</li> </ul> <p>UI:</p> <ul> <li>Search of Knowledge Base Fix: Resolved the issue causing a \"white screen\" when searching in the Knowledge Base.</li> <li>Log Code Formatting: Fixed the code display in logs to correctly return to the line when exceeding a specified size component.</li> <li>Dashboard Memory Leak: Addressed a memory leak in the dashboard, ensuring that memory consumption does not increase.</li> <li>Cortex Analyzer/Responder Report Fix: Rectified the error message \"id undefined\" in Cortex Analyzer/Responder reports.</li> <li>Organization Name Trimming: Trimmed organization names when creating a new organization to prevent unnecessary spaces.</li> </ul> <p>Dashboard: - Customizable Fragment Display: Take control of your visualizations by choosing the number of fragments to display on the Donut. Tailor the dashboard to your preferences for a personalized and efficient user experience.</p> <p>Docker:</p> <ul> <li>Docker User Fix: Docker containers are now using \u201cthehive\u201d user instead of root. sudo is used when necessary to address file permission issues.</li> </ul> <p>Packages:</p> <ul> <li>Dependency Updates for RPM/DEB Packages: Updated dependencies for RPM/DEB packages, ensuring compatibility with java11-runtime-headless provided by both packages.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#new-features-improvements","title":"New Features / Improvements","text":"<ul> <li>Case Status Modification: Users can now change the status of a case from \"Closed\" to any other status without reopening the case.</li> <li>SAML Authentication Enhancement: Improved SAML authentication by allowing minutes to be used in the Maximum Authentication Lifetime.</li> <li>Observable Management from UI: Introduced the ability to add/edit observables directly into an alert from the user interface for enhanced flexibility.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#529-21st-december-2023","title":"5.2.9 - 21st December 2023","text":""},{"location":"thehive/release-notes/release-notes-5.2/#fixes_3","title":"Fixes","text":"<p>Security:</p> <ul> <li>Fix security vulnerabilities. Please read the detailed advisories.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#528-23rd-november-2023","title":"5.2.8 - 23rd November 2023","text":""},{"location":"thehive/release-notes/release-notes-5.2/#fixes_4","title":"Fixes","text":"<p>API:</p> <ul> <li>MISP Sync: Fixed a bug introduced in the latest version which causes TheHive to crash when parsing MISP events with the fields <code>first_seen</code> or <code>last_seen</code>. This bug does not happen in earlier versions (&lt;= 5.2.6) or if those fields are not present in the MISP event</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#527-2nd-november-2023","title":"5.2.7 - 2nd November 2023","text":""},{"location":"thehive/release-notes/release-notes-5.2/#fixes_5","title":"Fixes","text":"<p>API:</p> <ul> <li>LDAP Sync Enhancement: We've fine-tuned our LDAP sync functionality to ensure that it doesn't overwrite critical user fields like API key and TotpSecret.</li> <li>Attachment Upload: Fixed an issue where the empty files was discarded by default. TheHive now accept empty files by default.</li> </ul> <p>User Interface:</p> <ul> <li>Case Reporting CSS Fix: We've corrected a CSS tag name for high severity cases, which fix a display issue in the reports.</li> <li>Alert Responders Tab Enhancement: Fixing issues on the responder reports list display.</li> <li>Filter Preview Improvement: Lengthy filters do not break the display in the UI anymore</li> <li>Report Template Enhancement: We've added a more intuitive format for Custom Fields variables in report templates, introducing the format case.customFieldValues.name</li> <li>SAML Information Display: Fixed a bug where the session lifetime parameter wasn't displaying the actual value.</li> </ul> <p>New Features / Improvements:</p> <ul> <li>Alert Assignee:<ul> <li>Alert auto assignment at start/closure: If the alert is yet unassigned, TheHive now automatically set the assignee to the user that is performing the action of starting/closing the alert.</li> </ul> </li> <li>MISP Integration:<ul> <li>Improved MISP Status and Sync: We've revamped MISP status and synchronization reports to provide real-time insights into ongoing synchronizations. Stay informed about the status of your data exchanges.</li> </ul> </li> <li>Task Management:<ul> <li>Extended Task Group Name: Increasing task groups with names size up to 64 characters in length (instead of 32), allowing more flexibility when organizing incident response tasks and playbooks.</li> </ul> </li> <li>Case Reporting:<ul> <li>Docx Case Report Generation: Generate case reports in the popular .docx format, making it easier to customise post generation and share your cases reports with stakeholders. NB: No report preview available for this format.</li> </ul> </li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#526-19th-october-2023","title":"5.2.6 - 19th October 2023","text":""},{"location":"thehive/release-notes/release-notes-5.2/#fixes_6","title":"Fixes","text":"<p>User Interface:</p> <ul> <li>SAML Configuration: We've enhanced security by checking the format of the 'name' SAML configuration field before creating the 'callback' URL. Special characters and uppercase letters are now automatically replaced for improved compatibility.</li> <li>Alert Similar Cases Query: We've resolved an issue where the similar cases query was not fetching results due to the length of default filters.</li> <li>Duplicate Observables: Multiple alert imports no longer lead to duplicate observables, as we now deduplicate them automatically.</li> <li>MISP Integration:<ul> <li>MISP Attribute Handling: When exporting a case, MISP attributes of observables are now set correctly, ensuring seamless integration with MISP.</li> <li>Event Editing Strategy: We've implemented a comprehensive event edit strategy to enhance your MISP experience.</li> </ul> </li> <li>Comments: Text comments without spaces now wrap correctly within the component, ensuring a neat and organized display.</li> <li>Authentication: Fixed a bug that did not correctly remove the link between two organisations, which could lead to connection problems.</li> </ul> <p>API:</p> <ul> <li>Cortex Responders: Cortex responders now show up for Tasks and Task Logs</li> <li>Duplicate Observables: When merging multiple alerts into a case, observables are now better duplicated</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#new-features-improvements_1","title":"New Features / Improvements","text":"<ul> <li>Date fields: All date fields in the application can now be set to hours and minutes.</li> <li>Menu Label Updates: We've made menu label changes for clarity and consistency. \"Related Alerts\" is now \"Linked Alerts,\" and \"Related Cases\" has become \"Similar Cases.\" URL updates accompany these changes for a seamless transition.</li> <li>Alerts Attachment: You can now add attachments to alerts using a dedicated tab. Attachments can be merged and imported into cases for better incident tracking.</li> <li>Sorting and Filtering: Within the attachment tab for cases and alerts, you have the ability to sort attachments by ascending or descending order and apply filters for efficient management.</li> <li>New Permissions: We've introduced two new permissions, \"Manage Reopen Case\" and \"Manage Restart Alert.\" This feature allows specific profiles to block access to reopening cases or restarting alerts. By default, these permissions are enabled for all profiles with \"Manage Update\" enabled.</li> <li>TTPs : It is now possible to create TTPs via the API without specifying tactics. If only one tactic matches the technique, it will be automatically defined by the system. Otherwise, a message will indicate that the tactic is missing, leaving the user free to specify it via the interface. Editing is now possible on TTPs.</li> <li>Detailed Browser Tab Content: Identify page content at a glance with improved browser tab titling</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#525-5th-october-2023","title":"5.2.5 - 5th October 2023","text":""},{"location":"thehive/release-notes/release-notes-5.2/#fixes_7","title":"Fixes","text":"<p>API:</p> <ul> <li>Cortex Responders: Resolved an issue related to Cortex responders not triggering on TLP:RED (4) cases due to a compatibility issue with TheHive's switch to TLPv2 while Cortex was using TLPv1.</li> </ul> <p>User Interface:</p> <ul> <li> <p>Cases: </p> <ul> <li>Improved Sorting: Now you can sort the list of related cases by title, date, and observables, providing better case management flexibility.</li> <li>Multi-Case Closure Fix: Fixed a problem that previously interchanged values when closing multiple cases simultaneously, ensuring accurate data handling.</li> <li>Merged Case Closure Fix: Fixed a problem that prevented a merged folder from being closed due to a new mechanism for deduplicating similar tasks during merging.</li> </ul> </li> <li> <p>Analyzers: Resolved a problem related to the selection of analyzers to launch on an observable after a previous analyzer had finished.</p> </li> <li>Global Search: The task log search results now correctly display the link to the task within the originating case.</li> <li>Date Display Format: Fixed a problem where the date format defined in the user profile was not being taken into.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#new-features","title":"New Features","text":"<ul> <li>Case URL option in MISP Connector: When exporting to MISP, TheHive could includes the case URL as an internal reference, enhancing traceability and information management.</li> <li>Session Duration Management: Introduced enhanced session termination and inactivity timeout management. Now, you can define session end and inactivity timeout times effectively, and even include a user warning message before session termination.<ul> <li>Read-only profiles will not be disconnected due to inactivity if they are connected to a dashboard and only to a dashboard. They will, however, be disconnected due to the end of a classic session.</li> </ul> </li> <li>Quick Assign to Me: A new action allows for quick assignment of cases or alerts directly from their details, streamlining task management.</li> <li>Cases: <ul> <li>Display number of alerts: The number of alerts imported into a case is now displayed in the case list information, providing valuable information at a glance.</li> <li>Attachment Previews: Preview HTML and Markdown attachments directly from the attachments list. Additionally, case reports are previewable from the report tab's attachments list.</li> </ul> </li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#524-19th-september-2023","title":"5.2.4 - 19th September 2023","text":""},{"location":"thehive/release-notes/release-notes-5.2/#fixes_8","title":"Fixes","text":"<p>API:</p> <ul> <li>Notification: The \"JobFinished\" trigger will no longer be triggered when the job is updated, but only when the responders have finished.</li> </ul> <p>User Interface:</p> <ul> <li>User Management: Administrators will now receive a warning message when attempting to create a new user with an existing login.</li> <li>Case Template: When applying a case template with tasks to an existing case with tasks, the template's tasks will be placed at the end.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#new-features_1","title":"New Features","text":"<ul> <li>Notification: You can now trigger a notification when an analyzer has finished using the \"ActionFinished\" trigger.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#523-5th-september-2023","title":"5.2.3 - 5th September 2023","text":""},{"location":"thehive/release-notes/release-notes-5.2/#fixes_9","title":"Fixes","text":"<p>API:</p> <ul> <li>Case Closure Enhancement: The case closing API has been enhanced to ensure that mandatory custom fields are filled in before a case can be closed.</li> </ul> <p>User Interface:</p> <ul> <li>Taxonomy Import: The message returned by the taxonomy import in the event of an error or duplication is clearer</li> <li>Task Creation: Newly created tasks are now added to the bottom of the task list to maintain task order consistency.</li> <li>Real-time Updates: The observables and tasks counter now updates in real-time after an item is deleted from the list.</li> <li>Sorting Fix: A sorting issue on the related alerts list has been addressed.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#new-features_2","title":"New Features","text":"<ul> <li>Enhanced Case Merging: When merging cases, identical unmodified tasks are now intelligently merged, streamlining case management.</li> <li>Attachment Previews: Enjoy the convenience of previewing image, text, and PDF attachments for cases and task logs.</li> <li>Task Date Handling: Task end dates now auto-populate when a task is cancelled, with added checks to ensure start-end date consistency.</li> <li>Redesigned Task Log Display: The task log display has undergone a redesign, optimizing readability for improved usability.</li> <li>Streamlined Comment Display: Comments are now displayed in a revamped layout, enhancing readability for easier comprehension.</li> <li>Self-Assignment Capability: It's now possible to assign cases and tasks to yourself</li> <li>Expanded Custom Field Usage: Custom fields can now be completed when closing an alert, offering more complete alert closure information.</li> <li>Markdown Support in Case Reports: Markdown formatting in task log displays within case reports is now correctly interpreted, maintaining rich text formatting.</li> <li>SMTP configuration testing: Easily validate your SMTP configuration directly from the platform, to ensure the smooth operation of e-mail evoies through the application.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#522-9th-august-2023","title":"5.2.2 - 9th August 2023","text":""},{"location":"thehive/release-notes/release-notes-5.2/#fixes_10","title":"Fixes","text":"<p>Infrastructure:</p> <ul> <li>Fix to create attachment directory if it doesn't exist when TheHive starts up</li> <li>A change in authentication configuration is now applied immediately, without the need to restart the platform.</li> <li>The http context is only present once when you configure a SAML authentication server like Okta</li> </ul> <p>API:</p> <ul> <li>Improve performances of notifications making http requests and limit the number of open processes</li> </ul> <p>User Interface:</p> <ul> <li>The name field is indicated as required in the endpoint configuration.</li> <li>Improved loading time for the list of observables</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#new-features_3","title":"New Features","text":"<p>Alerts, Cases and tasks:</p> <ul> <li>Cancelled tasks are now displayed in a case's task list and in the task menu. It will also be possible to see them in progress, and a quick filter on canceled tasks has been added.</li> <li>The severity component of a case and the case number have been split. A new severity component has been created and standardized in the application      </li> <li>Alert comments are visible in case they have been imported</li> <li>Added the ability to copy case number, case title and alert title to the clipboard</li> <li>Add an icon to display alerts, cases and unassigned tasks, and trigger a quick filter on lists</li> <li>Added ability to perform bulk actions on TTPs</li> <li>It is possible to obtain the URL of a case page so that it can be shared</li> </ul> <p>Administration: </p> <ul> <li>Improved case report templates<ul> <li>Added the ability to add a title to a case report widget</li> <li>It is possible to duplicate a case report template</li> <li>It is possible to duplicate a case report template widget</li> </ul> </li> <li>Notifications can now be triggered when an alert closes</li> <li>We have uniformized the labels for PAP, TLP and Severity, in making so, the template helpers severityLabel, tlpLabel and papLabel available in notifications now return the label in upper case</li> <li>The application will notify users by email when their account is modified by them or an admin in the following cases:<ul> <li>Modification of email address</li> <li>Modification of password</li> <li>Password reset</li> </ul> </li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#521-11th-july-2023","title":"5.2.1 - 11th July 2023","text":""},{"location":"thehive/release-notes/release-notes-5.2/#fixes_11","title":"Fixes","text":"<p>UI:</p> <ul> <li>Fix slow autocomplete of custom tags</li> </ul> <p>Docker:</p> <ul> <li>Fixed a behavior where cassandra hostnames were discarded when not resolvable by the entrypoint. This caused the application to use the local file database instead of the provided cassandra hosts when no host could be resolved. This issue appeared in environments like docker swarm.</li> </ul> <p>Warning</p> <p>The docker will no longer try to connect to a cassandra host called <code>cassandra</code> by default. If you use docker-compose with a cassandra database, make sure that you use the option <code>--cql-hostnames</code></p>"},{"location":"thehive/release-notes/release-notes-5.2/#520-6th-july-2023","title":"5.2.0 - 6th July 2023","text":""},{"location":"thehive/release-notes/release-notes-5.2/#breaking-changes","title":"Breaking changes","text":"<p>Warning</p> <ul> <li>The transition to TLP 2.0 involves changing the ID of the TLP:RED value and adding TLP:AMBER+STRICT. The updated assignments are: <ul> <li>TLP:CLEAR = 0</li> <li>TLP:GREEN = 1</li> <li>TLP:AMBER = 2</li> <li>New: TLP:AMBER+STRICT = 3</li> <li>Change: TLP:RED = 4 (previously = 3)</li> </ul> </li> <li>Please make sure to update your dashboard and any integrations that rely on these values.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#main-features","title":"Main features","text":"<ul> <li> <p>What's new in templates</p> <ul> <li> <p>Report template: Boost your reporting with Case Reporting</p> <p>Create customized, high-impact reports with Case Reporting. Use a variety of dynamic widgets such as text, images, tables and lists. Relevant case data (tasks, observables, etc.) are automatically integrated. Export your reports in HTML and Markdown.</p> <p>See dedicated page for more information (requires platinum license)</p> </li> <li> <p>Page template: Customize and organize your cases pages</p> <p>Guide your collaborators in writing the documentation for a case by importing pages directly from the template to provide all the necessary elements and improve processes.</p> <p>See dedicated page for more information (requires platinum license)</p> </li> </ul> <p> </p> </li> <li> <p>What's new in alerts: </p> <ul> <li> <p>Alert assignment</p> <p>Assign alerts to members of the organization. Filter to find alerts assigned to a user.</p> </li> <li> <p>Triggers for alerts in notifications</p> <p>Benefit from new alert triggers to trigger your notifications.</p> </li> </ul> <p> </p> </li> <li> <p>Transition to TLP 2.0</p> <p>Our compatibility with the new TLP 2.0 standard is a key advantage for your business. Use the new TLP 2.0 terminologies to strengthen your cases, dashboards and reports.</p> <p> </p> </li> <li> <p>Notifiers Redis and Microsoft Teams</p> <p>With the new Redis Notifiers and Microsoft Teams, strengthen your communication. Keep your teams informed in real time about the progress of your processes.</p> <p> </p> </li> <li> <p>List Export</p> <p>Export your list information as you wish in JSON or CSV format. Apply filters and/or select items for export to keep only what you need. Exploit exported data according to your specific needs.</p> <p> </p> </li> <li> <p>Two-factor authentication activation indicator</p> <p>Identify users with two-factor authentication enabled. Enhance access security and promote two-factor authentication adoption.</p> </li> <li> <p>Add your own certificate authority on your servers</p> <p>Use your own CA for enhanced server security. Manage your own CA for complete control over certificate issuance, revocation, and management.</p> </li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#other-features","title":"Other features","text":"<ul> <li>PAP:WHITE changes to PAP:CLEAR</li> <li>Add new observable to alert</li> <li>Custom field mandotory indicator added</li> <li>Improve markdown editor and library change</li> <li>Improve validation errors in api</li> <li>Improve performance of NotificationActor</li> <li>Add button to test MISP/Cortex configuration</li> <li>API could understand \"last x days\" filters</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.2/#fixes_12","title":"Fixes","text":"<ul> <li>Fixed some display problems on custom fields</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/","title":"Release Notes for Version 5.3","text":""},{"location":"thehive/release-notes/release-notes-5.3/#release-notes-of-53-series","title":"Release Notes of 5.3 series","text":"<p>Danger</p> <p>The 5.3 release comes with some changes on the database schema that can't be reversed. Please make sure to make a backup of your database before upgrading.</p> <p>This release also comes with some breaking changes, please review them below</p> <p>Warning</p> <p>Note: public API v0 is now considered as deprecated.</p> <p>The public API v0 is obsolete, and you should not be using it anymore. All the endpoints are available in the public API v1. That being said, it will be deactivated in a future version.</p> <p>The API v0 was initially developed for TheHive 3 and maintained for backward compatibility reasons only. </p> <p>API v0 endpoints refer to APIs beginning with <code>/api/</code> or <code>/api/v0/</code> (but not with <code>/api/v1/</code>).</p> <p>Info</p> <p>An upgrade guide is available to help you migrate from TheHive 5.x</p>"},{"location":"thehive/release-notes/release-notes-5.3/#533-9th-july-2024","title":"5.3.3 - 9th July 2024","text":""},{"location":"thehive/release-notes/release-notes-5.3/#improvements","title":"Improvements","text":""},{"location":"thehive/release-notes/release-notes-5.3/#ui-enhancements","title":"UI Enhancements","text":"<ul> <li>List Display Improvements: Various components within list displays have been updated to enhance the overall aesthetic and functional experience. These improvements include modifications to the design of status and other minor visual adjustments aimed at enhancing usability.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#fixes","title":"Fixes","text":""},{"location":"thehive/release-notes/release-notes-5.3/#sorting-on-similar-casesalerts","title":"Sorting on Similar Cases/Alerts","text":"<ul> <li>Sorting Logic Correction: Fixed an issue in the sorting logic of the similarities column to ensure that cases or alerts are first prioritized by the highest number of similar observables, and in cases where two or more entries have the same number, they are then ordered by the highest ratio of common to total observables.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#datatype-filtering-in-similar-casesalerts","title":"DataType Filtering in Similar Cases/Alerts","text":"<ul> <li>Filtering Functionality Restoration: Restored the ability to define and apply specific data types in the matches column that filter similar alerts and cases. This update ensures that only observables of the selected types are considered when displaying similar cases and alerts, thereby improving the relevance and accuracy of similarity matches.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#user-count-in-administration-section","title":"User Count in Administration Section","text":"<ul> <li>The user count is now immediately updated when an account is removed or locked.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#api-case-list","title":"API Case List","text":"<ul> <li>Fixed a bug in the <code>query</code> API route that caused cases to be unobtainable when using the <code>caseid</code> filter.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#custom-tags","title":"Custom Tags","text":"<ul> <li>Corrected an issue that generated an error when removing and re-adding a custom tag.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#dropdown-selectors","title":"Dropdown Selectors","text":"<ul> <li>Fixed a bug in dropdown menus that prevented the menu action from launching if the click was not on the text item.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#532-14th-june-2024","title":"5.3.2 - 14th June 2024","text":""},{"location":"thehive/release-notes/release-notes-5.3/#improvements_1","title":"Improvements","text":""},{"location":"thehive/release-notes/release-notes-5.3/#similar-casesalerts","title":"Similar Cases/Alerts","text":"<ul> <li>Improved the queries to get and display the similar Cases &amp; similar Alerts lists. Only useful information is now loaded, significantly speeding up the list display. A new query is performed when accessing the observable list drawer of a similar Case/Alert.</li> </ul> <p><code>To ensure performance, the observable list preview drawer can display up to 100 observables only.</code></p>"},{"location":"thehive/release-notes/release-notes-5.3/#fixes_1","title":"Fixes","text":""},{"location":"thehive/release-notes/release-notes-5.3/#directquery","title":"DirectQuery","text":"<ul> <li>Fixed an issue that could cause platform instability due to the handling of empty array requests when DirectQuery is activated.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#arrow-end-home-keys","title":"Arrow / end / home keys","text":"<ul> <li>Resolved a regression that prevented the use of arrow, end, and home keys in the edit mode of various components: Tasklog, Dashboard, Endpoints, Custom fields, Report template, and Case template.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#notifications","title":"Notifications","text":"<ul> <li>Corrected an issue in the httpRequest Notifier where the JSON mode was not properly applied when activated.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#misp-connector","title":"MISP Connector","text":"<ul> <li>Fixed an issue that prevented the MISP connector from capturing all events during synchronization.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#case-creation","title":"Case Creation","text":"<ul> <li> <p>Corrected a problem that prevented the selection of a custom Start Date when creating a case from an alert. Users can now select a custom Start Date for their cases.</p> </li> <li> <p>When multiple alerts are selected, the action button associated with a given alert now provides clearer information about the possible actions, like the creation of a case.</p> </li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#tasks","title":"Tasks","text":"<ul> <li>Locked users no longer appear in the assignable user list.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#531-16th-may-2024","title":"5.3.1 - 16th May 2024","text":""},{"location":"thehive/release-notes/release-notes-5.3/#improvements_2","title":"Improvements","text":""},{"location":"thehive/release-notes/release-notes-5.3/#email-intake","title":"Email Intake","text":"<ul> <li>Added the possibility to specify the Microsoft Office365/Google Workspace host instead of the one provided by default.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#fixes_2","title":"Fixes","text":""},{"location":"thehive/release-notes/release-notes-5.3/#custom-fields","title":"Custom Fields","text":"<ul> <li>Fixed a bug where merging of alerts and cases could generate duplicated customfields values in the index (invisible in the UI).</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#misp-connector_1","title":"MISP Connector","text":"<ul> <li>Resolved a problem that prevented alert deletion.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#alerts-cases","title":"Alerts &amp; Cases","text":"<ul> <li>Fixed a bug related to assignable users.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#similar-alerts","title":"Similar Alerts","text":"<ul> <li>Fixed the counter in the pagination.</li> <li>Fixed the display of the pending status.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#dashboard","title":"Dashboard","text":"<ul> <li>Fixed an issue with the properties of the donut widget.</li> <li>Changed the list to display more than 30 dashboards.</li> <li>Improved the behavior of the diagram widget.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#notifications_1","title":"Notifications","text":"<ul> <li>Fixed a problem with the recipent field of the email notifier.</li> <li>Solved an issue with the <code>{{ url }}</code> variable when it concerns a task.</li> <li>Fixed an issue that made it impossible to delete a webhook endpoint.</li> <li>An Event is now triggered when a Case is created from an Alert.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#responders","title":"Responders","text":"<ul> <li>Resolved a problem with the display of a responder report in the task preview. </li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#api","title":"API","text":"<ul> <li>Fixed an API error return code in the post <code>/case</code> route when the <code>status</code> value is <code>unknown</code>.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#ui","title":"UI","text":"<ul> <li>Renamed a field name in the SAML authentication configuration page for better understanding.</li> <li>Reviewed the breadcrumb to better manage long alert names.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#security-fixes","title":"Security Fixes","text":"<ul> <li>Embedded patches for the following vulnerability: CVE-2024-25710</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#530-24th-april-2024","title":"5.3.0 - 24th April 2024","text":"<p>Info</p> <p>The licensing model for the community version has been updated. Users are now required to register on our licensing portal and request a community license to use TheHive in the community version. Additionally, TheHive will now include a default 14-day free Platinum trial license, allowing users to explore the full range of features offered by the platform.</p>"},{"location":"thehive/release-notes/release-notes-5.3/#new-features","title":"New features","text":""},{"location":"thehive/release-notes/release-notes-5.3/#email-intake_1","title":"Email intake","text":"<p>The Email Intake connector now fully automates the transformation of incoming emails into actionable alerts on TheHive platform. It supports Microsoft 365, Google Workspace, and IMAP-based email services. </p> <p>It automates the detection and processing of suspicious elements such as links, attachments and sender details, which it all adds to the list of observables.</p>"},{"location":"thehive/release-notes/release-notes-5.3/#new-timeline-widget","title":"New timeline widget","text":"<p>We've added a timeline widget to TheHive's Platinum case reports, allowing users to visually track attack and defense actions. This widget displays key events and indicators like IOCs and TTPs.</p> <p>Only admins can customize these reports, choosing elements like alerts and tasks to include. This customization ensures the timeline meets the specific needs of each report, making it easier for everyone, including non-technical staff, to understand the sequence of security events.</p>"},{"location":"thehive/release-notes/release-notes-5.3/#data-list-export","title":"Data List Export","text":"<p>We've improved the data export options across TheHive. Users can now select specific fields from application lists to export, making the data more relevant and manageable for analysis.</p>"},{"location":"thehive/release-notes/release-notes-5.3/#opensearch-support","title":"OpenSearch Support","text":"<p>OpenSearch is now available as an indexing option alongside Elasticsearch. This addition offers more choices for your indexing needs.</p>"},{"location":"thehive/release-notes/release-notes-5.3/#dynamic-date-filtering","title":"Dynamic Date Filtering","text":"<p>A new relative date filter on dashboards and search pages allows users to filter data based on specific time frames like the last few days or months.</p>"},{"location":"thehive/release-notes/release-notes-5.3/#similar-case-and-alert-enhancements","title":"Similar Case and Alert Enhancements","text":"<p>We've updated the similar cases and alerts pages to display more accurate data, adding a drawer for observables common to related cases and alerts and including additional details like case status for clearer insights.</p>"},{"location":"thehive/release-notes/release-notes-5.3/#observable-export-improvements","title":"Observable Export Improvements","text":"<p>The observable export feature now supports more formats, including JSON and customizable CSV, allowing users to select specific fields for export.</p>"},{"location":"thehive/release-notes/release-notes-5.3/#elasticsearch-performance-and-interface-improvements","title":"Elasticsearch performance and interface improvements","text":"<ul> <li>Queries have been optimized for better dashboard and search performance.</li> <li>Full support for Elasticsearch 8.</li> <li>Improved handling of custom fields with new operators (isEmpty, nonEmpty, between).</li> <li>Users can now customize the number of segments in dashboard donuts.</li> <li>All data points are now included in category aggregations for donuts and charts, providing a complete view\u2014in the past, it was capped at 100 displayed values.</li> </ul>"},{"location":"thehive/release-notes/release-notes-5.3/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Fixed the display issue with the number of open cases in quick filters.</li> <li>Corrected a merging bug for custom tags to prevent duplicates.</li> <li>Fixed a bug in markdown editor related to the <code>&lt;</code> &amp; <code>&gt;</code> characters.</li> </ul> <p>Info</p> <p>As we have updated some front-end components, please remember to refresh your browser page after upgrading to version 5.3 to prevent any UI issues.</p>"},{"location":"thehive/user-guides/forgot-password/","title":"Forgot Password Guide","text":""},{"location":"thehive/user-guides/forgot-password/#i-forgot-my-password","title":"I forgot my password","text":"<p>If TheHive is connected to a SMTP server, as a user, you can change your password.</p> <ol> <li> <p>On the login page of TheHive, click the I forgot my password button</p> <p> </p> </li> <li> <p>Enter your email address and click send</p> <p> </p> </li> <li> <p>You should received an email including a magic link to change your password</p> <p> </p> </li> <li> <p>Click on the link and change your password. You should be able to login again once updated.</p> </li> </ol>"},{"location":"thehive/user-guides/indicators/","title":"Indicators & KPIs Overview","text":""},{"location":"thehive/user-guides/indicators/#indicators","title":"Indicators","text":"<p>Since version 5.1, TheHive provides valuable insights into the time metrics of events and incidents, allowing you to track key performance indicators related to your security operations. The support for mean time metrics has been added in dashboards, to make it easier for you to manage your operations and helps to identify areas that may require more attention or effort.</p>"},{"location":"thehive/user-guides/indicators/#list-of-indicators-in-case-and-alert","title":"List of indicators in Case and Alert","text":"<p>Each Case and Alert is defined by several time &amp; date information. From this data, TheHive computes useful indicators displayed in the detailed view of each Case and Alert.</p> <p>With the new time metrics indicators, you can now have visibility into the detection, acknowledgement, and triage times for each case and alert, providing a clearer understanding of your incident response processes.</p> <p></p>"},{"location":"thehive/user-guides/indicators/#time-to-detect-ttd","title":"Time to Detect (TTD)","text":"<p>From an alert: <code>alert.newDate - alert.date</code></p> <p>From a case: <code>case.newDate - case.startDate</code></p> <p>This is the amount of time between the date of the event (<code>alert.date</code>) and the alert creation date (<code>alert.newDate</code>).</p> <p>TTD is the amount of time it takes your security team and technologies to notice abnormal behavior that indicates potentially malicious, suspicious, or risky behavior in your ecosystem. The lower the TTD, the better your program is. As part of measuring this, you want to make sure that you continuously fine-tune the systems that alert your security staff because too many false positives can increase the time it takes to detect a true risk.</p> <p>(source: https://securityscorecard.com/blog/kpis-for-security-operations-incident-response/)</p>"},{"location":"thehive/user-guides/indicators/#time-to-triage-ttt","title":"Time to Triage (TTT)","text":"<p>From an alert: <code>alert.inProgressDate - alert.newDate</code></p> <p>From a case: <code>case.inProgressDate - case.newDate</code></p> <p>This is the amount of time between the alert creation date (<code>alert.newDate</code>) and the date on which the alert becomes in progress (<code>alert.inProgressDate</code>).</p> <p>According to one Palo Alto research report, the average security operations team received over 11,000 alerts per day. However, not all risks and alerts are created equally. Your team needs to be able to rapidly prioritize the alerts that indicate the highest risk to your organization\u2019s data. During this stage, the security team is looking for high, medium, and low-risk alerts. The faster they can triage and prioritize the alerts, the sooner the incident response team can acknowledge the alert. All of this ultimately leads to a faster process, reduced risk, and more resilient program.</p> <p>(source: https://securityscorecard.com/blog/kpis-for-security-operations-incident-response/)</p>"},{"location":"thehive/user-guides/indicators/#time-to-qualify-ttq-equivalent-to-assessmentduration","title":"Time to Qualify (TTQ) - Equivalent to assessmentDuration","text":"<p>From an alert: <code>max(alert.importedDate, alert.closedDate) - alert.newDate</code></p> <p>This is the amount of time between the alert creation date (<code>alert.newDate</code>) and the date on which the alert has been closed or imported (<code>alert.importedDate</code> or <code>alert.closedDate</code>).</p> <p>The triage process naturally leads to the qualification process. In some cases, your security operations team may determine that an alert qualifies to be moved to the incident response team. In some cases, this KPI overlaps with the meantime to acknowledge (MTTA) because the incident response team can\u2019t start acknowledging and moving the research forward until the security operations team qualifies it. </p> <p>(source: https://securityscorecard.com/blog/kpis-for-security-operations-incident-response/)</p>"},{"location":"thehive/user-guides/indicators/#time-to-acknowledge-tta-equivalent-to-sla","title":"Time to Acknowledge (TTA) - Equivalent to SLA","text":"<p>From an alert: <code>alert.inProgressDate - alert.date</code></p> <p>From a case: <code>case.inProgressDate - case.startDate</code></p> <p>This is the amount of time between the date of the event (<code>alert.date</code>) and the date on which the alert becomes in progress (<code>alert.inProgressDate</code>).</p> <p>Related to TTD, this metric tells you the amount of time it takes your security operations and incident response team to acknowledge an alert before they begin doing an investigation. This is the reason you want the MTTD to be lower. The better the alerts your team gets, the sooner they can acknowledge real threats. You want to keep the MTTA as low as possible as part of creating KPIs. </p> <p>(source: https://securityscorecard.com/blog/kpis-for-security-operations-incident-response/)</p>"},{"location":"thehive/user-guides/indicators/#time-to-resolve-ttr","title":"Time to Resolve (TTR)","text":"<p>From a case or an alert: <code>case.endDate - min(alert.inProgress, case.inProgress)</code></p> <p>This is the amount of time between the team start to work on incident (<code>alert.inProgressDate</code> or <code>case.inProgressDate</code>) and the end of the incident (<code>case.endDate</code>).</p> <p>Once your team acknowledges the threat, they need to resolve it. MTTI focuses on the amount of time it takes the incident response team to get from the investigation to the recovery step. This is another KPI that you want to have a low number. The resolution may be that the alert was a false positive or it may be that the team had to eradicate a threat and recover a system. In either case, the sooner that the team can complete all the necessary resolution steps, the stronger your cybersecurity posture is.</p> <p>(source: https://securityscorecard.com/blog/kpis-for-security-operations-incident-response/)</p>"},{"location":"thehive/user-guides/indicators/#use-kpis-in-dashboards","title":"Use KPIs in Dashboards","text":"<p>Using Mean Time To metrics in dashboards can building dashboards with useful KPIs to understand your efficiency and identify areas that may require more attention or effort.</p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/Filter-and-sort/","title":"Filters","text":""},{"location":"thehive/user-guides/analyst-corner/Filter-and-sort/#filters","title":"Filters","text":"<p>In this section, you can find information about applying filters.</p> <p>To apply filter:</p> <ol> <li>On the list of incidents page, switch on the Filters toggle button.</li> <li>Click Add filters.</li> </ol> <p>Apply Filter to the required field.</p> <p></p> <ol> <li>Select the filters from the list.</li> <li>Click Apply filters.</li> <li>(Optional) Click Clear filters to clear all applied filters.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/Filter-and-sort/#sorting","title":"Sorting","text":"<p>Sorting can be performed on any field values.  </p> <p>To Sort:</p> <ol> <li>On the list of incidents page, Click the small arrow that points upwards/downwards to sort on a particular filed name. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/authentication/","title":"Authentication","text":""},{"location":"thehive/user-guides/analyst-corner/contacting-support/","title":"Contact Support","text":""},{"location":"thehive/user-guides/analyst-corner/getting-started/","title":"Getting Started","text":""},{"location":"thehive/user-guides/analyst-corner/getting-started/#getting-started","title":"Getting Started","text":"<p>TheHive supports different roles for users. Whether you are an administrator of the platform, an organization, or an analyst, you can have access and run different actions on the platform.</p> <p>This user guide aims at describing all major howtos when you sign in as a Org-Admin.</p>"},{"location":"thehive/user-guides/analyst-corner/introduction/","title":"Introduction","text":""},{"location":"thehive/user-guides/analyst-corner/list-of-incidents/","title":"List of Incidents","text":""},{"location":"thehive/user-guides/analyst-corner/list-of-incidents/#list-of-incidents","title":"List of Incidents","text":"<p>After you login to TheHive application, you can see a list of incidents displayed on the screen by default. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/list-of-incidents/#main-menu","title":"Main Menu","text":"<p>In the left pane, you can see the main Menu. </p> <ul> <li>Case List</li> <li>Alerts</li> <li>Tasks</li> <li>Dashboards</li> <li>Search </li> <li>Organization</li> </ul> <p>Clicking on the arrow will switch you back to the image view. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/list-of-incidents/#top-menu","title":"Top Menu","text":"<p>On the top of the page are the following options: </p> <ul> <li>Defaults</li> <li>Quick Filters</li> <li>Auto Refresh</li> <li>Stats</li> <li>Filter</li> </ul>"},{"location":"thehive/user-guides/analyst-corner/list-of-incidents/#defaults","title":"Defaults","text":"<p>It displays the default view. </p> <ol> <li> <p>Click on Default. A list is displayed. </p> <p></p> <p>To Save a view: </p> </li> <li> <p>Click Save view As.</p> <p>A new window opens. </p> </li> <li> <p>Click Confirm. </p> <p></p> </li> </ol> <p>To Manage views: </p> <ol> <li>Click the default button. </li> <li> <p>Click Manage Views from the list. </p> <p></p> </li> </ol> <p>A new page opens. It has the Name of the view and the corresponding Actions. </p> <ol> <li>Click the ellipsis (...) corresponding to the name of the view.</li> <li>Select the action. e.g. Delete</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/list-of-incidents/#quick-filters","title":"Quick Filters","text":"<p>To apply Quick filter:</p> <ol> <li>Click the quick filter option. </li> </ol> <p>The list displays options to select. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/list-of-incidents/#auto-refresh","title":"Auto refresh","text":"<p>The auto-refresh option allows you to automatically refresh a page. </p> <p>To perform Auto refresh:</p> <ol> <li>On the list of incidents page, switch on the Auto refresh toggle button. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/list-of-incidents/#stats","title":"Stats","text":"<p>To view Stats: </p> <ol> <li>On the cases list page, switch on the Stats toggle button.</li> </ol> <p>The stats are displayed. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/list-of-incidents/#filters","title":"Filters","text":"<p>To apply filter:</p> <ol> <li> <p>On the cases list page, switch on the Filters button.</p> </li> <li> <p>Click Add filters.</p> <p>Apply Filter to the required field.</p> <p></p> </li> <li> <p>Select the filters from the list.</p> </li> <li>Click Apply filters.</li> <li>(Optional) Click Clear filters to clear all applied filters.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/organizations/","title":"About Organization","text":""},{"location":"thehive/user-guides/analyst-corner/organizations/#about-organization","title":"About Organization","text":"<p>Organizations are the customers or tenants who are using the TheHive application.</p> <p>TheHive comes with a default organisation named \"admin\" and is dedicated to users with administrator permissions of TheHive instance. This organisation is very specific so that it can manage global objects and cannot contain cases or any other related elements.</p> <p>By default, organisations can\u2019t see each other and can't share data. </p> <p>On the home page, you can see the organization List.</p>"},{"location":"thehive/user-guides/analyst-corner/sign-in-as-an-admin/","title":"Sign In as Org-Admin","text":""},{"location":"thehive/user-guides/analyst-corner/sign-in-as-an-admin/#sign-in-as-org-admin","title":"Sign In as Org-Admin","text":"<ol> <li>Open the TheHive application in your browser. </li> <li>Enter user credentials, Login name and password.</li> <li>Click the Let me In button. </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/alerts/about-alerts/","title":"About Alerts","text":""},{"location":"thehive/user-guides/analyst-corner/alerts/about-alerts/#alerts","title":"Alerts","text":"<p>In this section you can find information about Alerts. </p> <p>Alerts provide timely information about current security issues, vulnerabilities, and exploits.</p>"},{"location":"thehive/user-guides/analyst-corner/alerts/about-alerts/#view-alert-details","title":"View alert details","text":"<p>To view alert details: </p> <p>You can click on any of the alerts in the list to view more details. </p> <p>The Alerts page displays various tabs that have more details about alerts such general tab, observables, TTPs, similar cases, similar alerts, responders tab. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/actions/","title":"Actions on Alerts","text":""},{"location":"thehive/user-guides/analyst-corner/alerts/actions/#actions","title":"Actions","text":"<p>You can make use of any of the available actions.</p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/actions/#start","title":"Start","text":"<ol> <li>Click the Start option to begin an alert.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/alerts/actions/#close","title":"Close","text":"<ol> <li>Click the Close option to remove a task.</li> </ol> <p>A new window opens.</p> <ol> <li>Select Status from the list. </li> <li>Change the Summary</li> <li>Click the Close tasks and case button.</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/actions/#trackignore-new-updates","title":"Track/Ignore New Updates","text":"<ol> <li>Click the Track New Updates option to track an alert.</li> <li>A success message is displayed. </li> </ol> <ol> <li>Click the Ignore New Updates option to ignore an alert.</li> <li>A success message is displayed. </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/alerts/actions/#unlink","title":"Unlink","text":"<ol> <li>Click the Unlink option to unlink an alert.</li> <li>Click the OK button. </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/alerts/general/","title":"View Alert Details","text":""},{"location":"thehive/user-guides/analyst-corner/alerts/general/#general","title":"General","text":"<p>The information on the general page comes from templates and is auto-populated.  In the left pane of the window, you can see details such as created by, created date, TLP, PAP, severity details, status of the alert, start date, and task completion details. </p> <p>In the left pane of the window you can configure the PAP,TLP and Severity.  Refer to <code>Configure Alert Details</code> for more details. </p> <ol> <li>In the right pane of the window, enter Comments if any. </li> <li>Click the Comment button. </li> <li>Add Tags. (Refer to <code>Add tags</code>).</li> <li>Enter the Description. </li> <li>Add Custom fields. (Refer to <code>Add custom fields</code>)</li> <li> <p>Click Add to enter the respective business unit and location details. </p> <p></p> </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/manage-views/","title":"Manage Views","text":""},{"location":"thehive/user-guides/analyst-corner/alerts/manage-views/#manage-views","title":"Manage views","text":"<p>In this section, you can find information about managing views.  </p> <p>To manage views: </p> <ol> <li>Click the default button. </li> <li>Click on Manage Views from the list. </li> </ol> <p></p> <p>A new page opens. It has the Name of the view and the corresponding Actions. </p> <ol> <li>Click the ellipsis (...) corresponding to the name of the view that you want to delete. </li> <li>Click Delete.</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/manage-views/#manage-alerts","title":"Manage Alerts","text":"<p>There are various option available for to apply on the alerts. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/manage-views/#quick-filters","title":"Quick Filters","text":"<p>To apply Quick filter:</p> <ol> <li>Click the Quick Filter option. </li> <li>The list displays options to select from. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/manage-views/#auto-refresh","title":"Auto refresh","text":"<p>The auto-refresh option allows you to automatically refresh a page. </p> <p>To perform Auto refresh:</p> <ol> <li>On the alerts page, switch on the Auto refresh button. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/manage-views/#stats","title":"Stats","text":"<p>To view stats: </p> <ol> <li>On the alerts page, switch on the Stats toggle button, the stats will be displayed. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/manage-views/#filters","title":"Filters","text":"<p>To apply filter:</p> <ol> <li>On the alerts page, switch on the Filters toggle button.</li> <li>Click Add filters.</li> </ol> <p>Apply Filter to the required field.</p> <p></p> <ol> <li>Select the filters from the list.</li> <li>Click Apply filters.</li> <li>(Optional) Click Clear filters to clear all applied filters.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/alerts/manage-views/#sorting","title":"Sorting","text":"<p>Sorting can be performed on any field values.  </p> <p>To Sort:</p> <ol> <li>On the alerts page, Click the small arrow that points upwards/downwards to sort on a particular filed name. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/merge-alerts/","title":"Merge Alerts","text":""},{"location":"thehive/user-guides/analyst-corner/alerts/merge-alerts/#merge-alerts","title":"Merge alerts","text":"<p>In this section you can find information about merging alerts. </p> <p>On the main page where all the alerts are listed, there are various alerts. Some are new, some are imported. Merge alerts / merge selection into case option is available only for New alerts from the list. </p> <p>To merge alerts:</p> <ol> <li>Go to alert details page.</li> <li>Select the alert to merge data into</li> <li>Click merge alerts.</li> </ol> <p></p> <p>NOTE: Merging two cases, removes the originating cases, and creates a new one with all the merged data.</p>"},{"location":"thehive/user-guides/analyst-corner/alerts/new-case-from-selection/","title":"Create a Case from Alerts","text":""},{"location":"thehive/user-guides/analyst-corner/alerts/new-case-from-selection/#new-case-from-selection","title":"New case from selection","text":"<p>In this section you can find information about creating a new case from selection. </p> <p>On the main page where all the alerts are listed, there are various alerts. Some are new, some are imported. New case from selection option is available only for new alerts from the list. </p> <p></p> <p>To add a new case from selection:</p> <ol> <li>Go to alert details page.</li> <li>Select the alert for which you want to add new case.</li> <li>Click New Case from selection option.</li> </ol> <p>A new window opens.</p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/preview-alerts/","title":"Preview Alerts","text":""},{"location":"thehive/user-guides/analyst-corner/alerts/preview-alerts/#preview-alerts","title":"Preview alerts","text":"<p>In this section you can find information about  previewing alerts and associated details. </p> <p>To preview the alert details:</p> <p>On the list of alerts page, there is a Preview button corresponding to the specific alert name.</p> <ol> <li>Click the Preview option. </li> </ol> <p></p> <p>The alert details preview window opens.</p> <p></p> <p>You can see details like the id, created by, created at date, last reviewed by, last reviewed date, import date, TLP, PAP and severity details, title, tags, description, status and summary of the alert. </p> <p>Add Custom fields (Refer to <code>Add custom fields</code>), business unit, and location  details.</p> <ol> <li> <p>Click Add to enter business unit and location details. </p> <p></p> </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/alerts/preview-alerts/#actions","title":"Actions","text":"<p>You can click the <code>Actions Button</code>  to start, close, track/ignore new updates, unlink the alerts or to <code>Run Responders</code>. </p>"},{"location":"thehive/user-guides/analyst-corner/alerts/preview-alerts/#alert-details","title":"Alert Details","text":"<ol> <li>Click the Go to details button to view more details of the alert. </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/alerts/preview-alerts/#alert-details-menu","title":"Alert details menu","text":"<ol> <li>Click the Go to details button to view more details of the alert. </li> </ol> <p>On the top of the page, there are many task options available such as start, close, track/ignore new updates, unlink the alerts and run responders. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/run-responders/","title":"Run Responders on Alert","text":""},{"location":"thehive/user-guides/analyst-corner/alerts/run-responders/#run-responders","title":"Run responders","text":""},{"location":"thehive/user-guides/analyst-corner/alerts/run-responders/#responders","title":"Responders","text":"<ol> <li>Click on responders option.</li> </ol> <p>A new window appears. </p> <ol> <li>Search for a specific responder in the search box.</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/view-observables/","title":"View Observables","text":""},{"location":"thehive/user-guides/analyst-corner/alerts/view-observables/#view-observables","title":"View observables","text":"<p>In this section you can find information about viewing observables. </p> <p>When you install the TheHive application, it comes with a set of pre-defined observables such as IP and email addresses, URLs, domain names, files or hashes.</p> <p>You can define your own observable type. You can see the list of all Observable Types.</p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/view-responders/","title":"View Responders","text":""},{"location":"thehive/user-guides/analyst-corner/alerts/view-responders/#view-responders","title":"View responders","text":"<p>Responder is a tool that can be used in security penetration tests on the infrastructure of the networks. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/view-similar-alerts/","title":"View Similar Alerts","text":""},{"location":"thehive/user-guides/analyst-corner/alerts/view-similar-alerts/#view-similar-alerts","title":"View similar alerts","text":"<p>In this section, you can find information about all the similar alerts listed down.</p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/view-similar-cases/","title":"View Similar Cases","text":""},{"location":"thehive/user-guides/analyst-corner/alerts/view-similar-cases/#view-similar-cases","title":"View similar cases","text":"<p>In this section, you can find information about all the similar cases listed down.</p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/alerts/view-ttps/","title":"View TTPs","text":""},{"location":"thehive/user-guides/analyst-corner/alerts/view-ttps/#view-ttps","title":"View TTPS","text":"<p>In this section, you can find information about all the TTPS listed down.</p> <p>Tactics, techniques, and procedures (TTPs) are the patterns of activities or methods associated with a specific threat actor or group of threat actors.</p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases/about-a-case/","title":"About Cases","text":""},{"location":"thehive/user-guides/analyst-corner/cases/about-a-case/#about-a-case","title":"About a case","text":"<p>In this section you can find information about cases. </p> <p>A case provides information on suspicious activity in the environment. It provides information on the security incidents, observables, alerts, and affected users.  Security analysts can conduct specific analysis based on cases to assess the possibilities of threats. </p> <p>Cases can be created from various sources. Each security case consists of a title, tags, task rules, obsevable rules a description of case details, and all the details related to the case that help in building an argument for identifying and dealing with particular threats.</p>"},{"location":"thehive/user-guides/analyst-corner/cases/adding_to_a_case/","title":"Adding to a Case (Tags/Tasks/Custom Field Values)","text":""},{"location":"thehive/user-guides/analyst-corner/cases/adding_to_a_case/#adding-to-a-case-tagstaskscustom-field-values","title":"Adding to a case (Tags/Tasks/Custom field values)","text":""},{"location":"thehive/user-guides/analyst-corner/cases/adding_to_a_case/#add-tags","title":"Add tags","text":"<ol> <li>Choose tags from the Taxonomy. The selected tag will appear in the Selected Tags box.</li> <li>Click the Add selected tags button.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases/adding_to_a_case/#add-tasks","title":"Add tasks","text":"<p>The task Group is default. </p> <ol> <li>Enter the task Title.</li> <li>Enter the task description in the Description. </li> <li>Switch the toggle button to Flag this task?. </li> <li>Select the Due date. </li> <li>Click Save and add another, to add another task. </li> <li>Click Confirm.</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases/adding_to_a_case/#add-custom-field-values","title":"Add custom field values","text":"<ol> <li>Select custom field value from the given list. (location/business-unit/detection-source/test).</li> <li>Click Confirm custom field value creation.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases/create-a-new-case/","title":"Create a New Case","text":""},{"location":"thehive/user-guides/analyst-corner/cases/create-a-new-case/#create-new-cases-using-templates","title":"Create new cases using templates","text":"<p>A User can create new cases using templates.</p> <ol> <li>Click Create Case + on the header.</li> </ol> <p></p> <p>A new screen opens. A user can create cases by selecting any one of the following options: </p> <p>Click the below links to create each type of new case. </p> <ol> <li><code>Empty Case</code></li> <li><code>EDR / Phishing Template</code></li> <li><code>Archive</code></li> <li><code>MISP</code></li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-archive/","title":"Create a Case from Archive","text":""},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-archive/#from-archive-thar","title":"From archive (.thar)","text":"<p>Create a new case from archive.</p> <ol> <li>Upload File as Attachment.</li> <li>Enter the Password. </li> <li> <p>Click the Confirm case creation button. </p> <p></p> </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-misp/","title":"Create a Case from MISP","text":""},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-misp/#from-mispjson","title":"From MISP(.json)","text":"<p>Create a new case from MISP.</p> <ol> <li>Upload File as Attachment.(import from MISP).</li> <li>Add Tasks. (Refer to <code>Add tasks</code>).</li> <li> <p>Add Custom Fields. (Refer to <code>Add custom field values</code>).</p> </li> <li> <p>Click the Confirm case creation button. </p> <p></p> </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-template/","title":"Create a Case from Template","text":""},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-template/#from-template","title":"From template","text":"<p>Create a new case from templates</p>"},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-template/#1-create-a-new-case-from-edr-template","title":"1. Create a new case from EDR template.","text":"<ol> <li>Enter the case title in the Title.</li> <li>Select the date from the Date. </li> <li>Select Severity, (Low/Medium/High/Critical).</li> <li>Select TLP, (White/Green/Amber/Red).</li> <li>Select PAP, (White/Green/Amber/Red).</li> <li>Click + to add Tags. (Refer to <code>Add tags</code>.)</li> <li>Enter the case description in the Description. </li> <li>Choose a Task rule from the list, (manual/existingOnly/upcommingOnly/all).</li> <li>Choose an Observable rule from the list, (manual/existingOnly/upcommingOnly/all).</li> <li>Add Tasks. (Refer to <code>Add tasks</code>. / <code>Edit tasks</code>. /<code>Delete tasks</code>.) </li> <li>Add Custom Fields. (Refer to <code>Add custom field values</code>. /<code>Edit custom field values</code>. /<code>Delete custom field values</code>.)</li> <li>Add Pages. (Refer to <code>Add pages</code>. /<code>Delete pages</code>.)</li> <li>Sharing (Refer to <code>Sharing</code>.)</li> <li>Click the Confirm case creation button. </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-template/#2-create-a-new-case-from-phishing-template","title":"2. Create a new case from Phishing template.","text":"<ol> <li>Enter the case title in the Title.</li> <li>Select the date from the Date. </li> <li>Select Severity, (Low/Medium/High/Critical).</li> <li>Select TLP, (White/Green/Amber/Red).</li> <li>Select PAP, (White/Green/Amber/Red).</li> <li>Click + to add Tags. (Refer to <code>Add tags</code>.)</li> <li>Enter the case description in the Description. </li> <li>Choose a Task rule from the list, (manual/existingOnly/upcommingOnly/all).</li> <li>Choose an Observable rule from the list, (manual/existingOnly/upcommingOnly/all).</li> <li>Add Tasks. (Refer to <code>Add tasks</code>. /<code>Edit tasks</code>. /<code>Delete tasks</code>.) </li> <li>Add Custom Fields. (Refer to <code>Add custom field values</code>. /<code>Edit custom field values</code>. /<code>Delete custom field values</code>.)</li> <li>Add Pages. (Refer to <code>Add pages</code>. /<code>Edit pages</code>. /<code>Delete pages</code>.)</li> <li>Sharing (Refer to <code>Sharing</code>.) </li> <li>Click the Confirm case creation button. </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-template/#add-tags","title":"Add tags","text":"<ol> <li>Choose tags from the Taxonomy. The selected tag will appear in the Selected Tags box</li> <li>Click the Add selected tags button.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-template/#add-tasks","title":"Add tasks","text":"<p>The task Group is default. </p> <ol> <li>Enter the task Title.</li> <li>Enter the task description in the Description. </li> <li>Select the Due date. </li> <li>Click Confirm.</li> <li>Click Save and add another, to add another task. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-template/#add-custom-field-values","title":"Add custom field values","text":"<ol> <li>Select custom field value from the given list. (location/business-unit/detection-source/test).</li> <li>Click Confirm custom field value creation.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-template/#add-pages","title":"Add pages","text":"<p>By selecting Create new page</p> <ol> <li>Enter the page Title.</li> <li>Enter or select the Category. </li> <li>Enter the page content in the content. </li> <li>Click Confirm.</li> <li>Click Save and add another, to add another task. </li> </ol> <p></p> <p>By selecting Use an existing page template</p> <ol> <li>Choose template(s) from those available in the list of existing templates</li> <li>Click Confirm.</li> <li>Click Save and add another, to add another task. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-template/#edit-tasks","title":"Edit tasks","text":"<ol> <li>Click the edit link.</li> </ol> <p>A new window opens. </p> <ol> <li>Edit the required values </li> <li>Click the Confirm edition button.</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-template/#edit-custom-field-values","title":"Edit custom field values","text":"<ol> <li>Click the edit link.</li> </ol> <p>A new window opens.</p> <ol> <li>Edit the required custom field values </li> <li>Click the Confirm custom field value edition button.</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-template/#delete-tasks","title":"Delete tasks","text":"<ol> <li>Click the delete link beside the value that has to be deleted. </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-template/#delete-custom-field-values","title":"Delete custom field values","text":"<ol> <li>Click the delete link beside the custom field value that has to be deleted. </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases/create-case-from-template/#delete-pages","title":"Delete pages","text":"<ol> <li>Click the delete link beside the value that has to be deleted. </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases/create-empty-case/","title":"Create an Empty Case","text":""},{"location":"thehive/user-guides/analyst-corner/cases/create-empty-case/#from-an-empty-case","title":"From an empty case","text":"<p>Create a new case from an empty case. </p> <ol> <li>Enter the case title in the Title.</li> <li>Select the date from the Date. </li> <li>Select Severity, (Low/Medium/High/Critical).</li> <li>Select TLP, (White/Green/Amber/Red).</li> <li>Select PAP, (White/Green/Amber/Red).</li> <li>Click + to add Tags. (Refer to <code>Add tags</code>).</li> <li>Enter the case description in the Description. </li> <li>Choose a Task rule from the list, (manual/existingOnly/upcommingOnly/all).</li> <li>Choose an Observable rule from the list, (manual/existingOnly/upcommingOnly/all).</li> <li>Add Tasks. (Refer to <code>Add tasks</code>).</li> <li>Add Custom Fields. (Refer to <code>Add custom field values</code>).</li> <li>Click the Confirm case creation button. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/about-a-case-list/","title":"Cases","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/about-a-case-list/#cases","title":"Cases","text":"<p>In this section you can find information about cases. </p> <p>A case provides information on suspicious activity in the environment. It provides information on the security incidents, observables, alerts, and affected users.  Security analysts can conduct specific analysis based on cases to assess the possibilities of threats. </p> <p>Cases can be created from various sources. Each security case consists of a title, tags, task rules, obsevable rules a description of case details, and all the details related to the case that help in building an argument for identifying and dealing with particular threats.</p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/about-a-case-list/#view-case-details","title":"View case details","text":"<p>To view case details: </p> <ol> <li> <p>Click on any of the cases displayed in the list to view more details. </p> <p>The case list page displays various tabs that have more details about each case such general tab, tasks, observables, TTPs, attachments, timeline, pages tab. </p> </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/actions/","title":"Actions on Cases","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/actions/#actions","title":"Actions","text":"<p>You can make use of any of the available actions.</p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/actions/#flagunflag","title":"Flag/Unflag","text":"<ol> <li>Click the Flag/Unflag option to either flag or unflag a case. </li> </ol> <p>A pop-up message appears</p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/actions/#close","title":"Close","text":"<ol> <li>Click the Close option to remove a case</li> </ol> <p>A new window opens.</p> <ol> <li>Select Status from the list. </li> <li>Change the Summary</li> <li>Click the Close tasks and case button.</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/add-custom-event/","title":"Add custom event","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/add-custom-event/#add-custom-event","title":"Add custom event","text":"<p>In this section you can find information about adding cutom events.</p> <p>To add custom event: </p> <p>After clicking the + option. </p> <p>A new window opens. </p> <ol> <li>Add the Title.</li> <li>Enter the Description.</li> <li>Select Date.</li> <li>Select End date.</li> <li>Click the Add button.</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/add-custom-fields/","title":"Add Custom Fields","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/add-custom-fields/#add-custom-fields","title":"Add Custom Fields","text":"<p>In this section you can find information about adding custom fields.</p> <p>To add custom fields: </p> <p>After clicking the Add option beside the custom fields. </p> <p>A new window opens. </p> <ol> <li>Add the custom fields from the list.</li> <li>Click the Confirm button.</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/attachments/","title":"View Attachments","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/attachments/#view-attachments","title":"View attachments","text":"<p>In this section you can find information about Attachments . </p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/attachments/#add-attachment","title":"Add Attachment","text":"<p>To add a new attachment:</p> <ol> <li>On the cases list page, on the attachments tab, on the cases sub-tab, Click the + to add a new attachment.</li> </ol> <p></p> <p>A new window opens. </p> <ol> <li>Click the Drop file or Click option in attachment. </li> <li>Click the Confirm button. </li> </ol> <p></p> <p>NOTE: A user can add one or more files simultaneously. </p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/attachments/#copy-url","title":"Copy URL","text":"<ol> <li>On the cases list page, on the attachments tab, on the Tasks sub-tab, click the ellipsis (...) corresponding to the url to be copied.</li> <li>Click copy URL. </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases-list/attachments/#download","title":"Download","text":"<ol> <li>On the cases list page, on the attachments tab, on the Tasks sub-tab, Click the ellipsis (...) corresponding to the download option.</li> <li>Click Download. </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases-list/configure-pap-tlp-severity/","title":"Configure Case details","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/configure-pap-tlp-severity/#configure-case-details","title":"Configure Case details","text":"<p>In this section you can find information about configuring case details. </p> <p>Every case has three important elements the TLP, PAP and Severity.  TLP defines the confidentiality of information. PAP is the level of exposure of information to the outsde world and Severity implies the severity of information. </p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/configure-pap-tlp-severity/#configure-tlp-confidentiality-of-information","title":"Configure TLP (Confidentiality of information)","text":"<ol> <li>Select TLP, (White/Green/Amber/Red) from the list.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases-list/configure-pap-tlp-severity/#configure-pap-level-of-exposure-of-information","title":"Configure PAP (Level of exposure of information)","text":"<ol> <li>Select PAP, (White/Green/Amber/Red) from the list.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases-list/configure-pap-tlp-severity/#configure-severity-severity-of-information","title":"Configure Severity (Severity of information)","text":"<ol> <li>Select Severity, (Low/Medium/High/Critical) from the list.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases-list/general/","title":"View a Case","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/general/#general","title":"General","text":"<p>The information on this page comes from templates and is populated. In the left pane of the window, details are displayed, such as created by, created date, TLP, PAP, and severity details. The status of the alert, start date and task completion details can be seen. </p> <p>In the left pane of the window you can configure the PAP,TLP and Severity of the case.  Refer to <code>Configure Case Details</code> for more details. </p> <ol> <li>In the right pane, at bottom of the window, type Comments if any for the team. </li> <li>Enter the Title.</li> <li>Add Tags. (Refer to <code>Add tags</code>).</li> <li>Enter the Description. </li> <li>Add Custom fields. (Refer to <code>Add custom fields</code>).</li> <li>Enter the Company name. </li> <li> <p>Add business unit, detection source and location details. </p> <p></p> </li> <li> <p>Click the Confirm button. </p> </li> <li>Click the Comment button. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/manage-views/","title":"Manage Views","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/manage-views/#manage-views","title":"Manage views","text":"<p>In this section, you can find information about managing views.  </p> <p>To manage views: </p> <ol> <li>Click the default button. </li> <li>Click on Manage Views from the list. </li> </ol> <p></p> <p>A new page opens. It has the Name of the view and the corresponding Actions. </p> <ol> <li>Click the ellipsis (...) corresponding to the name of the view that you want to delete. </li> <li>Click Delete.</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/manage-views/#manage-cases","title":"Manage Cases","text":"<p>There are various option available to apply on the cases.</p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/manage-views/#quick-filters","title":"Quick Filters","text":"<p>To apply Quick filter:</p> <ol> <li>Click the Quick Filter option. </li> <li>The list displays options to select from. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/manage-views/#auto-refresh","title":"Auto refresh","text":"<p>The auto-refresh option allows you to automatically refresh a page. </p> <p>To perform Auto refresh:</p> <ol> <li>On the cases list page, switch on the Auto refresh button. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/manage-views/#stats","title":"Stats","text":"<p>To view Stats: </p> <ol> <li>On the cases list page, switch on the Stats toggle button, the stats will be displayed. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/manage-views/#filters","title":"Filters","text":"<p>To apply filter:</p> <ol> <li> <p>On the page, in the tab, switch on the Filters toggle button.</p> </li> <li> <p>Click Add filters.</p> </li> </ol> <p>Apply Filter to the required field.</p> <p></p> <ol> <li>Select the filters from the list.</li> <li>Click Apply filters.</li> <li>(Optional) Click Clear filters to clear all applied filters.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases-list/manage-views/#sorting","title":"Sorting","text":"<p>Sorting can be performed on any field values.  </p> <p>To Sort:</p> <ol> <li>On the cases list page, click the small arrow that points upwards/downwards to sort on a particular filed name. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/observables/","title":"View Observables","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/observables/#view-observables","title":"View observables","text":"<p>In this section you can find information about observables.</p> <p>Observables represent stateful properties (such as the MD5 hash of a file or the value of a registry key) or measurable events (such as the creation of a registry key or the deletion of a file) that are pertinent to the operation of computers and networks. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/observables/#add-observables","title":"Add observables","text":"<ol> <li>Click the + to add an observable.</li> <li>Type the Type.</li> <li>Type the Value.</li> <li>Select TLP, (White/Green/Amber/Red) from the options.</li> <li>Select PAP, (White/Green/Amber/Red) from the options.</li> <li>Switch the on button for Is IOC. (IoC repository contains objects, and each of the objects contain a specific piece of information.)</li> <li>Switch on the button for Has Been Sighted. </li> <li>Switch on the button for Ignore Similarity. </li> <li>Add Tags. (Refer to <code>Add tags</code>).</li> <li>Type the Description. </li> <li>Click the Save and add another button. </li> <li>Click the Confirm button. </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases-list/observables/#observables-actions","title":"Observables Actions","text":"<p>You can make use of any of the available actions.</p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/observables/#delete","title":"Delete","text":"<ol> <li>Click the Delete option to remove an observable.</li> </ol> <p>A message pops-up</p> <ol> <li>Click the OK button. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/observables/#run-analyzers","title":"Run Analyzers","text":"<ol> <li>Click the Run Analyzers option.</li> </ol> <p>A new window opens.</p> <ol> <li>Select one or more Analyzers from the list.</li> <li>Click the Run Analyzers button.  </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/observables/#responders","title":"Responders","text":"<ol> <li>Click the Responders option.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases-list/observables/#pinunpin","title":"Pin/Unpin","text":"<ol> <li>Click the Pin/Unpin option to pin or unpin observables.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases-list/observables/#export","title":"Export","text":"<p>To Export an observable details file: </p> <ol> <li>Click the Export option.</li> <li>A file is downloaded, that can be exported/sent.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases-list/observables/#copy-data","title":"Copy Data","text":"<ol> <li>Click the Copy data option.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases-list/pages/","title":"View Pages","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/pages/#view-pages","title":"View pages","text":"<p>In this section you will find information regarding the lessons learnt. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/pages/#addedit-pages","title":"Add/Edit Pages","text":"<p>A user can add a new page.</p> <ol> <li>In the right pane of the window, Click + </li> </ol> <p>A new window opens. </p> <ol> <li>Add a Title. </li> <li>Add a Category.</li> <li>Click the Confirm button. </li> </ol> <p></p> <p>A user can edit pages by clicking on the pencil icon. Make changes and click the save icon. </p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/pages/#search-a-page-by-title","title":"Search a Page by Title","text":"<p>A user can search a page by typing the title in the search box.</p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/preview-cases/","title":"Preview Cases","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/preview-cases/#preview-cases","title":"Preview Cases","text":"<p>In this section you can find information about previewing cases and associated details. </p> <p>To preview the cases details:</p> <p>On the list of case details page, there is a Preview button corresponding to the specific case name.</p> <ol> <li>Click the Preview option. </li> </ol> <p></p> <p>The case details preview window opens.</p> <p></p> <p>You can see details like the id, created by, created at date, updated date, TLP, PAP and severity details, title, status, tags, and description details. </p> <p>In the right pane of the window, there is an option to Add the following: </p> <ol> <li>Add Task.</li> <li>Add Observable.</li> <li>Add TTP.</li> </ol> <p>At the bottom of the window, there is an option to Add the following: </p> <ol> <li>Add custom fields.</li> <li> <p>Add business unit, and location  details.</p> </li> <li> <p>Click Add to type business unit, detection-source and location details. </p> <p></p> </li> </ol>"},{"location":"thehive/user-guides/analyst-corner/cases-list/preview-cases/#actions","title":"Actions","text":"<p>You can click the <code>Actions Button</code>  to Flag/unflag, close, or to <code>Run Responders</code>. </p> <p>and to `Run Analyzer. </p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/preview-cases/#case-details","title":"Case Details","text":"<ol> <li> <p>Click on the Go to details button. </p> <p>You can view more details of the case. </p> </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/preview-cases/#case-details-menu","title":"Case details menu","text":"<p>You can view more details of the case by going to the menu. </p> <p>On the top of the page, there are many case options available such as flag/unflag, merge, export, close delete and run responders. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/run-analyzer/","title":"Run Analyzers","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/run-responders/","title":"Run Responders on Case","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/run-responders/#run-responders","title":"Run responders","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/run-responders/#responders","title":"Responders","text":"<ol> <li>Click on responders option.</li> </ol> <p>A new window appears. </p> <ol> <li>Search for a specific responder in the search box.</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/tasks/","title":"View Tasks","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/tasks/#view-tasks","title":"View tasks","text":"<p>Refer to the full section on Tasks for more details.</p> <ul> <li>'About tasks'</li> <li>'Preview task details'</li> <li>'Manage Views'</li> </ul>"},{"location":"thehive/user-guides/analyst-corner/cases-list/timeline/","title":"View Timeline","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/timeline/#view-timeline","title":"View timeline","text":"<p>In this section you can find information about timelines.</p> <p>A time line is a where a user can view the case details at a glance. The timeline has details about custom events, TTPs, logs, tasks, alerts, case events shown on a timeline. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/timeline/#configure-timeline","title":"Configure timeline","text":"<p>Add a custom event (refer to - <code>Add a Custom Event</code>), Export to JSON, Zoom out, zoom in, centre timeline, graph view, list view. There is an option to view the list of custom events. (under the custom events i.e. the last icon, - you can select the custom event to include in the timeline or not).</p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/ttps/","title":"View TTPs","text":""},{"location":"thehive/user-guides/analyst-corner/cases-list/ttps/#ttps","title":"TTPs","text":"<p>In this section you will find information about TTPs. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/ttps/#add-ttp","title":"Add TTP","text":"<p>To add TTP: </p> <p>After clicking the + option. </p> <p>A new window opens. </p> <ol> <li>Select the Catalogue from the list.</li> <li>Select the Occur date.</li> <li>Click the Add toggle button on to add Procedure.</li> <li>Click the Save and add another or the Confirm button.</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/cases-list/ttps/#delete-ttp","title":"Delete TTP","text":"<ol> <li>Click the ellipsis (...) corresponding to the TTP to be deleted.</li> <li>Click the delete option.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/dashboard/about-dashboard/","title":"About Dashboard","text":""},{"location":"thehive/user-guides/analyst-corner/dashboard/about-dashboard/#dashboards","title":"Dashboards","text":"<p>In this section you can find information about dashboards.</p> <p>A dashboard is a visual display of data to provide information at-a-glance. The dashboard is configurable, allowing you to choose which data you want to see and whether you want to include charts or graphs to visualize the numbers.</p> <p>The dashboard lists all details of cases such as, status, name, version number, widget, created by, dates of creation and date on which data was updated. A user can apply filters on the dashboard, sort based on fields, and manage views. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/dashboard/filter-sort/","title":"Filter and Sort","text":""},{"location":"thehive/user-guides/analyst-corner/dashboard/filter-sort/#filters-and-sort","title":"Filters and Sort","text":"<p>In this section, you can find information about applying filters.</p>"},{"location":"thehive/user-guides/analyst-corner/dashboard/filter-sort/#filters","title":"Filters","text":"<p>To apply filter:</p> <ol> <li> <p>On the dashboard page, switch on the Filters toggle button.</p> </li> <li> <p>Click Add filters.</p> </li> </ol> <p>Apply Filter to the required field.</p> <p></p> <ol> <li>Select the filters from the list.</li> <li>Click Apply filters.</li> <li>(Optional) Click Clear filters to clear all applied filters.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/dashboard/filter-sort/#sorting","title":"Sorting","text":"<p>Sorting can be performed on any field values.  </p> <p>To Sort:</p> <ol> <li>On the dashboard page, Click the small arrow that points upwards/downwards to sort on a particular filed name. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/dashboard/manage-dashboard/","title":"Manage Dashboard","text":""},{"location":"thehive/user-guides/analyst-corner/dashboard/manage-dashboard/#manage-dashboard","title":"Manage Dashboard","text":"<p>In this section you can find information about managing dashboards.</p>"},{"location":"thehive/user-guides/analyst-corner/dashboard/manage-dashboard/#add-a-dashboard","title":"Add a dashboard","text":"<p>To Add a dashboard: </p> <ol> <li> <p>Click the + , to Add a dashboard. </p> <p></p> <p>A new window opens. </p> </li> <li> <p>Enter the  Title.</p> </li> <li>Enter the  Description.</li> <li>Select the Visibility option. (Private or Shared) </li> <li>Click the confirm button. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/dashboard/manage-dashboard/#edit-a-dashboard","title":"Edit a dashboard","text":"<p>To Edit a dashboard: </p> <ol> <li> <p>Click the Edit option from the list. </p> <p></p> <p>A new window opens. </p> </li> <li> <p>Edit the required fields.    </p> </li> <li>Click the Confirm button. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/dashboard/manage-dashboard/#delete-a-dashboard","title":"Delete a dashboard","text":"<p>To Delete a dashboard: </p> <ol> <li> <p>Click the Delete option from the list.</p> <p></p> <p>A new message pops-up.</p> </li> <li> <p>Click OK to delete the dashboard from the list.</p> </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/dashboard/manage-dashboard/#import-a-dashboard","title":"Import a dashboard","text":"<p>To Import a dashboard: </p> <ol> <li> <p>Click the Import dashboard option. </p> <p></p> <p>A new window opens. </p> </li> <li> <p>Click the Drop file or Click option in attachment. </p> </li> </ol> <p>NOTE: The file must be a valid JSON file. You can use the exported dashboard directly from theHive platform. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/dashboard/manage-dashboard/#export-a-dashboard","title":"Export a dashboard","text":"<p>To Export a dashboard: </p> <ol> <li>Click the Export option.</li> <li>A file is downloaded, that can be exported/sent.</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/dashboard/manage-views/","title":"Manage View","text":""},{"location":"thehive/user-guides/analyst-corner/dashboard/manage-views/#manage-views","title":"Manage Views","text":"<p>In this section, you can find information about managing dashboard views.  </p> <p>To manage views: </p> <ol> <li>Click the default button. </li> <li>Click the Manage Views from the list. </li> </ol> <p></p> <p>A new page opens. </p> <p>It has the Name of the view and the corresponding Actions. </p> <ol> <li>Click the ellipsis (...) corresponding to the name of the view that you want to delete. </li> <li>Click Delete.</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/search/about-search/","title":"Search","text":""},{"location":"thehive/user-guides/analyst-corner/search/about-search/#search","title":"Search","text":"<p>In this section you can find information about the Search function.</p> <p>The Search function allows you to refine search by selecting a particular search criteria. Often, the search criteria are broad, resulting in large number of results. The search function allow users to trim down their list of search results by applying filters on one or more search results.  </p> <p></p> <p>You can search based on the following search criteria: </p> <ol> <li>Cases.</li> <li>Tasks.</li> <li>Task Logs.</li> <li>Observables.</li> <li>Alerts.</li> <li>Jobs.</li> <li>Audit Logs. </li> </ol> <p>Based on your search criteria, a set of search results appear in the right pane of the page.  A maximum of 300 results can be displayed on the page that can be navigated using the Previous and Next buttons at the bottom of the page. </p>"},{"location":"thehive/user-guides/analyst-corner/search/search-scope-by/alerts/","title":"Search by Alerts","text":""},{"location":"thehive/user-guides/analyst-corner/search/search-scope-by/alerts/#search-by-alerts","title":"Search by Alerts","text":"<p>To search by Alerts:</p> <p>if a user wants to search based on Alerts</p> <ol> <li> <p>In the left pane, Click on Alerts.  </p> <p>A user can specify search criterias, by using Add new filters. A user can user clear all to remove any of the applied filters. </p> </li> <li> <p>Click on Add new filters. </p> </li> <li>Select the required filters from the list.</li> <li>Click the Search button at the bottom. </li> </ol> <p>A set of search results appear in the right pane of the page. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/search/search-scope-by/auditlogs/","title":"Search by Audit logs","text":""},{"location":"thehive/user-guides/analyst-corner/search/search-scope-by/auditlogs/#search-by-audit-logs","title":"Search by Audit logs","text":"<p>To search by Audit Logs:</p> <p>if you want to search based on Audit Logs</p> <ol> <li> <p>In the left pane, Click on Audit Logs.  </p> <p>You can specify search criterias, by using Add new filters. A user can user clear all to remove any of the applied filters. </p> </li> <li> <p>Click on Add new filters. </p> </li> <li>Select the required filters from the list.</li> <li>Click the Search button at the bottom. </li> </ol> <p>A set of search results appear in the right pane of the page. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/search/search-scope-by/cases/","title":"Search by Cases","text":""},{"location":"thehive/user-guides/analyst-corner/search/search-scope-by/cases/#search-by-cases","title":"Search by Cases","text":"<p>To search by Cases:</p> <p>if you want to search based on cases</p> <ol> <li> <p>In the left pane, Click on Cases.  </p> <p>You can specify search criterias, by using Add new filters. You can use clear all to remove any of the applied filters. </p> </li> <li> <p>Click  Add new filters. </p> </li> <li>Select the required filters from the list. (e.g. _createdBy, is not empty etc.)</li> <li>Click the Search button at the bottom. </li> </ol> <p>A set of search results appear in the right pane of the page. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/search/search-scope-by/jobs/","title":"Search by Jobs","text":""},{"location":"thehive/user-guides/analyst-corner/search/search-scope-by/jobs/#search-by-jobs","title":"Search by Jobs","text":"<p>To search by Jobs:</p> <p>if you want to search based on Jobs</p> <ol> <li> <p>In the left pane, Click on Jobs.  </p> <p>You can specify search criterias, by using Add new filters. You can use clear all to remove any of the applied filters. </p> </li> <li> <p>Click Add new filters. </p> </li> <li>Select the required filters from the list.</li> <li>Click the Search button at the bottom. </li> </ol> <p>A set of search results appear in the right pane of the page. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/search/search-scope-by/observables/","title":"Search by Observables","text":""},{"location":"thehive/user-guides/analyst-corner/search/search-scope-by/observables/#search-by-observables","title":"Search by Observables","text":"<p>To search by Observables:</p> <p>if you want to search based on Observables</p> <ol> <li> <p>In the left pane, Click on Observables.  </p> <p>You can specify search criterias, by using Add new filters. You can use clear all to remove any of the applied filters. </p> </li> <li> <p>Click Add new filters. </p> </li> <li>Select the required filters from the list.</li> <li>Click the Search button at the bottom. </li> </ol> <p>A set of search results appear in the right pane of the page. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/search/search-scope-by/tasklogs/","title":"Search by Task Logs","text":""},{"location":"thehive/user-guides/analyst-corner/search/search-scope-by/tasklogs/#search-by-tasks-logs","title":"Search by Tasks logs","text":"<p>To search by Task logs:</p> <p>if you want to search based on Tasks Logs</p> <ol> <li> <p>In the left pane, Click on Tasks Logs.  </p> <p>You can specify search criterias, by using Add new filters. You can use clear all to remove any of the applied filters. </p> </li> <li> <p>Click Add new filters. </p> </li> <li>Select the required filters from the list.</li> <li>Click the Search button at the bottom. </li> </ol> <p>A set of search results appear in the right pane of the page. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/search/search-scope-by/tasks/","title":"Search by Tasks","text":""},{"location":"thehive/user-guides/analyst-corner/search/search-scope-by/tasks/#search-by-tasks","title":"Search by Tasks","text":"<p>To search by Tasks:</p> <p>if you want to search based on Tasks</p> <ol> <li> <p>In the left pane, Click on Tasks.  </p> <p>You can specify search criterias, by using Add new filters. You can use clear all to remove any of the applied filters. </p> </li> <li> <p>Click Add new filters. </p> </li> <li>Select the required filters from the list.</li> <li>Click the Search button at the bottom. </li> </ol> <p>A set of search results appear in the right pane of the page. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/task/","title":"Index","text":"<p>Preview task details</p>"},{"location":"thehive/user-guides/analyst-corner/task/about-tasks/","title":"About Tasks","text":""},{"location":"thehive/user-guides/analyst-corner/task/about-tasks/#tasks","title":"Tasks","text":"<p>Task details, require action from its users. Task details page displays a list of tasks.  When navigating through the Task list, you can easily see and determine which Task needs an action. </p>"},{"location":"thehive/user-guides/analyst-corner/task/about-tasks/#to-view-task-details","title":"To view task details","text":"<p>You can click on any of the tasks in the list to view more details. </p> <p></p> <p>The details are displayed</p> <p></p> <p>In the left pane of the window you can configure the PAP, TLP and Severity.  Refer to 'Configure Alert Details' for more details. </p>"},{"location":"thehive/user-guides/analyst-corner/task/manage-views/","title":"Manage Views","text":""},{"location":"thehive/user-guides/analyst-corner/task/manage-views/#manage-views","title":"Manage Views","text":"<p>In this section, you can find information about managing views.  </p> <p>To manage views: </p> <ol> <li>Click the default button. </li> <li>Click the Manage Views from the list. </li> </ol> <p></p> <p>A new page opens. It has the Name of the view and the corresponding Actions. </p> <ol> <li>Click the ellipsis (...) corresponding to the name of the view that you want to delete. </li> <li>Click Delete.</li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/task/manage-views/#manage-tasks","title":"Manage Tasks","text":"<p>There are various option available to apply on the tasks. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/task/manage-views/#quick-filters","title":"Quick Filters","text":"<p>To apply Quick filter:</p> <ol> <li>Click the Quick Filter option. </li> <li>The list displays options to select from. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/task/manage-views/#auto-refresh","title":"Auto refresh","text":"<p>The auto-refresh option allows you to automatically refresh a page. </p> <p>To perform Auto refresh:</p> <ol> <li>On the tasks list page, switch on the Auto refresh button. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/task/manage-views/#show-tasks-as-groups","title":"Show tasks as groups","text":"<p>To view tasks as groups: </p> <ol> <li>On the tasks list page, switch on the tasks as groups button, the task groups will be displayed. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/task/manage-views/#filters","title":"Filters","text":"<p>To apply filter:</p> <ol> <li>On the tasks list page, switch on the Filters toggle button.</li> <li>Click Add filters.</li> </ol> <p>Apply Filter to the required field.</p> <p></p> <ol> <li>Select the filters from the list.</li> <li>Click Apply filters.</li> <li>(Optional) Click Clear filters to clear all applied filters.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/task/manage-views/#sorting","title":"Sorting","text":"<p>Sorting can be performed on any field values.  </p> <p>To Sort:</p> <ol> <li>On the tasks list page, Click the small arrow that points upwards/downwards to sort on a particular filed name. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/task/preview-task-details/Preview-tasks/","title":"Preview Tasks","text":""},{"location":"thehive/user-guides/analyst-corner/task/preview-task-details/Preview-tasks/#preview-tasks","title":"Preview Tasks","text":"<p>In this section you can find information about previewing tasks. </p> <p>To preview the task details:</p> <p>On the list of tasks page, there is a Preview button corresponding to the specific task name.</p> <ol> <li>Click the Preview option. </li> </ol> <p></p> <p>The task details preview window opens.</p> <p></p> <p>You can see details like the id, created by, created at date, updated date, title, flag, status, group, assignee, start date, due date, description, activity, responder reports of the task. </p> <p>You can click the <code>Actions Button</code>  to start, delete, pin/unpin, flag/unflag the tasks or to <code>Run Responders</code>. </p> <p></p> <p>You can add activities/task logs by clicking on the +. Refer to <code>Create a task log</code>.</p>"},{"location":"thehive/user-guides/analyst-corner/task/preview-task-details/Preview-tasks/#task-details","title":"Task Details","text":"<ol> <li>Click the Go to details button to view more details of the task.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/task/preview-task-details/Preview-tasks/#task-details-menu","title":"Task details menu","text":"<ol> <li>Click the Go to details button to view more details of the task. </li> </ol> <p>On the top of the page, there are many task options available such as flag, merge, export, close, delete, responders. </p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/task/preview-task-details/actions/","title":"Actions on Tasks","text":""},{"location":"thehive/user-guides/analyst-corner/task/preview-task-details/actions/#actions","title":"Actions","text":"<p>You can make use of any of the available actions.</p> <p></p>"},{"location":"thehive/user-guides/analyst-corner/task/preview-task-details/actions/#start","title":"Start","text":"<ol> <li>Click the Start option to begin a task.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/task/preview-task-details/actions/#delete","title":"Delete","text":"<ol> <li>Click the Delete option to remove a task.</li> </ol> <p>A message pops-up</p> <ol> <li>Click the OK button. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/task/preview-task-details/actions/#unflag","title":"Unflag","text":"<ol> <li>Click the Flag/Unflag option to flag or unflag a task.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/task/preview-task-details/actions/#unpin","title":"Unpin","text":"<ol> <li>Click the Pin/Unpin option to pin or unpin a task.</li> </ol>"},{"location":"thehive/user-guides/analyst-corner/task/preview-task-details/create-a-task-log/","title":"Create a task log","text":""},{"location":"thehive/user-guides/analyst-corner/task/preview-task-details/create-a-task-log/#create-a-task-log","title":"Create a task log","text":"<p>In this section you can find information about creating a task log.</p> <p>To  create a task log: </p> <ol> <li>Enter the log message in Description area of the given box. </li> <li>Switch on the Toggle the button to include in timeline. (If you opt for a timeline, you have to provide the date for the timeline).</li> <li>In Attachment Drop file or click. You can upload an attachment. </li> <li>Click the Create a Task Log button. </li> </ol> <p></p>"},{"location":"thehive/user-guides/analyst-corner/task/preview-task-details/run-responders/","title":"Run Responders from Task","text":""},{"location":"thehive/user-guides/analyst-corner/task/preview-task-details/run-responders/#run-responders","title":"Run Responders","text":""},{"location":"thehive/user-guides/analyst-corner/task/preview-task-details/run-responders/#responders","title":"Responders","text":"<ol> <li>Click on responders option.</li> </ol> <p>A new window appears. </p> <ol> <li>Search for a specific responder in the search box.</li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/accounts/","title":"User Management","text":""},{"location":"thehive/user-guides/organisation/accounts/#manage-users-in-your-organisation","title":"Manage users in your Organisation","text":"<p>Info</p> <p>org-admin profile or at least the permission <code>manageUser</code> is required to manage users in your organisation.</p>"},{"location":"thehive/user-guides/organisation/accounts/#list-of-users","title":"List of users","text":"<p>In your organisation, click on Organisation in the menu on the left to access the list of users. The first tab is Users.</p> <p> </p> List of user accounts"},{"location":"thehive/user-guides/organisation/accounts/#user-information","title":"User information","text":"<p>To access detailed information about a user, click the Preview button</p> <p> </p> User information"},{"location":"thehive/user-guides/organisation/accounts/#configuration-parameters","title":"Configuration parameters","text":"Avatar Update the avatar associated with the user by drag&amp;drop a new file (PNG or JPG files). Login User login Email email address for the account. This is used to send notifications or reset password links to users. Login is used if no email is filled there Type Type of the account. Normal or Service. A Service account cannot open interactive session Locked Block a user from logging in the application MFA Tells if a user has configured MFA or not (Multi Factor Authentication). If yes, Yes is displayed API Key Define, Renew, Reveal or Revoke API key of the account Profile Information about the profile given to the user Permissions List of permissions included in the profile Password Create or update the password of the user Reset Password If the application is configured with a SMTP server, send an email with a magic link to the user. link is active for a short time period. Sessions List of opened interactive sessions. Click delete to close a session"},{"location":"thehive/user-guides/organisation/accounts/#add-users","title":"Add Users","text":"<p>org-admin users or users with the role <code>manageUser</code> in their profile can add users in the current Organisation. </p> <p>Click the  button to add an account in the current organisation, and follow create an account and update an account guides.</p>"},{"location":"thehive/user-guides/organisation/accounts/#user-management","title":"User management","text":"<p>Accounts can be deleted or locked, only in the current organisation.</p>"},{"location":"thehive/user-guides/organisation/attachments/","title":"Attachments Usage Guide","text":""},{"location":"thehive/user-guides/organisation/attachments/#attachments","title":"Attachments","text":"<p>This sections gather all the files and images added at the Organisation level, like the knowledge base.</p>"},{"location":"thehive/user-guides/organisation/custom-tags/","title":"Custom Tags Management","text":""},{"location":"thehive/user-guides/organisation/custom-tags/#custom-tags","title":"Custom tags","text":"<p>Custom tags gathers all tags coming from Alerts or added to Cases or Observables that are not included in Taxonomies added to TheHive, even if they are not activated.</p> <p> </p> Custom tags <p>The list shows statistics about the number of them found in Cases,Alerts or Observables.</p>"},{"location":"thehive/user-guides/organisation/custom-tags/#configuration","title":"Configuration","text":"<ul> <li>Names and colors can be adjusted for all Custom tags</li> <li>Each tag can also be deleted</li> </ul> <p>Warning</p> <ul> <li>Deleting a tag from this menu will remove the tag on every Alert, Case &amp; Observables in the organisation.</li> </ul>"},{"location":"thehive/user-guides/organisation/functions/","title":"Functions Configuration","text":""},{"location":"thehive/user-guides/organisation/functions/#functions","title":"Functions","text":"<p>Info</p> <p>This feature is available with TheHive versions 5.1 and higher.</p> <p>Functions enable you to integrate external applications directly into TheHive processing.</p> <p>A Function is a piece of custom Javascript code that runs inside TheHive. The function can receive inputs from the outside, treat it and call TheHive APIs directly.</p> <p>This can be used for instance to create alerts inside TheHive without a python glue service that transforms the data.</p>"},{"location":"thehive/user-guides/organisation/functions/#create-a-function","title":"Create a function","text":"<p>Let's imagine that when an event occurs in your system, you want to create an alert in TheHive. Your external system has its own schema for the events, something like:</p> <pre><code>{\n    \"eventId\": \"d9ec98b1-410f-40eb-8634-cfe189749da6\",\n    \"date\": \"2021-06-05T12:45:36.698Z\",\n    \"title\": \"An intrusion was detected\",\n    \"details\": \"An intrusion was detected on the server 10.10.43.2\",\n    \"data\": [\n        {\"kind\": \"ip\", \"value\": \"10.10.43.2\", \"name\": \"server-ip\" },\n        {\"kind\": \"name\", \"value\": \"root\", \"name\": \"login\" },\n        {\"kind\": \"ip\", \"value\": \"92.43.123.1\", \"name\": \"origin\" }\n    ]\n}\n</code></pre> <p>This format is not the same as TheHive, so you need to transform the data to match TheHive alert format.</p> <p>As an <code>org-admin</code>, you can create new functions for your organisation that can take this input, transform it into TheHive format and create an alert from it.</p> <p>The code of the function would be something like this:</p> <pre><code>function handle(input, context) {\n    const theHiveAlert = {\n        \"type\": \"event\",\n        \"source\": \"my-system\",\n        \"sourceRef\": input.eventId,\n        \"title\": input.title,\n        \"description\": input.details,\n        \"date\": (new Date(input.date)).getTime(),\n        \"observables\": input.data.map(data =&gt; {\n            // map event data kind to TheHive Observable type\n            const dataType = data.kind === \"ip\" ? \"ip\": \"other\";\n            return {\n                \"dataType\": dataType,\n                \"data\": data.value,\n                \"tags\": [`name:${data.name}`] // use a tag for the data name\n            }\n        })\n    };\n    // call TheHive APIs, here alert creation\n    return context.alert.create(theHiveAlert);\n}\n</code></pre> <p></p> <p>A function can be in one of three modes:</p> <ul> <li><code>Enabled</code>: The function will be executed when called</li> <li><code>Disabled</code>: The function will not be executed when called</li> <li><code>Dry-Run</code>: The function will be executed but no entity will be created or modified in TheHive. Entity creations will return <code>null</code> instead. This can be useful to test your integration before setting it live.</li> </ul> <p>The creation page allows you to test your function and see what it would return once executed. In <code>dry-run</code> mode, the function will be executed but no resource creation or modification will be executed.</p> <p></p> <p></p>"},{"location":"thehive/user-guides/organisation/functions/#call-a-function","title":"Call a function","text":"<p>Once saved, the function can then be called with an http call from your system:</p> <pre><code>curl -X POST -H 'Authorization: Bearer $API_KEY' https://&lt;thehive_url&gt;/api/v1/function/&lt;function_name&gt; -H 'Content-Type: application/json' --data '\n{\n    \"eventId\": \"d9ec98b1-410f-40eb-8634-cfe189749da6\",\n    \"date\": \"2021-06-05T12:45:36.698Z\",\n    \"title\": \"An intrusion was detected\",\n    \"details\": \"An intrusion was detected on the server 10.10.43.2\",\n    \"data\": [\n        {\"kind\": \"ip\", \"value\": \"10.10.43.2\", \"name\": \"server-ip\" },\n        {\"kind\": \"name\", \"value\": \"root\", \"name\": \"login\" },\n        {\"kind\": \"ip\", \"value\": \"92.43.123.1\", \"name\": \"origin\" }\n    ]\n}\n'\n</code></pre> <p>TheHive will take your input (the body of the http call), the definition of your function and execute the function with the input. It will respond to the http call with the data returned by the function.</p>"},{"location":"thehive/user-guides/organisation/functions/#example-create-an-alert-from-a-splunk-alert","title":"Example: Create an alert from a Splunk alert","text":"<p>When creating a Splunk alert, you can define a webhook as an action. So when the alert is triggered the webhook is called with a payload. But the payload is defined by splunk and can't be changed.</p> <p>It should look a bit like:</p> <pre><code>{\n\"sid\": \"rt_scheduler__admin__search__RMD582e21fd1bdd5c96f_at_1659705853_1.1\",\n\"search_name\": \"My Alert\",\n\"app\": \"search\",\n\"owner\": \"admin\",\n\"results_link\": \"http://8afeb4633464:8000/app/search/search?q=%7Cloadjob%20rt_scheduler__admin__search__RMD582e21fd1bdd5c96f_at_1659705853_1.1%20%7C%20head%201%20%7C%20tail%201&amp;earliest=0&amp;latest=now\",\n\"result\": {\n    \"_time\": \"1659705859.827088\",\n    \"host\": \"8afeb4633464\",\n    \"source\": \"audittrail\",\n    \"sourcetype\": \"audittrail\",\n    \"action\": \"edit_search_schedule_priority\",\n    \"info\": \"granted\",\n    \"user\": \"admin\",\n    \"is_searches\": \"0\",\n    \"is_not_searches\": \"1\",\n    \"is_modify\": \"0\",\n    \"is_not_modify\": \"1\",\n    \"_confstr\": \"source::audittrail|host::8afeb4633464|audittrail\",\n    \"_indextime\": \"1659705859\",\n    \"_kv\": \"1\",\n    \"_raw\": \"Audit:[timestamp=08-05-2022 13:24:19.827, user=admin, action=edit_search_schedule_priority, info=granted ]\",\n    \"_serial\": \"1\",\n    \"_si\": [\n    \"8afeb4633464\",\n    \"_audit\"\n    ],\n    \"_sourcetype\": \"audittrail\",\n    \"_subsecond\": \".827088\"\n}\n}\n</code></pre> <p>To transform this splunk alert into a TheHive alert, a function like this can be used:</p> <pre><code>function handle(input, context) {\n    const theHiveAlert = {\n        \"type\": \"splunk\",\n        \"source\": input.search_name,\n        \"sourceRef\": input.result._serial,\n        \"title\": `Splunk Alert triggered: ${input.search_name} by ${input.result.sourcetype}`,\n        \"description\": `Alert created by splunk search '${input.search_name}:\\n${input.result._raw}'`,\n        \"date\": (new Date(parseFloat(input.result._time)*1000)).getTime(),\n        \"observables\": [\n            {\"dataType\": \"hostname\", \"data\": input.result.host},\n            {\"dataType\": \"other\", \"data\": input.result.action, \"message\": \"action\"},\n            {\"dataType\": \"other\", \"data\": input.result._raw, \"message\": \"raw\"}\n        ]\n    };\n    return context.alert.create(theHiveAlert);\n}\n</code></pre> <p>In splunk, you will need to set the webhook url to TheHive function url.</p>"},{"location":"thehive/user-guides/organisation/functions/#example-cold-case-automation","title":"Example: Cold case automation","text":"<p>When called, this function will:</p> <ul> <li>find all cases that are <code>New</code> or <code>InProgress</code> and were not updated in the last month</li> <li>to each of those cases, add a tag <code>cold-case</code></li> </ul> <pre><code>// Will find the \"New\" or \"InProgress\" cases that were not updated since one month\n// For each case, add a tag \"cold-case\"\nfunction handle(input, context) {\n    const now = new Date();\n    const lastMonth = new Date();\n    lastMonth.setMonth(now.getMonth() - 1);\n    const filters = [\n        {\n            _name: \"filter\",\n            _and: [\n                {\n                    _or: [{ _field: \"stage\", _value: \"New\" }, { _field: \"stage\", _value: \"InProgress\" },]\n                },\n                {\n                    _lt: { _field: \"_updatedAt\", _value: lastMonth.getTime() }\n                }\n            ]\n        }\n    ];\n    const list = context.caze.find(filters);\n    const authorizedCases = list\n        .filter(caze =&gt; caze.userPermissions.indexOf(\"manageCase/update\") &gt; 0);\n    console.log(authorizedCases.map(c =&gt; c.number));\n    console.log(`Will update ${authorizedCases.length} cases`);\n\n    authorizedCases.forEach(caze =&gt; {\n        context.caze.update(caze._id, { addTags: [\"cold-case\"] })\n    });\n}\n</code></pre>"},{"location":"thehive/user-guides/organisation/functions/#context-api","title":"Context API","text":"<p>Info</p> <p>The objects in the context API are the same as the ones used in the v1 Http Api.</p> <p>For more details on the expected fields of each object, please refer to the Http Api Documentation</p>"},{"location":"thehive/user-guides/organisation/functions/#user","title":"User","text":"<ul> <li><code>userId: string</code> : login of the user executing the function</li> <li><code>userName: string</code>: name of the user executing the function</li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#http-request","title":"Http request","text":"<ul> <li><code>request.queryString() : Record&lt;string, string[]&gt;</code>: Dictionary with the request query string formatted as a map</li> <li><code>request.getQueryString(key: string): string | null</code>: Get a value from the query string</li> <li><code>request.getHeader(name: string): string | null</code>: Get the value of a header from the request</li> <li><code>request.headers(): Record&lt;string, string&gt;</code>: Get the request headers</li> <li><code>request.contentType: string</code>: Value of the request header Content-Type</li> <li><code>request.remoteAddress()</code>: Get the ip adress of the caller</li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#query","title":"Query","text":"<ul> <li><code>query.execute(query: any[])</code>: execute a query on the database (cf. Api docs =&gt; query)</li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#alert","title":"Alert","text":"<ul> <li><code>alert.create(input: InputCreateAlert): OutputAlert</code></li> <li><code>alert.get(id: string): OutputAlert</code></li> <li><code>alert.update(InputUpdateAlert): OutputAlert</code></li> <li><code>alert.delete(alertId: string): void</code></li> <li><code>alert.createCase(alert: InputCreateAlert): OutputCase</code></li> <li><code>alert.bulkDelete(input: {ids: string[]}): void</code></li> <li><code>alert.mergeWithCase(alertId: string, caseId: string): OutputCase</code></li> <li><code>alert.bulkMergeWithCase( {caseId: string, alertIds: string[]} ): OutputCase</code></li> <li><code>alert.followAlert(alertId: string): OutputAlert</code></li> <li><code>alert.unfollowAlert(alertId: string): OutputAlert</code></li> <li><code>alert.importInCase(alertId: string, caseId: string): OutputAlert</code></li> <li><code>alert.bulkUpdate(input: {ids: string[]} &amp; InputUpdateAlert): void</code></li> <li><code>alert.find(query: any[]): OutputAlert[]</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#case","title":"Case","text":"<p><code>case</code> is a reserved keywork is java, so <code>caze</code> is used instead.</p> <ul> <li><code>caze.create(input: InputCreateCase): OutputCase</code></li> <li><code>caze.get(idOrNumber: string): OutputCase</code></li> <li><code>caze.update(idOrNumber: string, update: InputUpdateCase): void</code></li> <li><code>caze.merge(ids: string[]): OutputCase</code></li> <li><code>caze.delete(idOrNumber: string): void</code></li> <li><code>caze.changeCaseOwnership(idOrNumber: string, update: InputChangeCaseOwnership): void</code></li> <li><code>caze.unlinkAlert(caseId: string, alertId: string): void</code></li> <li><code>caze.mergeSimilarObservables(caseId: string): void</code></li> <li><code>caze.bulkUpdate(update: {ids: string[]} &amp; InputUpdateCase): void</code></li> <li><code>caze.bulkApplyCaseTemplate(update: {ids: string[]} &amp; InputApplyCaseTemplate): void</code></li> <li><code>caze.find(query: any[]): OutputCase[]</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#tasks","title":"Tasks","text":"<ul> <li><code>task.get(id: string): OutputTask</code></li> <li><code>task.update(idOrName: string, update: Partial&lt;OutputTask&gt;): void</code></li> <li><code>task.delete(id: string): void</code></li> <li><code>task.find(query: any[]): OutputTask[]</code></li> <li><code>task.setActionRequired(taskId: string, orgId: string): void</code></li> <li><code>task.setActionDone(taskId: string, orgId: string): void</code></li> <li><code>task.isActionRequired(taskId: string): Record&lt;string, bool&gt;</code></li> <li><code>task.createInCase(caseId: string, task: InputTask): OutputTask</code></li> <li><code>task.bulkUpdate(update: {ids: string[]} &amp; Partial&lt;OutputTask&gt;): void</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#log","title":"Log","text":"<ul> <li><code>log.create(taskId: string, log: InputCreateLog): OutputLog</code></li> <li><code>log.update(logId: string, update: InputUpdateLog): void</code></li> <li><code>log.delete(logId: string): void</code></li> <li><code>log.deleteAttachment(logId: string, attachmentId: string): void</code></li> <li><code>log.find(query: any[]): OutputLog[]</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#observable","title":"Observable","text":"<ul> <li><code>observable.createInCase(caseId: string, observable: InputObservable): OutputObservable</code></li> <li><code>observable.createInAlert(alertId: string, observable: InputObservable): OutputObservable)</code> </li> <li><code>observable.bulkUpdate(update: {ids: string[]} &amp; Partial&lt;OutputObservable&gt;)</code></li> <li><code>observable.get(idOrName: string): OutputObservable</code></li> <li><code>observable.update(id: string, update: Partial&lt;OutputObservable&gt;): void</code></li> <li><code>observable.delete(id: string): void</code></li> <li><code>observable.find(query: any[]): OutputObservable[]</code></li> <li><code>observable.updateAllTypes(fromType: string, toType: String): void</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#observable-type","title":"Observable Type","text":"<ul> <li><code>observableType.get(id: string): OutputObservableType</code></li> <li><code>observableType.delete(id: string): void</code></li> <li><code>observableType.create(ot: InputObservableType)</code></li> <li><code>observableType.find(query: any[]): OutputObservableType[]</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#customfield","title":"CustomField","text":"<ul> <li><code>customField.list(): OutputCustomField[]</code></li> <li><code>customField.update(idOrName: string, update: Partial&lt;OutputCustomField&gt;): void</code></li> <li><code>customField.delete(idOrName: string): void</code></li> <li><code>customField.create(cf: InputCustomField): OutputCustomField</code></li> <li><code>customField.find(query: any[]): OutputCustomField[]</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#case-template","title":"Case Template","text":"<ul> <li><code>caseTemplate.get(idOrName: string): OutputCaseTemplate</code> </li> <li><code>caseTemplate.update(idOrName: string, update: Partial&lt;InputCaseTemplate&gt;): void</code></li> <li><code>caseTemplate.delete(idOrName: string): void</code></li> <li><code>caseTemplate.create(template: InputCaseTemplate): OutputCaseTemplate</code></li> <li><code>caseTemplate.find(query: any[]): OutputCaseTemplate[]</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#procedure","title":"Procedure","text":"<ul> <li><code>procedure.bulkCreateInCase(caseId: string, input: {procedures: InputProcedure[]}): OutputProcedure[]</code></li> <li><code>procedure.bulkCreateInAlert(alertId: string, input: {procedures: InputProcedure[]}): OutputProcedure[]</code></li> <li><code>procedure.createInCase(caseId: string, procedure: InputProcedure): OutputProcedure</code></li> <li><code>procedure.createInAlert(alertId: string, procedure: InputProcedure): OutputProcedure</code></li> <li><code>procedure.update(id: string, procedure: Partial&lt;OutputProcedure&gt;): void</code></li> <li><code>procedure.delete(id: string): void</code></li> <li><code>procedure.find(query: any[])</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#case-status","title":"Case Status","text":"<ul> <li><code>caseStatus.create(input: InputCreateCaseStatus): OutputCaseStatus</code></li> <li><code>caseStatus.update(idOrName: string, update: InputUpdateCaseStatus): void</code></li> <li><code>caseStatus.delete(idOrName: string): void</code></li> <li><code>caseStatus.find(query: any[]): OutputCaseStatus[]</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#alert-status","title":"Alert Status","text":"<ul> <li><code>alertStatus.create(input: InputCreateAlertStatus): OutputAlerttatus</code></li> <li><code>alertStatus.update(idOrName: string, update: InputUpdateAlertStatus): void</code></li> <li><code>alertStatus.delete(idOrName: string): void</code></li> <li><code>alertStatus.find(query: any[]): OutputAlerttatus[]</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#comment","title":"Comment","text":"<ul> <li><code>comment.createInCase(caseId: string, comment: InputCreateComment): OutputComment</code></li> <li><code>comment.createInAlert(alertId:: string, comment: InputCreateComment): OutputComment</code></li> <li><code>comment.update(id: string, update: InputUpdateComment): void</code></li> <li><code>comment.delete(id: string): void</code></li> <li><code>comment.find(query: any[]): OutputComment[]</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#share","title":"Share","text":"<ul> <li><code>share.setCaseShares(caseId: string, input: InputCreateShares): OutputShare[]</code></li> <li><code>share.removeSharesFromCase(caseId: string, input: InputRemoveShares): void</code></li> <li><code>share.removeShare(shareId: string): void</code></li> <li><code>share.removeShares(input: {ids: string[]} ): void</code></li> <li><code>share.removeTaskShares(taskId: string, input: InputRemoveShares): void</code></li> <li><code>share.removeObservableShares(observableId: string, input: InputRemoveShares): void</code></li> <li><code>share.listShareCases(caseId: string): OutputShare[]</code></li> <li><code>share.listShareTasks(taskId: string): OutputShare[]</code></li> <li><code>share.listShareObservables(observableId: string): OutputShare[]</code></li> <li><code>share.shareCase(caseId: string, input: InputCreateShare): OutputShare</code></li> <li><code>share.shareTask(taskId: string, input: InputCreateShare): OutputShare</code></li> <li><code>share.shareObservable(observableId: string, input: InputCreateShare): OutputShare</code></li> <li><code>share.updateShare(shareId: string, update: InputUpdateShare): void</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#organisation","title":"Organisation","text":"<ul> <li><code>organisation.get(orgIdOrName: string): OutputOrganisation</code></li> <li><code>organisation.create(org: InputCreateOrganisation): OutputOrganisation</code></li> <li><code>organisation.update(orgIdOrName: string, update: InputUpdateOrganisation): void</code></li> <li><code>organisation.bulkLink(orgIdOrName: string, links: InputOrganisationBulkLink): void</code></li> <li><code>organisation.listLinks(orgIdOrName: string): OutputOrganisationLink[]</code></li> <li><code>organisation.listSharingProfiles(): OutputSharingProfile[]</code></li> <li><code>organisation.link(orgA: string, orgB: string, link: InputOrganisationLink | null): void</code></li> <li><code>organisation.unlink(orgA: string, orgB: string): void</code></li> <li><code>organisation.find(query: any[]): OutputOrganisation[]</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#profile","title":"Profile","text":"<ul> <li><code>profile.get(idOrName: string): OutputProfile</code></li> <li><code>profile.update(profileIdOrName: string, update: InputUpdateProfile): void</code></li> <li><code>profile.delete(profileIdOrName: string): void</code></li> <li><code>profile.create(profile: InputCreateProfile): OutputProfile</code></li> <li><code>profile.find(query: any[]): OutputProfile[]</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#custom-event","title":"Custom Event","text":"<ul> <li><code>customEvent.createInCase(caseId: string, input: InputCreateCustomEvent): OutputCustomEvent</code></li> <li><code>customEvent.update(id: string, update: InputUpdateCustomEvent): void</code></li> <li><code>customEvent.delete(id: string): void</code></li> <li><code>customEvent.find(query: any[]): OutputCustomEvent[]</code></li> </ul>"},{"location":"thehive/user-guides/organisation/functions/#function","title":"Function","text":"<ul> <li><code>function.create(function: InputCreateFunction): OutputFunction</code></li> <li><code>function.update(functionIdOrName: string, update: InputUpdateFunction): void</code></li> <li><code>function.delete(functionIdOrName: string): void</code></li> <li><code>function.find(query: any[]): OutputFunction</code></li> </ul>"},{"location":"thehive/user-guides/organisation/ui-configuration/","title":"UI Configuration Guide","text":""},{"location":"thehive/user-guides/organisation/ui-configuration/#ui-configuration","title":"UI configuration","text":"<p>At the Organisation level, few UI behaviours can be configured.</p> <p>Access to the list by opening the Organisation menu, and the UI Configuration tab.</p> <p> </p> UI configuration panel"},{"location":"thehive/user-guides/organisation/ui-configuration/#configuration-parameters","title":"Configuration parameters","text":"Hide Empty Case button disabled by default. When enabled, users from the current Organisation cannot create empty Cases and will have to chose between create a Case using a template or from an archive Disable Empty Case Merge Alerts into closed cases disabled by default. When enabled, users from the current Organisation are allowed to choose a closed Case to merge Alerts in Disallow refresh option in dashboards disabled by default. When enabled, users from the current organisation cannot refresh dashboards. This could be useful if performance issues are encountered with the UI with a certain number of users Don't refresh dashboards Disallow \"All\" period option in dashboards disabled by default. When enabled, users from the current Organisation cannot use the All period in dashboards. This could be useful if performance issues are encountered with the UI Don't select period on dashboards Select the default filter of alert case similarity panel Default filter for Similar Case tab, in the view of an Alert Select default filters for Case similarity Result in Similar Cases tab in Alerts Define the default date format used to display dates Select which format you want the dates to be displayed within the current organisation"},{"location":"thehive/user-guides/organisation/configure-organization/manage-case-templates/","title":"Manage case templates","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-case-templates/#manage-case-templates","title":"Manage case templates","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-case-templates/#about-case-templates","title":"About Case templates","text":"<p>A case template auto-populates fields when a new case is being created. A user can create templates to simplify the process of creating tasks and cases by populating fields automatically.</p> <p>A user can add, edit, delete, import and export case templates in addition to managing views and filters. </p> <p></p> <p>The case templates tab, shows a list of cases and corresponding case details such as display name, name, details, created/updated dates. Each case entry on the list can be edited, exported and deleted. A user can add new case templates and can import case templates. </p> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-case-templates/#add-a-case-template","title":"Add a case template","text":"<p>To Add a case template: </p> <ol> <li>Click the + , to Add a case template. </li> </ol> <p></p> <p>A new window opens. </p> <ol> <li>Enter the case title prefix in the Prefix.</li> <li>Enter the case template Name. </li> <li>Enter the case template Display Name. </li> <li>Select TLP, (White/Green/Amber/Red).</li> <li>Select PAP, (White/Green/Amber/Red).</li> <li>Select Severity, (Low/Medium/High/Critical).</li> <li>Click + to add Tags. (Refer to <code>Add tags</code>).</li> <li>Enter the case description in the Description. </li> <li>Click + to Add Tasks. (Refer to <code>Add tasks</code>).</li> <li>Click + to Add Custom Fields. (Refer to <code>Add custom field values</code>).</li> <li>Click the Confirm case template creation button. </li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-case-templates/#edit-a-case-template","title":"Edit a case template","text":"<p>To Edit a case template: </p> <p></p> <ol> <li>Click the Edit option from the list. </li> </ol> <p>A new window opens. </p> <ol> <li>Edit the required fields.    </li> <li>Click the Confirm case template edition button. </li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-case-templates/#delete-a-case-template","title":"Delete a case template","text":"<p>To Delete a case template: </p> <ol> <li>Click the Delete option from the list.</li> </ol> <p></p> <p>A new message pops-up.</p> <ol> <li>Click OK to delete the case template from the list.</li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-case-templates/#import-a-case-template","title":"Import a case template","text":"<p>To Import a case template: </p> <p></p> <ol> <li>Click the Import Case Template option. </li> </ol> <p>A new window opens. </p> <ol> <li>Click the Drop file or Click option in attachment. </li> <li>Click the Confirm button. </li> </ol> <p>NOTE: The file must be a valid JSON file. You can use the exported case template directly from theHive platform. </p> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-case-templates/#export-a-case-template","title":"Export a case template","text":"<p>To Export a case template: </p> <ol> <li>Click the Export Case Template option.</li> <li>A file is downloaded, that can be exported/sent.</li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-attachments/about-attachments/","title":"About Attachments","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-attachments/about-attachments/#about-attachments","title":"About Attachments","text":"<p>In this section you can find information about Attachments . </p> <p>To add a new attachment:</p> <ol> <li>On the attachments tab, Click the + to add a new attachment.</li> </ol> <p></p> <p>A new window opens. </p> <p></p> <ol> <li>Click the Drop file or Click option in attachment. </li> <li>Click the Confirm button. </li> </ol> <p>NOTE: A user can add one or more files simultaneously. </p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-attachments/update-attachments/","title":"Update Attachments","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-attachments/update-attachments/#update-attachments","title":"Update Attachments","text":"<p>In this section you can find more information about updating Attachments . </p> <p>To copy an attachment:</p> <ol> <li>On the attachments tab, Click the ellipsis (...) corresponding to the attachment to view more options.</li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-attachments/update-attachments/#copy","title":"Copy","text":"<p>The copy URL option lets you copy the url of the attachment. </p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-attachments/update-attachments/#download","title":"Download","text":"<p>The download option lets you download the attachment. </p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-attachments/update-attachments/#remove","title":"Remove","text":"<p>The Remove option lets you remove/delete the attachment. </p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-custom-tags/about_custom_tags/","title":"Custom Tags","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-custom-tags/about_custom_tags/#custom-tags","title":"Custom Tags","text":"<p>In this section you can find information about custom tags. </p> <p>Custom tags or free tags are free text tags associated with TheHive objects. Custom tags are not shared across organisations. Users can define sensitive data in tags without worrying about any data leakage issue.</p> <p>A user can add, edit, delete, and change the colour of a custom tags. A user can use the toggle filter button to apply filters, add filters and clear the filters. By default, the custom tags page displays a list of 30 items that can be navigated using the previous and next buttons. A user can manage views.  </p> <p>A user can view all the details about the number of cases, alerts, observables, templates, and details of created By, dates of creation and dates of update. </p> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-custom-tags/manage_views/","title":"Manage Views","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-custom-tags/manage_views/#manage-views","title":"Manage Views","text":"<p>A user can manage views. </p> <p>To manage views:</p> <ol> <li>Click the default button. </li> <li>Click the Manage Views from the list. </li> </ol> <p></p> <p>A new page opens. It has the Name of the view and the corresponding Actions. </p> <ol> <li>Click the ellipsis (...) corresponding to the name of the view that you want to delete. </li> <li>Click Delete.</li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-custom-tags/update/","title":"Update Custom Tags","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-custom-tags/update/#update-custom-tags","title":"Update Custom Tags","text":"<p>To update custom tags:</p> <ol> <li>Edit the Name of the tag. </li> <li>Click the tick mark to save changes.</li> <li>Click the cross mark to discard changes. </li> </ol> <p></p> <p>To update colour of the tag:</p> <ol> <li>Edit the Colour of the tag.</li> <li>Select the colour from the pallette.</li> <li>Click the tick mark to save changes.</li> <li>Click the cross mark to discard changes. </li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-notifications/manage_notification/","title":"Manage Notification","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-notifications/manage_notification/#manage-notification","title":"Manage Notification","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-notifications/manage_notification/#add-notification","title":"Add notification","text":"<p>A user can create a notification. </p> <p>To Add a notification:</p> <ol> <li>Enter notfification Name.</li> <li>Send notification to every user in the organization. </li> <li>Select Trigger from the list. (Refer to <code>Trigger events</code>)</li> <li>Enable notification.</li> <li>Select Notifiers. (Refer to <code>Selecting supported notifiers</code>)</li> <li>Click the Save button.</li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-notifications/supported-notifiers/","title":"Supported notifiers","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-notifications/supported-notifiers/#supported-notifiers","title":"Supported notifiers","text":"<ol> <li> <p>Click EmailerToAddr.</p> <ol> <li>Enter Subject.</li> <li>Enter the email address of the sender in From.</li> <li>Enter the email address of the receiver in To.</li> <li>Click Add variable to add in the Template field.</li> <li>Click the Save button.</li> </ol> <p></p> </li> <li> <p>Click HttpRequest.</p> <ol> <li>Select Endpoint. (Refer to <code>Add endpoints</code>)</li> <li>Select the HTTP Method.</li> <li>Either enter the URL. </li> <li>Or select the option Use endpoint url as prefix. </li> <li>Click Add variable to add in the Template field.</li> <li>Select option to Log errors. </li> <li>Click the + to add headers.         <ul> <li>enter a valid header key.</li> <li>enter a valid header value.</li> </ul> </li> <li>Under Authentication select the Type. </li> <li>Select the option Proxy. </li> <li>Select the option Do not check certificate Authority. (Not recommended)</li> <li>Select the option Disable hostname verification.</li> <li>Click the Save button.</li> </ol> <p></p> </li> <li> <p>Click MatterMost.</p> <ol> <li>Select Endpoint.(Refer to <code>Add endpoints</code>)</li> <li>Enter the Username.</li> <li>Enter the Channel.</li> <li>Click Add variable to add in the Template field.</li> <li>Click the Save button.</li> </ol> <p></p> </li> <li> <p>Click Slack.</p> <ol> <li>Select Endpoint. (Refer to <code>Add endpoints</code>)</li> <li>Click Add variable to add in the Text Template field.</li> <li>Enter the Username.</li> <li>Enter the Channel.</li> <li>Check the box for Advanced Settings <p>If you need help filling the Advance Settings fields, check the Slack documentation link </p> </li> <li>Enter a Blocks template.</li> <li>Enter an Attachment template.</li> <li>Select As User.</li> <li>Enter an icon emoji.</li> <li>Enter icon URL.</li> <li>Select Link names.</li> <li>Select Markdown.</li> <li>Enter a valid Parse string.</li> <li>Select Unfirl Links.</li> <li>Select Unfirl Media. </li> <li>Click the Save button.</li> </ol> <p></p> </li> <li> <p>Click Kafka.</p> <ol> <li>Enter a Topic.</li> <li>Enter type the list of servers in Bootstrap servers. </li> </ol> <p></p> </li> </ol>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-notifications/triggering-events/","title":"Trigger events","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-notifications/triggering-events/#trigger-events","title":"Trigger events","text":"<ol> <li> <p>Default (AnyCase).</p> </li> <li> <p>Case.</p> <ul> <li>CaseClosed</li> <li>CaseShared</li> <li>CaseCreated</li> </ul> </li> <li> <p>Alert.</p> <ul> <li>AlertCreated</li> <li>AlertImported</li> </ul> </li> <li> <p>Observable.</p> <ul> <li>ObservableCreated</li> </ul> </li> <li> <p>Job.</p> <ul> <li>JobFinished</li> </ul> </li> <li> <p>Task.</p> <ul> <li>TaskAssigned</li> <li>TaskClosed</li> </ul> </li> </ol>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-ui-configuration/about-ui-configuration/","title":"About UI Configuration","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-ui-configuration/about-ui-configuration/#about-ui-configuration","title":"About UI Configuration","text":"<p>In this section you can find information about UI Configuration . </p> <p>To configure the UI:</p> <ol> <li>Select the option Hide Empty Case Button to disallow creating empty cases.</li> <li>Select the option Merge alerts into closed cases to disallow merging alerts into closed cases.</li> <li>Select the option Disallow refresh option in dashboards to disallow the refresh option in dashboards</li> <li>Select the default filter of alert case similarity panel from the list.</li> <li>Define the default date format used to display dates.<ul> <li>e.g. YYYY-MM-DD HH:mm</li> </ul> </li> <li>Click the Save button. </li> </ol> <p></p> <p>NOTE:  Default date format can be configured at the Organisation level, by defining the preferred format in the UI Configuration view. This requires a user with org-admin profile or any profile with manageConfig permission.</p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-users/add-delete-user/","title":"Users (Add/Delete)","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-users/add-delete-user/#users-adddelete","title":"Users (Add/Delete)","text":"<p>After you login to TheHive application, you can see the organization icon on the page. </p> <ol> <li>Click the Organization icon. </li> </ol> <p></p> <p>On the first tab, you can find information about Users. You can add and delete user details.</p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-users/add-delete-user/#add-user","title":"Add User","text":"<p>To Add the user details:</p> <ol> <li>Click the + to Add user. </li> </ol> <p></p> <p>A new window opens.</p> <ol> <li>Enter Login credentials.</li> <li>Enter the Name.</li> <li>Select a Profile from the list. </li> <li>Click the Save and add another button to add more users. </li> <li>Click the Save button to add a user. </li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-users/add-delete-user/#delete-user","title":"Delete User","text":"<p>If a user has left the organization in such cases a user profile may be deleted. </p> <p>To Delete the user details:</p> <ol> <li>Click the ellipsis (...) corresponding to the required user.</li> <li>Click the Delete option.</li> </ol> <p></p> <p>The following message pops up:</p> <ol> <li>Click the OK button.</li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-users/lock/","title":"Lock User","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-users/lock/#lock-user","title":"Lock User","text":"<p>If a user is on contract in the organization and the contract ends, in such cases a user profile may be locked. </p> <p>To lock the user details:</p> <ol> <li>Click the ellipsis (...) corresponding to the user details that you want to lock.</li> <li>Click the Lock option.</li> </ol> <p></p> <p>A new pop up message opens.</p> <ol> <li>Click the OK button.</li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-users/manage-views/","title":"Manage Views","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-users/manage-views/#manage-views","title":"Manage Views","text":"<p>In this section, you can find information about managing views.  </p> <p>To manage views: </p> <ol> <li>Click the default button. </li> <li>Click Manage Views from the list. </li> </ol> <p></p> <p>A new page opens. It has the Name of the view and the corresponding Actions. </p> <ol> <li>Click the ellipsis (...) corresponding to the name of the view.</li> <li>Select the action. e.g. Delete</li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-users/preview/","title":"Preview","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-users/preview/#preview","title":"Preview","text":"<p>You can manage previews to see the details of a specific user.</p> <p>To preview the user details:</p> <p>On the list of users page, there is a Preview button corresponding to the specific user name.</p> <ol> <li>Click the Preview option. </li> </ol> <p></p> <p>The user details preview window opens.</p> <p>You can see details like the id, created by, created at date, updated date, name, login id, email, logo, MFA, API Key, and locked status of the user. </p> <p></p>"},{"location":"thehive/user-guides/organisation/configure-organization/manage-users/sort/","title":"Sort","text":""},{"location":"thehive/user-guides/organisation/configure-organization/manage-users/sort/#sort","title":"Sort","text":"<p>If a user wants to re-arranged any field, it can be done using the sort option.</p> <p>To sort on a field:</p> <ol> <li>Click the small arrows beside the field to be sorted.</li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/manage-endpoints/about-endpoints/","title":"About Endpoints","text":""},{"location":"thehive/user-guides/organisation/manage-endpoints/about-endpoints/#about-endpoints","title":"About endpoints","text":"<p>In this section you can find information regarding endpoints. An endpoint is the point of entry in a communication channel when two systems interact with each other. </p> <p>A user can create an endpoint. </p> <p></p>"},{"location":"thehive/user-guides/organisation/manage-endpoints/about-endpoints/#supported-connectors","title":"Supported connectors","text":"<ol> <li>Webhook.</li> <li>Mattermost.</li> <li>Slack.</li> <li>Http.</li> </ol>"},{"location":"thehive/user-guides/organisation/manage-endpoints/add_endpoints/","title":"Add Endpoints","text":""},{"location":"thehive/user-guides/organisation/manage-endpoints/add_endpoints/#add-endpoints","title":"Add endpoints","text":"<p>A user can create an endpoint. </p> <p>To Add an Endpoint:</p> <ol> <li>Click the + button or Click the Add a new endpoint link.</li> </ol> <p></p> <p>A new page opens.</p> <ol> <li>Select the connector.</li> <li>Click the Save button.</li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/notifications/analyzers/","title":"Analyzers","text":""},{"location":"thehive/user-guides/organisation/notifications/email-to-addr/","title":"Email to addr","text":""},{"location":"thehive/user-guides/organisation/notifications/email-to-users/","title":"Email to users","text":""},{"location":"thehive/user-guides/organisation/notifications/endpoints/","title":"Endpoints","text":""},{"location":"thehive/user-guides/organisation/notifications/filteredevents/","title":"Filtered Events Setup","text":""},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#how-to-write-filtered-events-for-notifications","title":"How to write filtered events for notifications ?","text":"Filtered event example: \"Case severity has been updated to High or Critical\" <p>Selecting \u201cFilteredEvent\u201d opens an empty field where we set our custom filter. The filters will apply to the audit of every actions that are happening in your organization. When there is a match, a notification is sent. </p> <p>Info</p> <p>We recommend reading this blog post that introduces the capabilities of filtered events.</p>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#anatomy-of-an-audit","title":"Anatomy of an audit","text":"<p>An audit is presented as a json like this:</p> <pre><code>{\n  \"_id\": \"~327684328\",\n  \"_type\": \"Audit\",\n  \"_createdBy\": \"director@movies.io\",\n  \"_createdAt\": 1694441999960,\n  \"action\": \"update\",\n  \"requestId\": \"74dc37479904ebe7:3957d351:18a847d4266:-8000:109\",\n  \"rootId\": \"~327925760\",\n  \"details\": {\n    \"status\": \"InProgress\",\n    \"stage\": \"InProgress\"\n  },\n  \"objectId\": \"~327925760\",\n  \"objectType\": \"Case\",\n  \"object\": {\n    \"_id\": \"~327925760\",\n    \"_type\": \"Case\",\n    \"_createdBy\": \"mia@movies.io\",\n    \"_updatedBy\": \"director@movies.io\",\n    \"_createdAt\": 1693412835689,\n    \"_updatedAt\": 1694441999230,\n    \"number\": 34,\n    \"title\": \"Behind themselves watch price take. I probably single service. Develop fear hotel real.\",\n    \"description\": \"***Description***\",\n    \"severity\": 3,\n    \"severityLabel\": \"HIGH\",\n    \"startDate\": 1693151602000,\n    \"tags\": [\n      \"tagA\"\n    ],\n    \"flag\": false,\n    \"tlp\": 2,\n    \"tlpLabel\": \"AMBER\",\n    \"pap\": 1,\n    \"papLabel\": \"GREEN\",\n    \"status\": \"InProgress\",\n    \"stage\": \"InProgress\",\n    \"assignee\": \"mia@movies.io\",\n    \"customFields\": [],\n    \"userPermissions\": [],\n    \"extraData\": {},\n    \"newDate\": 1693412835673,\n    \"inProgressDate\": 1694441998841,\n    \"timeToDetect\": 261233673,\n    \"timeToTriage\": 1029163168,\n    \"timeToAcknowledge\": 1290396841,\n    \"customFieldValues\": {}\n  },\n  \"context\": {\n    \"_id\": \"~327925760\",\n    \"_type\": \"Case\",\n    \"_createdBy\": \"mia@movies.io\",\n    \"_updatedBy\": \"director@movies.io\",\n    \"_createdAt\": 1693412835689,\n    \"_updatedAt\": 1694441999230,\n    \"number\": 34,\n    \"title\": \"Behind themselves watch price take. I probably single service. Develop fear hotel real.\",\n    \"description\": \"***Description***\",\n    \"severity\": 3,\n    \"severityLabel\": \"HIGH\",\n    \"startDate\": 1693151602000,\n    \"tags\": [\n      \"tagA\"\n    ],\n    \"flag\": false,\n    \"tlp\": 2,\n    \"tlpLabel\": \"AMBER\",\n    \"pap\": 1,\n    \"papLabel\": \"GREEN\",\n    \"status\": \"InProgress\",\n    \"stage\": \"InProgress\",\n    \"assignee\": \"mia@movies.io\",\n    \"customFields\": [],\n    \"userPermissions\": [],\n    \"extraData\": {},\n    \"newDate\": 1693412835673,\n    \"inProgressDate\": 1694441998841,\n    \"timeToDetect\": 261233673,\n    \"timeToTriage\": 1029163168,\n    \"timeToAcknowledge\": 1290396841,\n    \"customFieldValues\": {}\n  },\n  \"organisation\": {\n    \"organisationId\": \"~4169864\",\n    \"organisation\": \"Pulp Fiction\"\n  }\n}\n</code></pre> <p>Audit fields:</p> <ul> <li><code>_id</code>: id of the audit</li> <li><code>_type</code>: always <code>Audit</code></li> <li><code>_createdBy</code>: Author of the action</li> <li><code>action</code>: nature of the action. Can be <code>create</code>, <code>update</code>, <code>delete</code>, <code>merge</code> or <code>invoke</code> (for functions)</li> <li><code>requestId</code>: Id of the http request</li> <li><code>details</code>: a json object that contains the changes made on the object </li> <li><code>objectId</code>: <code>_id</code> of the object that was impacted by the action</li> <li><code>objectType</code>: <code>_type</code> of the object</li> <li><code>object</code>: full description of the object (after the modification)</li> <li><code>rootId</code>: <code>_id</code> of the top most object. For instance if a task is updated, this will be the <code>_id</code> of the case</li> <li><code>context</code>: context of the audit. This depends on what object is updated / created.</li> <li><code>organisation</code>: details about the organisation where this action happened</li> </ul>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#writing-a-filter","title":"Writing a filter","text":"<p>Filters must be JSON formatted and can use following operators:</p> <ul> <li><code>_and</code></li> <li><code>_or</code></li> <li><code>_not</code></li> <li><code>_any</code></li> <li><code>_lt</code></li> <li><code>_gt</code></li> <li><code>_lte</code></li> <li><code>_gte</code></li> <li><code>_eq</code></li> <li><code>_is</code></li> <li><code>_startsWith</code></li> <li><code>_endsWith</code></li> <li><code>_between</code></li> <li><code>_in</code></li> <li><code>_contains</code></li> <li><code>_like</code></li> <li><code>_has</code></li> <li><code>_empty</code></li> </ul>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#field-selection","title":"Field selection","text":"<p>When targeting a field, you can use a <code>.</code> notation to walk inside the json object.</p> <p>For instance <code>object.severity</code> will get the field severity in the json: </p> <pre><code>{\n  \"object\": {\n    \"severity\": 3\n  }\n}\n</code></pre>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#_and","title":"<code>_and</code>","text":"<pre><code>{\"_and\": [\n  { ... filterA },\n  { ... filterB }\n]}\n</code></pre> <p>Returns true when all filters (filterA and filterB) return true</p>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#_or","title":"<code>_or</code>","text":"<pre><code>{\"_or\": [\n  { ... filterA },\n  { ... filterB }\n]}\n</code></pre> <p>Returns true when one filters (filterA or filterB) return true</p>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#_not","title":"<code>_not</code>","text":"<pre><code>{\"_not\": {... filterA} }\n</code></pre> <p>Inverse the result of filterA</p>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#_any","title":"<code>_any</code>","text":"<pre><code>{  \"_any\": \"\" }\n</code></pre> <p>Returns always true, useful for testing</p>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#_lt-_gt-_lte-_gte","title":"<code>_lt</code>, <code>_gt</code>, <code>_lte</code>, <code>_gte</code>","text":"<pre><code>{ \"_lt\": { \"foo\" : 42 } }\n</code></pre> <p>Returns true when field <code>foo</code> is strictly lower than <code>42</code></p>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#_eq-_is","title":"<code>_eq</code>, <code>_is</code>","text":"<p><code>_eq</code> is an alias for <code>_is</code></p> <pre><code>{ \"_eq\": { \"foo\": 42 } }\n</code></pre> <p>Returns true when field <code>foo</code> is equal to <code>42</code>.</p> <pre><code>{ \"_eq\": { \"foo\": \"LOW\" } }\n</code></pre> <p>Returns true when field <code>foo</code> is equal to <code>\"LOW\"</code>.</p> <p>Also works with arrays:</p> <pre><code>{ \"_eq\": { \"tags\": [\"foo\", \"bar\"] } }\n</code></pre> <p>Returns true when field <code>tags</code> will be equal to <code>[\"foo\", \"bar\"]</code></p>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#_startswith-_endswith","title":"<code>_startsWith</code>, <code>_endsWith</code>","text":"<pre><code>{ \"_startsWith\": { \"foo\": \"LOW\" } }\n</code></pre> <p>Check that field <code>foo</code> starts with string <code>\"LOW\"</code></p>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#_like","title":"<code>_like</code>","text":"<pre><code>{ \"_like\": { \"foo\": \"*ice\" } }\n</code></pre> <p>Returns true when field <code>foo</code> ends with <code>\"lce\"</code></p> <pre><code>{ \"_like\": { \"foo\": \"ali*\" } }\n</code></pre> <p>Returns true when field <code>foo</code> starts with <code>\"ali\"</code></p> <pre><code>{ \"_like\": { \"foo\": \"*lce*\" } }\n</code></pre> <p>Returns true when field <code>foo</code> contains with <code>\"lce\"</code></p>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#_between","title":"<code>_between</code>","text":"<pre><code>{ \"_between\": { \"_field\": \"foo\", \"_from\": 0, \"_to\": 2 } }\n</code></pre> <p>Check that field <code>foo</code> is between <code>0</code> and <code>2</code> (<code>_from &lt;= value &lt; _to</code>). Only works when <code>foo</code> is a number</p>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#_in","title":"<code>_in</code>","text":"<pre><code>{ \"_in\": { \"_field\": \"foo\", \"_values\": [\"foo\", \"bar\"] } }\n</code></pre> <p>Checks that field <code>foo</code> is either <code>foo</code> or <code>bar</code>.</p> <p>This also works with numbers:</p> <pre><code>{ \"_in\": { \"_field\": \"foo\", \"_values\": [1, 42] } }\n</code></pre> <p><code>_in</code> can also be used when the field is an array:</p> <pre><code>{ \"_in\": { \"_field\": \"tags\", \"_values\": [\"foo\", \"bar\"] } }\n</code></pre> <pre><code>{\"tags\": [\"foo\"] } ==&gt; true\n{\"tags\": [\"bar\", \"baz\"] } ==&gt; true\n{\"tags\": [\"alice\", \"bob\"] } ==&gt; false\n</code></pre>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#_contains","title":"<code>_contains</code>","text":"<pre><code>{ \"_contains\": { \"foo\": \"LOW\" } }\n</code></pre> <p>Check that field <code>foo</code> contains the value <code>\"LOW\"</code>. This works with string and arrays of string.</p> <pre><code>{\"foo\": \"LOWER\" } ==&gt; true\n{\"foo\": [\"bar\", \"LOW\"] } ==&gt; true\n{\"foo\": [\"bar\", \"LOWER\"] } ==&gt; false\n</code></pre>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#_has","title":"<code>_has</code>","text":"<pre><code>{ \"_has\": \"foo\" }\n</code></pre> <p>Returns true when json object has a field named <code>foo</code></p>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#_empty","title":"<code>_empty</code>","text":"<pre><code>{ \"_empty\": \"foo\" }\n</code></pre> <p>Returns true when field <code>foo</code> is either <code>[]</code> or <code>\"\"</code> or <code>null</code>.</p> <p>It will return false if the field is not defined. For this use case, use <code>{\"_not\": {\"_has\": \"foo\" } }</code></p>"},{"location":"thehive/user-guides/organisation/notifications/filteredevents/#examples","title":"Examples","text":"<ul> <li>An alert is closed but no assignee was defined</li> </ul> <pre><code>{\n    \"_and\": [\n        {\n            \"_is\": {\n                \"objectType\": \"Alert\"\n            }\n        },\n        {\n            \"_is\": {\n                \"details.stage\": \"Closed\"\n            }\n        },\n        {\n            \"_not\": {\n                \"_has\": \"object.assignee\"\n            }\n        },\n        {\n            \"_not\": {\n                \"_has\": \"details.assignee\"\n            }\n        }\n    ]\n}\n</code></pre> <ul> <li>An observable was updated with a report from the analyzer <code>Crt_sh_Transparency_Logs_1_0</code></li> </ul> <pre><code>{\n    \"_and\": [\n        {\n            \"_is\": {\n                \"action\": \"update\"\n            }\n        },\n        {\n            \"_is\": {\n                \"objectType\": \"Observable\"\n            }\n        },\n        {\n            \"_has\": \"details.reports.Crt_sh_Transparency_Logs_1_0\"\n        }\n    ]\n}\n</code></pre> <ul> <li>Responder has finished</li> </ul> <pre><code>{\n    \"_and\": [\n        {\n            \"_is\": {\n                \"action\": \"update\"\n            }\n        },\n        {\n            \"_is\": {\n                \"objectType\": \"Action\"\n            }\n        },\n        {\n            \"_or\": [\n                {\n                    \"_is\": {\n                        \"details.status\": \"Success\"\n                    }\n                },\n                {\n                    \"_is\": {\n                        \"details.status\": \"Failure\"\n                    }\n                }\n            ]\n        }\n    ]\n}\n</code></pre> <ul> <li>Case is updated with a status <code>TruePositive</code> or <code>FalsePositive</code> and the custom field <code>business-unit</code> is either <code>Sales</code> or <code>Marketing</code></li> </ul> <pre><code>{\n    \"_and\": [\n        {\n            \"_is\": {\n                \"action\": \"update\"\n            }\n        },\n        {\n            \"_is\": {\n                \"objectType\": \"Case\"\n            }\n        },\n        {\n            \"_or\": [\n                {\n                    \"_is\": {\n                        \"details.status\": \"TruePositive\"\n                    }\n                },\n                {\n                    \"_is\": {\n                        \"details.status\": \"FalsePositive\"\n                    }\n                }\n            ]\n        },\n        {\n            \"_or\": [\n                {\n                    \"_is\": {\n                        \"object.customFieldValues.business-unit\": \"Sales\"\n                    }\n                },\n                {\n                    \"_is\": {\n                        \"object.customFieldValues.business-unit\": \"Marketing\"\n                    }\n                }\n            ]\n        }\n    ]\n}\n</code></pre> <ul> <li>The analyzer <code>EmlParser_2_1</code> finishes with a Success <pre><code>{\n    \"_and\": [\n        {\n            \"_is\": {\n                \"objectType\": \"Job\"\n            }\n        },\n        {\n            \"_is\": {\n                \"object.analyzerName\": \"EmlParser_2_1\"\n            }\n        },\n        {\n            \"_is\": {\n                \"object.status\": \"Success\"\n            }\n        }\n    ]\n}\n</code></pre></li> </ul>"},{"location":"thehive/user-guides/organisation/notifications/http-request/","title":"Send notifications to a HTTP Request endpoint","text":""},{"location":"thehive/user-guides/organisation/notifications/introduction/","title":"Introduction to Notifications & Endpoints","text":""},{"location":"thehive/user-guides/organisation/notifications/introduction/#notifications","title":"Notifications","text":""},{"location":"thehive/user-guides/organisation/notifications/introduction/#definition","title":"Definition","text":"<p>A notification is a described by:</p> <ol> <li>A Trigger</li> <li>One or more Notifiers</li> </ol> <p></p>"},{"location":"thehive/user-guides/organisation/notifications/introduction/#triggers","title":"Triggers","text":"<p>Each notification is associated to only one trigger. TheHive comes with several predefined triggers on Cases, Alerts, Tasks, Observables and Jobs. Custom triggers can also be defined with FilteredEvent.</p> <p>Another trigger let you run notifications on any event when selecting AnyEvents.</p>"},{"location":"thehive/user-guides/organisation/notifications/introduction/#triggers-on-cases","title":"Triggers on Cases","text":"<ul> <li>CaseClosed: Run an action when closing a Case</li> <li>CaseCreated: Run an action when a Case is created</li> <li>CaseShared: Run an action when a Case is shared</li> </ul>"},{"location":"thehive/user-guides/organisation/notifications/introduction/#triggers-on-alerts","title":"Triggers on Alerts","text":"<ul> <li>AlertCreated: Run an action when an Alert is created</li> <li>AlertImported: Run an action when an Alert in imported (a Case is created from an Alert or an Alert is attached to an existing Case)</li> </ul>"},{"location":"thehive/user-guides/organisation/notifications/introduction/#triggers-on-jobs","title":"Triggers on Jobs","text":"<ul> <li>JobFinished: Run an action when a Job is terminated, with success or failure</li> </ul>"},{"location":"thehive/user-guides/organisation/notifications/introduction/#triggers-on-observables","title":"Triggers on Observables","text":"<ul> <li>ObservableCreated: Run an action when an Observable is created</li> </ul>"},{"location":"thehive/user-guides/organisation/notifications/introduction/#triggers-on-tasks","title":"Triggers on Tasks","text":"<ul> <li>LoginMyTask: Run an action when a Task gain a new Log</li> <li>TaskAssigned: Run an action when a Task is assigned, or the assignee is updated</li> <li>TaskClosed: Run an action when a Task is closed</li> </ul>"},{"location":"thehive/user-guides/organisation/notifications/introduction/#filtered-event","title":"Filtered Event","text":"<p>When selecting FilteredEvent, TheHive lets you write a structured JSON filter. This filter aims to match particular events in the application that will trigger one or more actions described by notifiers.</p> <p> </p> Filtered event example: \"Case severity has been updated to Hight or Critical <p>Learn how to write filtered events and find more example in the dedicated page.</p>"},{"location":"thehive/user-guides/organisation/notifications/introduction/#notifiers","title":"Notifiers","text":"<p>Several types of Notifiers are available in TheHive:</p> <ul> <li>EmailToUser: send an email to all users in the current Organisation</li> <li>EmailToAddr: send an email to a specific email address</li> <li>HTTP Request: send data to a chosen HTTP endpoint</li> <li>Mattermost: send data to a chosen Mattermost endoint</li> <li>Slack: send data to a chosen Slack endpoint</li> <li>MS Teams: send data to a chosen Microsoft Teams endpoint</li> <li>Webhook: send data to a chosen webhook endpoint</li> <li>Kafka: send data to a chosen Kafka queue</li> <li>Redis: send data to a chosen Redis endpoint</li> </ul> <p>Two of them are dedicated to run Cortex Analyzers and Responders:</p> <ul> <li>RunAnalyzer: run selected Analyzers</li> <li>RunResponder: run selected Responders</li> </ul> <p>Some Notifiers require configuring Endpoints</p> <p>Some Notifiers require at least one endpoint to be defined. Refer to the page dedicated to each Notifier to learn how to create related endpoints.</p>"},{"location":"thehive/user-guides/organisation/notifications/introduction/#create-a-notification","title":"Create a Notification","text":"<p>Access to the Notifications list by opening the Organisation menu, and the Notifications tab.</p> <p></p> <p>Click the  button to add a notification.</p> <p></p> <ol> <li>Give a unique name to the notification</li> <li>Select a trigger</li> <li>Select a notifier and configure it</li> </ol> <p>Then click confirm to register the notification.</p>"},{"location":"thehive/user-guides/organisation/notifications/introduction/#operations-on-notifications","title":"Operations on Notifications","text":""},{"location":"thehive/user-guides/organisation/notifications/introduction/#delete-a-notification","title":"Delete a Notification","text":"<p>In the list of notification, click on the delete option:</p> <p></p>"},{"location":"thehive/user-guides/organisation/notifications/introduction/#disable-a-notificaiton","title":"Disable a Notificaiton","text":"<ul> <li>In the list of Notifications, edit the one to disable: </li> </ul> <ul> <li>Verify the result in the list of Notifications</li> </ul>"},{"location":"thehive/user-guides/organisation/notifications/kafka/","title":"Kafka Configuration","text":""},{"location":"thehive/user-guides/organisation/notifications/kafka/#send-notifications-to-kafka","title":"Send notifications to Kafka","text":"<p>Info</p> <ul> <li>No endpoint definition is required to send data to a topic in Kafka</li> </ul>"},{"location":"thehive/user-guides/organisation/notifications/kafka/#configuration","title":"Configuration","text":"<ul> <li>When creating a Notification select Kafka as Notifier and complete the form with: </li> <li>The Topic used in Kafka</li> <li>The IP address/homstname and port to connect</li> </ul> <ul> <li>Then, add other notifiers by clicking on the  button, or click confirm to create the Notification</li> </ul>"},{"location":"thehive/user-guides/organisation/notifications/mattermost/","title":"Mattermost Configuration","text":""},{"location":"thehive/user-guides/organisation/notifications/mattermost/#send-notifications-to-a-mattermost-channel","title":"Send notifications to a Mattermost channel","text":"<p>Using Mattermost as Notifier requires to create at least one endpoint. This endpoint defines how TheHive will connect to Mattermost.</p>"},{"location":"thehive/user-guides/organisation/notifications/mattermost/#create-an-enpoint","title":"Create an enpoint","text":"<p>In the Organisation configuration view, open the Endpoints tab. Then, click on the  button to create a new Notifier. </p> <p></p>"},{"location":"thehive/user-guides/organisation/notifications/mattermost/#enpoint-configuration","title":"Enpoint configuration","text":"<p>Choose Mattermost and complete required information.</p> <p></p> <ul> <li>Name: give a unique name to the endpoint</li> <li>URL: specify the URL to connect to your Mattermost instance</li> <li>Username: default username used to send data</li> <li>Channel: default channel used to send data</li> <li>Auth Type: Use Basic authentication to connect to this endpoint, or use Key or Bearer method</li> <li>Proxy settings: choose to use a web proxy to connect to this endpoint</li> <li>Certificate authorities: add custom Certificate Authorities if required (PEM format)</li> <li>SSL settings: disable Certificate Authority checking and/or checks on hostnames</li> </ul> <p>Then, click confirm to create the endpoint.</p>"},{"location":"thehive/user-guides/organisation/notifications/mattermost/#notification-configuration","title":"Notification configuration","text":"<p>When creating a Notification select Mattermost as Notifier and complete the form.</p> <p></p> <p>TheHive uses Handlebars to let you build templates with input data, and this can be used in most of all fields of the form:</p> <ul> <li>Endpoint: choose the endpoint to use</li> <li>Username: choose a username. Click on add variable if you want to use an information from the input data. This will override the default username configured in the endpoint</li> <li>Channel: choose the target channel on Mattermost to send data to. Click on add variable if you want to use an information from the input data. This will override the default channel configured in the endpoint</li> <li>Template:<ul> <li>Available format are: JSON, Markdown and Plain text </li> <li>Click Add variable to select a variable to insert in the template</li> </ul> </li> </ul> <p>Then click confirm to register this Notifier.</p>"},{"location":"thehive/user-guides/organisation/notifications/redis/","title":"Redis Configuration","text":""},{"location":"thehive/user-guides/organisation/notifications/redis/#send-notifications-to-kafka","title":"Send notifications to Kafka","text":"<p>Info</p> <ul> <li>No endpoint definition is required to send data to a database in Redis</li> </ul>"},{"location":"thehive/user-guides/organisation/notifications/redis/#configuration","title":"Configuration","text":"<p>Start by clicking on the  button to create an new notification.</p> <p></p> <p>Then select Redis as Notifier and complete the form with: </p> <p></p> <ul> <li>The Channel name used in Redis</li> <li>The IP address/homstname and port to connect</li> <li>A Username</li> <li>A password</li> <li>A database name</li> <li>SSL settings and custom Certificate Authorities can also be configured</li> </ul> <p>Then click confirm to create the Notification.</p>"},{"location":"thehive/user-guides/organisation/notifications/responders/","title":"Run Responders on events","text":""},{"location":"thehive/user-guides/organisation/notifications/slack/","title":"Slack Configuration","text":""},{"location":"thehive/user-guides/organisation/notifications/slack/#send-notifications-to-slack-channels","title":"Send notifications to Slack channels","text":"<p>Using Slack as Notifier requires to create at least one endpoint. This endpoint defines how TheHive will connect to Slack.</p>"},{"location":"thehive/user-guides/organisation/notifications/slack/#create-an-enpoint","title":"Create an enpoint","text":"<p>In the Organsation configuration view, open the Endpoints tab. Then, click on the  button to create a new Notifier. </p> <p></p>"},{"location":"thehive/user-guides/organisation/notifications/slack/#enpoint-configuration","title":"Enpoint configuration","text":"<p>Choose Slack and complete required information.</p> <p></p> <ul> <li>Name: give a unique name to the endpoint</li> <li>Token: specify the token to use to connect to the service</li> <li>Auth Type: Use Basic authentication to connect to this endpoint, or use Key or Bearer method</li> <li>Proxy settings: choose to use a web proxy to connect to this endpoint</li> <li>Certificate authorities: add custom Certificate Authorities if required (PEM format)</li> <li>SSL settings: disable Certificate Authority checking and/or checks on hostnames</li> </ul> <p>Then, click confirm to create the endpoint.</p>"},{"location":"thehive/user-guides/organisation/notifications/slack/#notification-configuration","title":"Notification configuration","text":"<p>When creating a Notification select Slack as Notifier and complete the form.</p> <p></p> <p>TheHive uses Handlebars to let you build templates with input data, and this can be used in most form fields:</p> <ul> <li>Endpoint: choose the endpoint to use</li> <li>Username: choose a username. Click on add variable if you want to use an information from the input data. This will override the default username configured in the endpoint</li> <li>Channel: choose the target channel on Slack to send data to. Click on add variable if you want to use an information from the input data. This will override the default channel configured in the endpoint</li> <li>Template: * Available format are: JSON, Markdown and Plain text <ul> <li>Click Add variable to select a variable to insert in the template</li> </ul> </li> </ul> <p>Then click confirm to register this Notifier.</p>"},{"location":"thehive/user-guides/organisation/notifications/slack/#advanced-settings","title":"Advanced settings","text":"<p>Several configuration options come with the integration with Slack. </p> <p>Slack documentation to the rescue</p> <ul> <li>Test your integration here</li> <li>Build your blocks with the Slack Block kit builder</li> </ul> <p></p>"},{"location":"thehive/user-guides/organisation/notifications/slack/#examples","title":"Examples","text":"<p>Example of a blocks template: send notification about case creation</p> <ul> <li>Trigger: CaseCreated</li> <li>Notifier: Slack</li> </ul> <pre><code>[\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": \"*New Case created: Case #{{object.number}}*\"\n    }\n  },\n  {\n    \"type\": \"divider\"\n  },\n  {\n    \"type\": \"section\",\n    \"text\": {\n      \"type\": \"mrkdwn\",\n      \"text\": \"&lt;{{url}}|{{object.title}}&gt; \\n :bee: \\n {{object.description}}\"\n    }\n  },\n  {\n    \"type\": \"section\",\n    \"fields\": [\n      {\n        \"type\": \"mrkdwn\",\n        \"text\": \"*Created by*\\n{{object._createdBy}}\\n*Assigned to*\\n{{object.assignee}}\"\n      }\n    ]\n  }\n]\n</code></pre>"},{"location":"thehive/user-guides/organisation/notifications/teams/","title":"MS Teams Configuration","text":""},{"location":"thehive/user-guides/organisation/notifications/teams/#send-notifications-to-a-ms-teams-channel","title":"Send notifications to a MS Teams channel","text":"<p>Using Microsoft Teams as Notifier requires to create at least one endpoint. This endpoint defines how TheHive will connect to MS Teams.</p>"},{"location":"thehive/user-guides/organisation/notifications/teams/#prepare-microsoft-teams","title":"Prepare Microsoft Teams","text":"<p>Info</p> <p>In this example, we are creating an incoming webhook in a dedicated channel named thehive.</p>"},{"location":"thehive/user-guides/organisation/notifications/teams/#create-an-enpoint","title":"Create an enpoint","text":"<p>In the Organisation configuration view, open the Endpoints tab. Then, click on the  button to create a new Notifier. </p> <p></p>"},{"location":"thehive/user-guides/organisation/notifications/teams/#enpoint-configuration","title":"Enpoint configuration","text":"<p>Choose Teams and complete required information.</p> <p></p> <ul> <li>Name: give a unique name to the endpoint</li> <li>URL: specify the URL to connect to your MS Teams ; This is the URL copied while creating the incoming webhook in Teams</li> <li>Auth Type: Use Basic authentication to connect to this endpoint, or use Key or Bearer method</li> <li>Proxy settings: choose to use a web proxy to connect to this endpoint</li> <li>Certificate authorities: add custom Certificate Authorities if required (PEM format)</li> <li>SSL settings: disable Certificate Authority checking and/or checks on hostnames</li> </ul> <p>Then, click confirm to create the endpoint.</p>"},{"location":"thehive/user-guides/organisation/notifications/teams/#notification-configuration","title":"Notification configuration","text":"<p>When creating a Notification select Teams/ENDPOINT (with ENDPOINT the name of the endpoint created) as Notifier and complete the form.</p> <p></p> <p>TheHive uses Handlebars to let you build templates with input data, and this can be used in most of all fields of the form:</p> <ul> <li>Endpoint: choose the endpoint to use</li> <li>Text template: This is required, even if an adaptive card template is filled. This is used in the summary part, in notifications. Format is plain text.</li> <li>Adaptive card template:<ul> <li>Available format are: JSON, Markdown and Plain text </li> <li>Click Add variable to select a variable to insert in the template</li> </ul> </li> </ul> <p>Example: template used to display notification when a new Case is created</p> <pre><code>{\n\"type\": \"AdaptiveCard\",\n\"body\": [\n    {\n    \"type\": \"TextBlock\",\n    \"size\": \"Medium\",\n    \"weight\": \"Bolder\",\n    \"text\": \"#{{object.number}}: {{object.title}}\",\n    \"horizontalAlignment\": \"Left\",\n    \"spacing\": \"None\",\n    \"wrap\": true\n    },\n    {\n    \"type\": \"ColumnSet\",\n    \"columns\": [\n        {\n        \"type\": \"Column\",\n        \"items\": [\n            {\n            \"type\": \"TextBlock\",\n            \"weight\": \"Bolder\",\n            \"text\": \"{{object._createdBy}}\",\n            \"fontType\": \"Default\",\n            \"color\": \"Accent\",\n            \"spacing\": \"None\"\n            },\n            {\n            \"type\": \"TextBlock\",\n            \"spacing\": \"None\",\n            \"text\": \"Created {{dateFormat object._createdAt \"EEEE d MMMM, k:m Z\" locale=\"en\" tz=\"Europe/Paris\"}}\",\n            \"isSubtle\": true,\n            \"wrap\": true,\n            \"fontType\": \"Default\",\n            \"weight\": \"Default\",\n            \"size\": \"Default\"\n            }\n        ]\n        }\n    ]\n    },\n    {\n    \"type\": \"FactSet\",\n    \"facts\": [\n        {\n        \"title\": \"severity\",\n        \"weight\": \"Bolder\",\n        \"value\": \"{{ severityLabel object.severity}}\"\n        },\n        {\n        \"title\": \"TLP\",\n        \"weight\": \"Bolder\",\n        \"value\": \"{{ tlpLabel object.tlp}}\"\n        }\n    ]\n    },\n    {\n    \"type\": \"TextBlock\",\n    \"weight\": \"Bolder\",\n    \"text\": \"Description\",\n    \"spacing\": \"Large\",\n    \"wrap\": true,\n    \"horizontalAlignment\": \"Left\"\n    },\n    {\n    \"type\": \"TextBlock\",\n    \"text\": \"{{object.description}}\",\n    \"spacing\": \"None\",\n    \"wrap\": true,\n    \"horizontalAlignment\": \"Left\",\n    \"maxLines\": 3\n    }\n],\n\"actions\": [\n    {\n    \"type\": \"Action.OpenUrl\",\n    \"title\": \"Open Case in TheHive\",\n    \"iconUrl\": \"https://docs.strangebee.com/images/thehive.png\",\n    \"url\": \"{{url}}\",\n    \"style\": \"positive\"\n    }\n],\n\"$schema\": \"http://adaptivecards.io/schemas/adaptive-card.json\",\n\"version\": \"1.5\"\n}\n</code></pre> <p>Used with the trigger Case created, this template will create a card like this in Microsoft Teams:</p> <p></p> <p>Tips</p> <p>Finish registering this notifier by clicking on the Confirm button.</p>"},{"location":"thehive/user-guides/organisation/notifications/teams/#write-ms-teams-active-cards","title":"Write MS Teams active Cards","text":"<p>Use https://adaptivecards.io/designer/ as a starting point to design your adaptive card</p>"},{"location":"thehive/user-guides/organisation/notifications/teams/#format-dates","title":"Format dates","text":"<ul> <li>TheHive uses handlerbars string helpers to read dates</li> <li>Formatting date and time in notifications requires using dedicated Java patterns</li> </ul>"},{"location":"thehive/user-guides/organisation/notifications/teams/#format-other-custom-data-from-thehive","title":"Format other custom data from TheHive","text":"<p>Few data custom to TheHive can be properly displayed using custom string handlers together with <code>object</code> data in notifications: </p> <ul> <li><code>tlpLabel</code> to display the TLP value (example: <code>{{tlpLabel object.tlp}}</code>)</li> <li><code>papLabel</code> to display the PAP value (example: <code>{{papLabel object.pap}}</code>)</li> <li><code>severityLabel</code> to display the severity value (example: <code>{{severityLabel object.severity}}</code>)</li> </ul>"},{"location":"thehive/user-guides/organisation/notifications/webhook/","title":"Use a Webhook notifier","text":""},{"location":"thehive/user-guides/organisation/templates/case-templates/","title":"Cases Templates Management","text":""},{"location":"thehive/user-guides/organisation/templates/case-templates/#define-case-templates","title":"Define Case templates","text":"<p>This section contains the Case templates you prepare for your organisation.</p>"},{"location":"thehive/user-guides/organisation/templates/case-templates/#list-of-case-templates","title":"List of Case Templates","text":"<p>Access to the list by opening the Organisation menu, then the Templates tab, and the Cases tab.</p> <p> </p> List of Case templates <p>Click the  button to create a new Case template.</p>"},{"location":"thehive/user-guides/organisation/templates/case-templates/#new-case-template","title":"New Case template","text":"Create a case template"},{"location":"thehive/user-guides/organisation/templates/case-templates/#configuration-parameters","title":"Configuration parameters","text":"Prefix String that will be prepended to the title of a Case when created with this template Name Name of the Case template. Used to identify the Case template with the API Display Name Name of the Case template displayed in the UI TLP Default TLP of the Case when created with this template PAP Default PAP of the Case when created with this template Severity Default Severity of the Case when created with this template Tags List of tags that will be added to the Cases created with this template Description Default description of Cases created with this template if not modified. Tasks Add tasks to the templates. They will be automatically added to the Case when created with this template Custom Fields Add Custom fields to the template. Default value can be set for Custom fields as well. Pages Add pages template to the template. They will be automatically added to the Case when created with this template"},{"location":"thehive/user-guides/organisation/templates/case-templates/#importexport","title":"Import/Export","text":""},{"location":"thehive/user-guides/organisation/templates/case-templates/#export-a-case-template","title":"Export a Case template","text":"<p>Case templates can be exported and stored as JSON files by clicking on the option icon  and selecting  Export case template</p> <p></p> Export a case template"},{"location":"thehive/user-guides/organisation/templates/case-templates/#import-a-case-template","title":"Import a Case template","text":"<p>Click on the button Import Case Template and select the JSON formatted file to import.</p> <p></p> Import a case template"},{"location":"thehive/user-guides/organisation/templates/page-templates/","title":"Pages Templates Management","text":""},{"location":"thehive/user-guides/organisation/templates/page-templates/#define-page-templates","title":"Define Page templates","text":"<p>This section contains the Page templates you prepare for your organisation.</p>"},{"location":"thehive/user-guides/organisation/templates/page-templates/#list-of-page-templates","title":"List of Page Templates","text":"<p>Access to the list by opening the Organisation menu, then the Templates tab, and the Pages tab.</p> <p> </p> List of Pages templates <p>Click the  button to create a new Page template.</p>"},{"location":"thehive/user-guides/organisation/templates/page-templates/#new-page-template","title":"New Page template","text":"Create a page template"},{"location":"thehive/user-guides/organisation/templates/page-templates/#configuration-parameters","title":"Configuration parameters","text":"Title Page template title. Used to identify the Page template with the API. Also used as a page title when the template is used in a case. Category Category for grouping pages on a common theme. Is used as a page tree in the case of. Content Default page content when the page template is used in a case."},{"location":"thehive/user-guides/organisation/templates/page-templates/#importexport","title":"Import/Export","text":""},{"location":"thehive/user-guides/organisation/templates/page-templates/#export-a-page-template","title":"Export a Page template","text":"<p>Page templates can be exported and stored as JSON files by clicking on the option icon  and selecting  Export page template</p> <p></p> Export a page template"},{"location":"thehive/user-guides/organisation/templates/page-templates/#import-a-page-template","title":"Import a Page template","text":"<p>Click on the button Import Page Template and select the JSON formatted file to import.</p> <p></p> Import a page template"},{"location":"thehive/user-guides/organisation/templates/report-templates/","title":"Reports Templates Management","text":""},{"location":"thehive/user-guides/organisation/templates/report-templates/#define-reports-templates","title":"Define Reports templates","text":"<p>This section contains the reports templates you prepare for your organisation.</p>"},{"location":"thehive/user-guides/organisation/templates/report-templates/#list-of-reports-templates","title":"List of Reports Templates","text":"<p>Access to the list by opening the Organisation menu, then the Templates tab, and the Reports tab.</p> <p> </p> List of reports templates <p>Click the  button to create a new Report template.</p>"},{"location":"thehive/user-guides/organisation/templates/report-templates/#new-report-template","title":"New Report template","text":"Create a report template"},{"location":"thehive/user-guides/organisation/templates/report-templates/#configuration-parameters","title":"Configuration parameters","text":"<p>Define report title and description</p> Title Page template title. Used to identify the Page template with the API. Also used as a page title when the template is used in a case. Description Category for grouping pages on a common theme. Is used as a page tree in the case of. Content Default page content when the page template is used in a case. <p>Then define the report content, consisting of a header, footer and widget list</p>"},{"location":"thehive/user-guides/organisation/templates/report-templates/#header","title":"Header","text":"<p>The header is composed of markdown content. It is not mandatory to define a header.</p> <p> </p> Define header"},{"location":"thehive/user-guides/organisation/templates/report-templates/#text-widget","title":"Text widget","text":"<p>It is possible to define a text widget to define content.</p> <p> </p> Define text widget <p>A text widget can be defined to add separating titles between widgets. </p> <p> </p> Define section title"},{"location":"thehive/user-guides/organisation/templates/report-templates/#image-widget","title":"Image widget","text":"<p>Add images by searching for them in computer files or by drag&amp;drop</p> <p> </p> Image widget"},{"location":"thehive/user-guides/organisation/templates/report-templates/#table-widget","title":"Table widget","text":"<p>Add tables containing case elements</p>"},{"location":"thehive/user-guides/organisation/templates/report-templates/#parameters-table","title":"Parameters table","text":"<ol> <li>Define entity to select which case elements will be displayed in the table.</li> <li>It is possible to define the maximum number of elements in the table.</li> <li>It is possible to activate protection on the display of observables to fang information.</li> </ol> Define entity table"},{"location":"thehive/user-guides/organisation/templates/report-templates/#data-columns","title":"Data Columns","text":"<ol> <li>When the widget is added, the most relevant information is automatically pre-selected. However, new columns can be added at any time.</li> <li>It is also possible to redefine the order of columns using drag&amp;drop.</li> <li>Finally, it is possible to delete any column by clicking on the cross button of the item</li> </ol> Define data columns table"},{"location":"thehive/user-guides/organisation/templates/report-templates/#sorts","title":"Sorts","text":"<ol> <li>It is possible to sort table information</li> <li>Define the data on which the table is to be filtered</li> <li>Choose sort order</li> <li>Add multiple sort data</li> <li>Delete excess sort data</li> </ol> Define sort table"},{"location":"thehive/user-guides/organisation/templates/report-templates/#filters","title":"Filters","text":"<ol> <li>You can add filters to the table to limit the data in the table.</li> <li>Select any information from the selected entity</li> <li>Define the operator to be applied to the filter</li> <li>Define control values</li> <li>Multiple filters can be added. </li> <li>To delete a filter, use the cross button.</li> <li>Finally, you can clear all the filters to be applied</li> </ol> Define filters table"},{"location":"thehive/user-guides/organisation/templates/report-templates/#list-widget","title":"List widget","text":"<p>Add list containing case elements</p>"},{"location":"thehive/user-guides/organisation/templates/report-templates/#parameters-list","title":"Parameters list","text":"<ol> <li>Define entity to select which case elements will be displayed in the list</li> <li>It is possible to define the maximum number of elements in the list</li> <li>It is possible to activate protection on the display of observables to fang informations</li> </ol> Define entity list"},{"location":"thehive/user-guides/organisation/templates/report-templates/#data-list","title":"Data List","text":"<ol> <li>When the widget is added, the most relevant informations is automatically pre-selected. However, new datas can be added at any time</li> <li>It is also possible to redefine the order of data using drag&amp;drop</li> <li>Finally, it is possible to delete any data by clicking on the cross button of the item</li> </ol> Define data list"},{"location":"thehive/user-guides/organisation/templates/report-templates/#sorts_1","title":"Sorts","text":"<ol> <li>It is possible to sort list informations</li> <li>Define the data on which the list is to be filtered</li> <li>Choose sort order</li> <li>Add multiple sort data</li> <li>Delete excess sort data</li> </ol> Define sort list"},{"location":"thehive/user-guides/organisation/templates/report-templates/#filters_1","title":"Filters","text":"<ol> <li>You can add filters to the list to limit the data in the list</li> <li>Select any information from the selected entity</li> <li>Define the operator to be applied to the filter</li> <li>Define control values</li> <li>Multiple filters can be added </li> <li>To delete a filter, use the cross button</li> <li>Finally, you can clear all the filters to be applied</li> </ol> Define filters list"},{"location":"thehive/user-guides/organisation/templates/report-templates/#footer","title":"Footer","text":"<p>The footer is composed of markdown content. It is not mandatory to define a footer.</p> <p> </p> Define footer"},{"location":"thehive/user-guides/organisation/templates/report-templates/#organize-elements","title":"Organize elements","text":"<ol> <li>When a widget is added to a report, it can be dropped into the desired location.</li> <li>It is possible to place the element at the end of the report by clicking on the desired widget button.</li> <li>The order of widgets can be rearranged at any time.</li> <li>Header and footer cannot be moved</li> </ol> Organise elements"},{"location":"thehive/user-guides/organisation/templates/report-templates/#actions-on-a-report","title":"Actions on a report","text":"<ol> <li>Edit any widget in the report</li> <li>Delete any widget except header and footer</li> <li>Preview the report</li> </ol> <ol> <li>Save the report at any time</li> <li>Exit report editing to return to report list</li> </ol> Actions on reports"}]}